{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total number of gaps:  38\n",
      "Number of Linear Gaps filled: 8\n",
      "Single gaps filled\n",
      "Number of gaps with backup water level: 3\n",
      "Gaps filled\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def read_wl_csv(file_path):\n",
    "    wl_df = pd.read_csv(file_path)\n",
    "\n",
    "    ##### if the csv is from lighthouse then this drop function is always true\n",
    "    ##### if the csv is not from lighthouse then you will need to modify the function\n",
    "\n",
    "    wl_df.drop(labels=range(len(wl_df)-6,len(wl_df)), axis=0, inplace=True)\n",
    "\n",
    "    keys = wl_df.keys().to_list()\n",
    "    \n",
    "    wl_df['date'] = pd.to_datetime(wl_df[keys[0]])\n",
    "    wl_df[keys[1]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df[keys[2]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df[keys[3]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df['pwl'] = pd.to_numeric(wl_df[keys[1]],errors= 'coerce')\n",
    "    wl_df['bwl'] = pd.to_numeric(wl_df[keys[2]],errors= 'coerce')\n",
    "    wl_df['harmwl'] = pd.to_numeric(wl_df[keys[3]],errors= 'coerce')\n",
    "    wl_df['pwl surge'] = wl_df['pwl'] - wl_df['harmwl']\n",
    "    wl_df['bwl surge'] = wl_df['bwl'] - wl_df['harmwl']\n",
    "    wl_df = wl_df.drop(columns=keys[0],axis=0)\n",
    "    wl_df = wl_df.drop(columns=keys[1],axis=0)\n",
    "    wl_df = wl_df.drop(columns=keys[2],axis=0)\n",
    "    wl_df = wl_df.drop(columns=keys[3],axis=0)\n",
    "    del keys\n",
    "    return wl_df\n",
    "\n",
    "def locate_gaps(WL_data):\n",
    "    lengthMissVal = []\n",
    "    dates = []\n",
    "    count = 0\n",
    "    for i in range(len(WL_data)):\n",
    "        if pd.isna(WL_data['pwl surge'][i]):\n",
    "            if count == 0:  # Start of a new NaN gap\n",
    "                dates.append(WL_data['date'][i])  # Record the start date of the gap\n",
    "            count += 1  # Increment the gap length\n",
    "\n",
    "        else:\n",
    "            if count > 0:  # End of a NaN gap\n",
    "                lengthMissVal.append(count)\n",
    "                count = 0  # Reset count after recording the gap length\n",
    "\n",
    "    # Finalize the DataFrame\n",
    "    WL_data_gaps = pd.DataFrame()\n",
    "    WL_data_gaps['date'] = pd.to_datetime(dates)\n",
    "    WL_data_gaps['gapLength'] = lengthMissVal\n",
    "    WL_data_gaps['gapTime(min)'] = WL_data_gaps['gapLength'] * 6\n",
    "\n",
    "    del lengthMissVal,dates,count\n",
    "\n",
    "    return WL_data_gaps\n",
    "\n",
    "def eligible_gap_length(WL_gaps): #Function to sort the lengh of the gaps into three categories\n",
    "    WL_gaps_filter_6min = WL_gaps['gapLength'] == 1\n",
    "    WL_gaps_filter = (WL_gaps['gapLength'] <= 576) & (WL_gaps['gapLength'] > 1)\n",
    "\n",
    "    #filters the data into individual dataframes\n",
    "    linear_gaps = WL_gaps[WL_gaps_filter_6min]\n",
    "    gaps_less_5_days = WL_gaps[WL_gaps_filter]\n",
    "\n",
    "    del WL_gaps_filter,WL_gaps_filter_6min\n",
    "\n",
    "    return linear_gaps,gaps_less_5_days\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_fill(Wl_data,linear_gaps): #function to fill in gaps with length of 1 using linear approach\n",
    "\n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(linear_gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "        new_value = (Wl_data.loc[(index_locations[i])-1,'pwl surge']+ Wl_data.loc[index_locations[i]+1,'pwl surge']) / 2\n",
    "        Wl_data.loc[index_locations[i],'pwl surge'] = new_value\n",
    "\n",
    "    del matching_dates, index_locations, new_value\n",
    "    \n",
    "    return Wl_data\n",
    "\n",
    "\n",
    "def check_bwl(Wl_data,gaps):\n",
    "\n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    gap_length = gaps['gapLength'].tolist()\n",
    "\n",
    "    valid_gaps = []\n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "\n",
    "        is_valid = Wl_data['bwl surge'][index_locations[i]:index_locations[i]+gap_length[i]].isna().sum() == 0\n",
    "        valid_gaps.append(is_valid)\n",
    "    \n",
    "    filtered_gaps = gaps[valid_gaps].reset_index(drop=True)\n",
    "\n",
    "    del matching_dates, index_locations, gap_length, valid_gaps, is_valid\n",
    "\n",
    "    return filtered_gaps\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "def poly_gap_fill(Wl_data, gaps):\n",
    "\n",
    "    poly_df_list = list()\n",
    "    \n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    gap_length = gaps['gapLength'].tolist()\n",
    "\n",
    "    gap_date_list = list()\n",
    "\n",
    "    for i in range(len(matching_dates)):\n",
    "\n",
    "        gap_date_df = pd.DataFrame()\n",
    "\n",
    "        gap_date_df['date'] = Wl_data['date'][index_locations[i]:index_locations[i]+gap_length[i]]\n",
    "\n",
    "        gap_date_list.append(gap_date_df)\n",
    "        \n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "\n",
    "        if index_locations[i]- 2161  > 0 and index_locations[i]+2161+gap_length[i] < len(Wl_data):\n",
    "\n",
    "\n",
    "            pwl_30_days = Wl_data['pwl surge'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            bwl_30_days = Wl_data['bwl surge'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            dates = Wl_data['date'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            model = sm.OLS(pwl_30_days, sm.add_constant(bwl_30_days), missing='drop')\n",
    "\n",
    "\n",
    "            results = model.fit()\n",
    "\n",
    "            slope = results.params[1]\n",
    "\n",
    "            intercept = results.params[0]\n",
    "\n",
    "            \n",
    "            poly_df = pd.DataFrame({'bwl surge': bwl_30_days, 'pwl surge': pwl_30_days,'date' : pd.to_datetime(dates)})\n",
    "\n",
    "            poly_df['mwl surge'] = intercept + slope*poly_df['bwl surge']\n",
    "\n",
    "            poly_df.loc[abs(poly_df['mwl surge'] - poly_df['pwl surge']) > 0.1, ['mwl surge', 'pwl surge']] = np.nan\n",
    "\n",
    "            \n",
    "            if poly_df['bwl surge'].isna().sum() + poly_df['pwl surge'].isna().sum() < len(poly_df)*0.1:\n",
    "\n",
    "\n",
    "                poly_df_copy = poly_df.copy()\n",
    "\n",
    "                poly_df_copy.dropna(inplace=True)\n",
    "\n",
    "                poly =np.polynomial.polynomial.Polynomial.fit(poly_df_copy['pwl surge'],poly_df_copy['bwl surge'],4)\n",
    "\n",
    "                pred_values = poly(poly_df['bwl surge'].values)\n",
    "\n",
    "                poly_df['mwl surge'] = pred_values\n",
    "\n",
    "                poly_df_list.append(poly_df)\n",
    "\n",
    "                del poly_df_copy, poly, pred_values\n",
    "\n",
    "    matched_dates1 = []\n",
    "    matched_dates2 = []\n",
    "\n",
    "    for df1, df2 in zip(gap_date_list, poly_df_list):\n",
    "\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "        df2['date'] = pd.to_datetime(df2['date'])\n",
    "        \n",
    "       \n",
    "        common_dates = df1['date'][df1['date'].isin(df2['date'])]\n",
    "        \n",
    "        filtered_df1 = df1[df1['date'].isin(common_dates)]\n",
    "        filtered_df2 = df2[df2['date'].isin(common_dates)]\n",
    "        \n",
    "        \n",
    "        matched_dates1.append(filtered_df1)\n",
    "        matched_dates2.append(filtered_df2)\n",
    "\n",
    "    match_df_1 = pd.concat(matched_dates1, ignore_index=True)\n",
    "    match_df_2 = pd.concat(matched_dates2, ignore_index=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    Wl_data_total = match_df_2.merge(Wl_data,on='date', how='outer')\n",
    "\n",
    "    Wl_data_total = Wl_data_total.drop(columns='bwl surge_x',axis=0)\n",
    "    Wl_data_total = Wl_data_total.drop(columns='pwl surge_x',axis=0)\n",
    "\n",
    "    Wl_data_total['pwl surge'] = Wl_data_total['pwl surge_y']\n",
    "    Wl_data_total['bwl surge'] = Wl_data_total['bwl surge_y']\n",
    "\n",
    "    Wl_data_total = Wl_data_total.drop(columns='pwl surge_y',axis=0)\n",
    "    Wl_data_total = Wl_data_total.drop(columns='bwl surge_y',axis=0)\n",
    "\n",
    "\n",
    "    del poly_df_list, matching_dates, index_locations, gap_length, gap_date_list, matched_dates1, matched_dates2, match_df_1, match_df_2\n",
    "\n",
    "\n",
    "    return Wl_data_total\n",
    "\n",
    "\n",
    "\n",
    "def cbi_gapfill(filepath):\n",
    "\n",
    "    wl_dataset = read_wl_csv(filepath)\n",
    "\n",
    "    print('Reading dataset')\n",
    "\n",
    "    Wl_gaps = locate_gaps(wl_dataset)\n",
    "\n",
    "    print('Total number of gaps: ', len(Wl_gaps))\n",
    "\n",
    "    linear_gaps,multi_gaps = eligible_gap_length(Wl_gaps)\n",
    "\n",
    "    print('Number of Linear Gaps filled:', len(linear_gaps))\n",
    "\n",
    "    dataset_LF = linear_fill(wl_dataset,linear_gaps)\n",
    "\n",
    "    print('Single gaps filled')\n",
    "\n",
    "    valid_multi_gaps = check_bwl(dataset_LF,multi_gaps)\n",
    "\n",
    "    print('Number of gaps with backup water level:', len(valid_multi_gaps))\n",
    "\n",
    "\n",
    "    filled_wl_dataset = poly_gap_fill(dataset_LF,valid_multi_gaps)\n",
    "\n",
    "    print('Gaps filled')\n",
    "\n",
    "    return filled_wl_dataset , wl_dataset\n",
    "    \n",
    "filled_data, orig_data = cbi_gapfill(r'/Users/rprocious/Waterlevels_CBI/CBI-2/Gap Filling/P21_2016_gaps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
