{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrpro\\AppData\\Local\\Temp\\ipykernel_17340\\2700076026.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  wl_df[keys[1]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
      "C:\\Users\\mrpro\\AppData\\Local\\Temp\\ipykernel_17340\\2700076026.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  wl_df[keys[2]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
      "C:\\Users\\mrpro\\AppData\\Local\\Temp\\ipykernel_17340\\2700076026.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  wl_df[keys[3]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total number of gaps:  178\n",
      "Number of Linear Gaps filled: 83\n",
      "Single gaps filled\n",
      "Number of gaps with backup water level: 14\n",
      "Gaps filled\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def read_wl_csv(file_path):\n",
    "    wl_df = pd.read_csv(file_path)\n",
    "\n",
    "    ##### if the csv is from lighthouse then this drop function is always true\n",
    "    ##### if the csv is not from lighthouse then you will need to modify the function\n",
    "\n",
    "    wl_df.drop(labels=range(len(wl_df)-6,len(wl_df)), axis=0, inplace=True)\n",
    "\n",
    "    keys = wl_df.keys().to_list()\n",
    "    \n",
    "    wl_df['date'] = pd.to_datetime(wl_df[keys[0]])\n",
    "    wl_df[keys[1]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df[keys[2]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df[keys[3]].replace([-999, -99, 99, 'NA', 'RM'], np.nan, inplace=True)\n",
    "    wl_df['pwl'] = pd.to_numeric(wl_df[keys[1]],errors= 'coerce')\n",
    "    wl_df['bwl'] = pd.to_numeric(wl_df[keys[2]],errors= 'coerce')\n",
    "    wl_df['harmwl'] = pd.to_numeric(wl_df[keys[3]],errors= 'coerce')\n",
    "    wl_df['pwl surge'] = wl_df['pwl'] - wl_df['harmwl']\n",
    "    wl_df['bwl surge'] = wl_df['bwl'] - wl_df['harmwl']\n",
    "    return wl_df\n",
    "\n",
    "def locate_gaps(WL_data):\n",
    "    lengthMissVal = []\n",
    "    dates = []\n",
    "    count = 0\n",
    "    for i in range(len(WL_data)):\n",
    "        if pd.isna(WL_data['pwl surge'][i]):\n",
    "            if count == 0:  # Start of a new NaN gap\n",
    "                dates.append(WL_data['date'][i])  # Record the start date of the gap\n",
    "            count += 1  # Increment the gap length\n",
    "\n",
    "        else:\n",
    "            if count > 0:  # End of a NaN gap\n",
    "                lengthMissVal.append(count)\n",
    "                count = 0  # Reset count after recording the gap length\n",
    "\n",
    "    # Finalize the DataFrame\n",
    "    WL_data_gaps = pd.DataFrame()\n",
    "    WL_data_gaps['date'] = pd.to_datetime(dates)\n",
    "    WL_data_gaps['gapLength'] = lengthMissVal\n",
    "    WL_data_gaps['gapTime(min)'] = WL_data_gaps['gapLength'] * 6\n",
    "    return WL_data_gaps\n",
    "\n",
    "def eligible_gap_length(WL_gaps): #Function to sort the lengh of the gaps into three categories\n",
    "    WL_gaps_filter_6min = WL_gaps['gapLength'] == 1\n",
    "    WL_gaps_filter = (WL_gaps['gapLength'] <= 576) & (WL_gaps['gapLength'] > 1)\n",
    "\n",
    "    #filters the data into individual dataframes\n",
    "    linear_gaps = WL_gaps[WL_gaps_filter_6min]\n",
    "    gaps_less_5_days = WL_gaps[WL_gaps_filter]\n",
    "\n",
    "    return linear_gaps,gaps_less_5_days\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_fill(Wl_data,linear_gaps): #function to fill in gaps with length of 1 using linear approach\n",
    "\n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(linear_gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "        new_value = (Wl_data.loc[(index_locations[i])-1,'pwl surge']+ Wl_data.loc[index_locations[i]+1,'pwl surge']) / 2\n",
    "        Wl_data.loc[index_locations[i],'pwl surge'] = new_value\n",
    "    \n",
    "    return Wl_data\n",
    "\n",
    "\n",
    "def check_bwl(Wl_data,gaps):\n",
    "\n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    gap_length = gaps['gapLength'].tolist()\n",
    "\n",
    "    valid_gaps = []\n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "\n",
    "        is_valid = Wl_data['bwl surge'][index_locations[i]:index_locations[i]+gap_length[i]].isna().sum() == 0\n",
    "        valid_gaps.append(is_valid)\n",
    "    \n",
    "    filtered_gaps = gaps[valid_gaps].reset_index(drop=True)\n",
    "\n",
    "    return filtered_gaps\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "def poly_gap_fill(Wl_data, gaps):\n",
    "\n",
    "    poly_df_list = list()\n",
    "    \n",
    "    matching_dates = Wl_data[Wl_data['date'].isin(gaps['date'])]\n",
    "\n",
    "    index_locations = matching_dates.index.tolist()\n",
    "\n",
    "    gap_length = gaps['gapLength'].tolist()\n",
    "\n",
    "    gap_date_list = list()\n",
    "\n",
    "    for i in range(len(matching_dates)):\n",
    "\n",
    "        gap_date_df = pd.DataFrame()\n",
    "\n",
    "        gap_date_df['date'] = Wl_data['date'][index_locations[i]:index_locations[i]+gap_length[i]]\n",
    "\n",
    "        gap_date_list.append(gap_date_df)\n",
    "        \n",
    "\n",
    "    for i in range(len(index_locations)):\n",
    "\n",
    "        if index_locations[i]- 2161  > 0 and index_locations[i]+2161+gap_length[i] < len(Wl_data):\n",
    "\n",
    "\n",
    "            pwl_30_days = Wl_data['pwl surge'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            bwl_30_days = Wl_data['bwl surge'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            dates = Wl_data['date'][(index_locations[i]- 2160):index_locations[i]+2160+gap_length[i]].tolist()\n",
    "\n",
    "            bwl_30_days_with_constants = sm.add_constant(bwl_30_days)\n",
    "\n",
    "            model = sm.OLS(pwl_30_days,bwl_30_days_with_constants,missing='drop')\n",
    "\n",
    "            results = model.fit()\n",
    "\n",
    "            slope = results.params[1]\n",
    "\n",
    "            intercept = results.params[0]\n",
    "\n",
    "            \n",
    "            poly_df = pd.DataFrame({'bwl surge': bwl_30_days, 'pwl surge': pwl_30_days,'date' : pd.to_datetime(dates)})\n",
    "\n",
    "            poly_df['mwl surge'] = intercept + slope*poly_df['bwl surge']\n",
    "\n",
    "            poly_df.loc[abs(poly_df['mwl surge'] - poly_df['pwl surge']) > 0.1, ['mwl surge', 'pwl surge']] = np.nan\n",
    "\n",
    "            \n",
    "            if poly_df['bwl surge'].isna().sum() + poly_df['pwl surge'].isna().sum() < len(poly_df)*0.1:\n",
    "\n",
    "\n",
    "                poly_df_copy = poly_df.copy()\n",
    "\n",
    "                poly_df_copy.dropna(inplace=True)\n",
    "\n",
    "                poly =np.polynomial.polynomial.Polynomial.fit(poly_df_copy['pwl surge'],poly_df_copy['bwl surge'],4)\n",
    "\n",
    "                pred_values = poly(poly_df['bwl surge'].values)\n",
    "\n",
    "                poly_df['mwl surge'] = pred_values\n",
    "\n",
    "                poly_df_list.append(poly_df)\n",
    "\n",
    "    matched_dates1 = []\n",
    "    matched_dates2 = []\n",
    "\n",
    "    for df1, df2 in zip(gap_date_list, poly_df_list):\n",
    "\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "        df2['date'] = pd.to_datetime(df2['date'])\n",
    "        \n",
    "       \n",
    "        common_dates = df1['date'][df1['date'].isin(df2['date'])]\n",
    "        \n",
    "        filtered_df1 = df1[df1['date'].isin(common_dates)]\n",
    "        filtered_df2 = df2[df2['date'].isin(common_dates)]\n",
    "        \n",
    "        \n",
    "        matched_dates1.append(filtered_df1)\n",
    "        matched_dates2.append(filtered_df2)\n",
    "\n",
    "    match_df_1 = pd.concat(matched_dates1, ignore_index=True)\n",
    "    match_df_2 = pd.concat(matched_dates2, ignore_index=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    Wl_data_total = match_df_2.merge(Wl_data,on='date', how='outer')\n",
    "\n",
    "    Wl_data_total = Wl_data_total.drop(columns='bwl surge_x',axis=0)\n",
    "    Wl_data_total = Wl_data_total.drop(columns='pwl surge_x',axis=0)\n",
    "    Wl_data_total = Wl_data_total.drop(columns='#date+time',axis=0)\n",
    "\n",
    "    Wl_data_total['pwl surge'] = Wl_data_total['pwl surge_y']\n",
    "    Wl_data_total['bwl surge'] = Wl_data_total['bwl surge_y']\n",
    "\n",
    "    Wl_data_total = Wl_data_total.drop(columns='pwl surge_y',axis=0)\n",
    "    Wl_data_total = Wl_data_total.drop(columns='bwl surge_y',axis=0)\n",
    "\n",
    "    return Wl_data_total\n",
    "\n",
    "\n",
    "\n",
    "def cbi_gapfill(filepath):\n",
    "\n",
    "    wl_dataset = read_wl_csv(filepath)\n",
    "\n",
    "    print('Reading dataset')\n",
    "\n",
    "    Wl_gaps = locate_gaps(wl_dataset)\n",
    "\n",
    "    print('Total number of gaps: ', len(Wl_gaps))\n",
    "\n",
    "    linear_gaps,multi_gaps = eligible_gap_length(Wl_gaps)\n",
    "\n",
    "    print('Number of Linear Gaps filled:', len(linear_gaps))\n",
    "\n",
    "    dataset_LF = linear_fill(wl_dataset,linear_gaps)\n",
    "\n",
    "    print('Single gaps filled')\n",
    "\n",
    "    valid_multi_gaps = check_bwl(dataset_LF,multi_gaps)\n",
    "\n",
    "    print('Number of gaps with backup water level:', len(valid_multi_gaps))\n",
    "\n",
    "\n",
    "    filled_wl_dataset = poly_gap_fill(dataset_LF,valid_multi_gaps)\n",
    "\n",
    "    print('Gaps filled')\n",
    "\n",
    "    return filled_wl_dataset\n",
    "    \n",
    "filled_data = cbi_gapfill(r'C:\\Users\\mrpro\\Documents\\Code\\CBI\\data 2\\lighthouse\\Rockport\\Rockport_2003-2012_pwl_harmwl_bwl.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filled_2014' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmwl surge\u001b[39m\u001b[38;5;124m'\u001b[39m],label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpwl surge\u001b[39m\u001b[38;5;124m'\u001b[39m],label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpwl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],filled_2014[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbwl surge\u001b[39m\u001b[38;5;124m'\u001b[39m],label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbwl\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filled_2014' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(filled_2014['date'],filled_2014['mwl surge'],label = 'filled', linestyle = 'dashed')\n",
    "\n",
    "plt.plot(filled_2014['date'],filled_2014['pwl surge'],label = 'pwl')\n",
    "\n",
    "plt.plot(filled_2014['date'],filled_2014['bwl surge'],label = 'bwl', color='Black')\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.ylim(0,4)\n",
    "\n",
    "start_date = pd.to_datetime('2016-06-13 16:00:00')\n",
    "\n",
    "end_date = pd.to_datetime('2016-06-13 18:00:00')\n",
    "\n",
    "plt.xlim(start_date, end_date)\n",
    "plt.legend(frameon = False)\n",
    "plt.title('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
