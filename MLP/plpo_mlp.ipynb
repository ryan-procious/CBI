{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9541485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all used imoorts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754e2db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM limited to 100 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 11:33:02.150174: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:04.594060: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:04.594133: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "#configure TensorFlow to limit VRAM usage\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=10000)]  # MB\n",
    "        )\n",
    "        print(\"VRAM limited to 100 MB.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory configuration must be set at program start:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e48ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_248698/823489.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "/tmp/ipykernel_248698/823489.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "# Used Functions\n",
    "\n",
    "#used to load and clean the csv files\n",
    "def load_and_clean_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Drop last 6 rows\n",
    "    df = df.iloc[:-6]\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df['#date+time'] = pd.to_datetime(df['#date+time'], errors='coerce')\n",
    "    df = df.rename(columns={'#date+time': 'date_time'})\n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != 'date_time':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "# Function to convert wind speed and direction to U/V components\n",
    "\n",
    "def wind_to_uv(speed, direction_deg):\n",
    "    # Convert to radians\n",
    "    direction_rad = np.deg2rad(direction_deg)\n",
    "\n",
    "    # U = -speed * sin(direction), V = -speed * cos(direction)\n",
    "\n",
    "    u = -speed * np.sin(direction_rad)\n",
    "    v = -speed * np.cos(direction_rad)\n",
    "    return u, v\n",
    "\n",
    "# Function to prepare the DataFrame for model input\n",
    "def prepare_dataframe(df):\n",
    "    # Rename columns to a consistent format\n",
    "    df = df.rename(columns={\n",
    "        df.columns[0]: 'date_time',\n",
    "        df.columns[1]: 'pwl',\n",
    "        df.columns[2]: 'wsd',\n",
    "        df.columns[3]: 'wdr'\n",
    "    })\n",
    "    \n",
    "    # Convert wind to U/V components\n",
    "    u, v = wind_to_uv(df['wsd'], df['wdr'])\n",
    "    df[['u', 'v']] = np.column_stack((u, v))\n",
    "    \n",
    "    # Drop raw wind columns\n",
    "    df.drop(columns=['wsd', 'wdr'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "# Function to create input and output arrays for the model\n",
    "def input_output_arrays(pl_df, po_df, wl_window, wind_window):\n",
    "\n",
    "    wl_window = wl_window * 10\n",
    "    wind_window = wind_window * 10\n",
    "\n",
    "    pl_df = pl_df.reset_index(drop=True)\n",
    "    po_df = po_df.reset_index(drop=True)\n",
    "\n",
    "    pl_pwl = pl_df['pwl'].to_numpy()\n",
    "    pl_u   = pl_df['u'].to_numpy()\n",
    "    pl_v   = pl_df['v'].to_numpy()\n",
    "    po_pwl = po_df['pwl'].to_numpy()\n",
    "\n",
    "    X, y, indices = [], [], []\n",
    "\n",
    "    for t in range(max(wl_window, wind_window), len(pl_df) - wl_window):\n",
    "        # Water level window: centered at t\n",
    "        wl_slice = slice(t - wl_window, t + wl_window + 1)\n",
    "\n",
    "        # Wind window: past `wind_window` values ending at t\n",
    "        wind_slice = slice(t - wind_window + 1, t + 1)\n",
    "\n",
    "        pwl_input = pl_pwl[wl_slice]\n",
    "        u_input = pl_u[wind_slice]\n",
    "        v_input = pl_v[wind_slice]\n",
    "        target = po_pwl[t]\n",
    "\n",
    "        if (\n",
    "            np.isnan(pwl_input).any() or\n",
    "            np.isnan(u_input).any() or\n",
    "            np.isnan(v_input).any() or\n",
    "            np.isnan(target)\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        features = np.concatenate([pwl_input, u_input, v_input])\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "        indices.append(t)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(indices)\n",
    "\n",
    "file_paths = {\n",
    "    'po11': '/home/ryan/Downloads/po_2011-2012.csv',\n",
    "    'pl11': '/home/ryan/Downloads/pl_2011-2012.csv',\n",
    "    'po13': '/home/ryan/Downloads/po_2013-2014.csv',\n",
    "    'pl13': '/home/ryan/Downloads/pl_2013-2014.csv',\n",
    "    'po15': '/home/ryan/Downloads/po_2015-2016.csv',\n",
    "    'pl15': '/home/ryan/Downloads/pl_2015-2016.csv',\n",
    "}\n",
    "# Function to calculate the percentage of predictions with a central frequency (CF)\n",
    "def calculate_central_frequency_percentage(testing_label_array, predictions, cm):\n",
    "  \"\"\"Find the percentage of predictions with a central frequency (CF) of less than\n",
    "  or equal to a given number of centimeters (cm)\n",
    "\n",
    "\tArgs:\n",
    "        testing_label_array (array): Testing labels\n",
    "\n",
    "        predictions (array): Model predictions\n",
    "\n",
    "        cm (int): Number of centimeters\n",
    "\n",
    "\tReturns:\n",
    "\t\t(float): central frequency (CF) percentage\n",
    "\t\"\"\"\n",
    "  less_than_cm_counter = 0\n",
    "\n",
    "  # Convert cm to m\n",
    "  cm_to_m = cm / 100\n",
    "\n",
    "  for index, prediction in enumerate(predictions):\n",
    "    if abs(testing_label_array[index] - prediction) <= cm_to_m:\n",
    "      less_than_cm_counter += 1\n",
    "\n",
    "  cf_percentage = (less_than_cm_counter / len(predictions)) * 100\n",
    "\n",
    "  return cf_percentage\n",
    "\n",
    "# Function to evaluate the model on the testing data\n",
    "def evaluate_model(model, testing_input_array, testing_label_array):\n",
    "  \"\"\"Calculates loss, makes predictions, and calculates Central Frequency (CF),\n",
    "  Mean Squared Error (MSE), Root Mean Squared Error(RMSE), Mean Absolute Error (MAE),\n",
    "  Median Absolute Error, and R-squared (R2)\n",
    "\n",
    "\tArgs:\n",
    "        model (tf.keras.model): The trained model\n",
    "\n",
    "        testing_input_array (array): Testing inputs\n",
    "\n",
    "        testing_label_array (array): Testing labels\n",
    "\t\"\"\"\n",
    "  print(\"Calculating Loss:\")\n",
    "  test_loss = model.evaluate(testing_input_array, testing_label_array, batch_size = len(testing_input_array))\n",
    "\n",
    "  print(\"Loss:\", test_loss)\n",
    "\n",
    "\n",
    "  print(\"\\nGenerating output predictions with model:\")\n",
    "  predictions = model.predict(testing_input_array, batch_size = len(testing_input_array))\n",
    "\n",
    "  # Calculate evaluation metrics\n",
    "  cf_15cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 15)\n",
    "  cf_5cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 5)\n",
    "  cf_1cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 1)\n",
    "  mse = mean_squared_error(testing_label_array, predictions)\n",
    "  rmse = root_mean_squared_error(testing_label_array, predictions)\n",
    "  mae = mean_absolute_error(testing_label_array, predictions)\n",
    "  medae = median_absolute_error(testing_label_array, predictions)\n",
    "  r2 = r2_score(testing_label_array, predictions)\n",
    "\n",
    "  print(\"\\nCentral Frequency Percentage 15cm:\", cf_15cm_percentage)\n",
    "  print(\"\\nCentral Frequency Percentage 5cm:\", cf_5cm_percentage)\n",
    "  print(\"\\nCentral Frequency Percentage 1cm:\", cf_1cm_percentage)\n",
    "  print(\"Mean Squared Error:\", mse)\n",
    "  print(\"Root Mean Squared Error:\", rmse)\n",
    "  print(\"Mean Absolute Error:\", mae)\n",
    "  print(\"Median Absolute Error:\", medae)\n",
    "  print(\"R-squared:\", r2)\n",
    "\n",
    "datasets = {key: load_and_clean_csv(path) for key, path in file_paths.items()}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    datasets[name] = prepare_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f90aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>pwl</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-11-01 00:00:00</td>\n",
       "      <td>3.493</td>\n",
       "      <td>-4.414738</td>\n",
       "      <td>2.347358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-11-01 00:06:00</td>\n",
       "      <td>3.495</td>\n",
       "      <td>-4.792432</td>\n",
       "      <td>1.744303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-11-01 00:12:00</td>\n",
       "      <td>3.498</td>\n",
       "      <td>-4.673729</td>\n",
       "      <td>2.279530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-11-01 00:18:00</td>\n",
       "      <td>3.504</td>\n",
       "      <td>-4.633234</td>\n",
       "      <td>2.360751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-11-01 00:24:00</td>\n",
       "      <td>3.509</td>\n",
       "      <td>-4.633234</td>\n",
       "      <td>2.360751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182873</th>\n",
       "      <td>2013-12-01 23:18:00</td>\n",
       "      <td>3.640</td>\n",
       "      <td>0.167246</td>\n",
       "      <td>1.591235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182874</th>\n",
       "      <td>2013-12-01 23:24:00</td>\n",
       "      <td>3.645</td>\n",
       "      <td>-0.219510</td>\n",
       "      <td>2.088496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182875</th>\n",
       "      <td>2013-12-01 23:30:00</td>\n",
       "      <td>3.647</td>\n",
       "      <td>0.167246</td>\n",
       "      <td>1.591235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182876</th>\n",
       "      <td>2013-12-01 23:36:00</td>\n",
       "      <td>3.655</td>\n",
       "      <td>0.523711</td>\n",
       "      <td>1.826397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182877</th>\n",
       "      <td>2013-12-01 23:42:00</td>\n",
       "      <td>3.658</td>\n",
       "      <td>-0.160440</td>\n",
       "      <td>2.294397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182878 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date_time    pwl         u         v\n",
       "0      2011-11-01 00:00:00  3.493 -4.414738  2.347358\n",
       "1      2011-11-01 00:06:00  3.495 -4.792432  1.744303\n",
       "2      2011-11-01 00:12:00  3.498 -4.673729  2.279530\n",
       "3      2011-11-01 00:18:00  3.504 -4.633234  2.360751\n",
       "4      2011-11-01 00:24:00  3.509 -4.633234  2.360751\n",
       "...                    ...    ...       ...       ...\n",
       "182873 2013-12-01 23:18:00  3.640  0.167246  1.591235\n",
       "182874 2013-12-01 23:24:00  3.645 -0.219510  2.088496\n",
       "182875 2013-12-01 23:30:00  3.647  0.167246  1.591235\n",
       "182876 2013-12-01 23:36:00  3.655  0.523711  1.826397\n",
       "182877 2013-12-01 23:42:00  3.658 -0.160440  2.294397\n",
       "\n",
       "[182878 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['po11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00b7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167924, 81) (167924,)\n"
     ]
    }
   ],
   "source": [
    "# create input and output arrays for training, validation, and testing\n",
    "\n",
    "X_train, y_train, indices_train = input_output_arrays(datasets['pl11'], datasets['po11'], wl_window=2, wind_window=2)\n",
    "X_valid, y_valid, indices_valid = input_output_arrays(datasets['pl13'], datasets['po13'], wl_window=2, wind_window=2)\n",
    "X_test, y_test, indices_test = input_output_arrays(datasets['pl15'], datasets['po15'], wl_window=2, wind_window=2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 11:33:09.111810: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.111944: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.111997: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.112076: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.112131: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.112183: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-02 11:33:09.112202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory:  -> device: 0, name: Radeon RX 7900 GRE, pci bus id: 0000:29:00.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748881990.548327  249798 service.cc:146] XLA service 0x7feef0207aa0 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748881990.548362  249798 service.cc:154]   StreamExecutor device (0): Radeon RX 7900 GRE, AMDGPU ISA version: gfx1100\n",
      "2025-06-02 11:33:10.561956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - loss: 15.8594 - mae: 3.9697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748881995.316303  249798 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 14.48162, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 15.8594 - mae: 3.9697 - val_loss: 14.4816 - val_mae: 3.7942\n",
      "Epoch 2/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 14.8890 - mae: 3.8453\n",
      "Epoch 2: val_loss improved from 14.48162 to 13.66857, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 14.8890 - mae: 3.8453 - val_loss: 13.6686 - val_mae: 3.6840\n",
      "Epoch 3/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 13.9765 - mae: 3.7233\n",
      "Epoch 3: val_loss improved from 13.66857 to 12.90080, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 13.9765 - mae: 3.7233 - val_loss: 12.9008 - val_mae: 3.5757\n",
      "Epoch 4/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 13.1100 - mae: 3.6025\n",
      "Epoch 4: val_loss improved from 12.90080 to 12.17666, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 13.1100 - mae: 3.6025 - val_loss: 12.1767 - val_mae: 3.4693\n",
      "Epoch 5/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 12.3029 - mae: 3.4848\n",
      "Epoch 5: val_loss improved from 12.17666 to 11.49331, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 12.3029 - mae: 3.4848 - val_loss: 11.4933 - val_mae: 3.3646\n",
      "Epoch 6/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 11.5368 - mae: 3.3682\n",
      "Epoch 6: val_loss improved from 11.49331 to 10.84716, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 11.5368 - mae: 3.3682 - val_loss: 10.8472 - val_mae: 3.2613\n",
      "Epoch 7/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 10.8232 - mae: 3.2543\n",
      "Epoch 7: val_loss improved from 10.84716 to 10.23468, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 10.8232 - mae: 3.2543 - val_loss: 10.2347 - val_mae: 3.1590\n",
      "Epoch 8/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 10.1466 - mae: 3.1414\n",
      "Epoch 8: val_loss improved from 10.23468 to 9.65285, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 10.1466 - mae: 3.1414 - val_loss: 9.6529 - val_mae: 3.0575\n",
      "Epoch 9/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 9.5063 - mae: 3.0291\n",
      "Epoch 9: val_loss improved from 9.65285 to 9.09966, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 9.5063 - mae: 3.0291 - val_loss: 9.0997 - val_mae: 2.9563\n",
      "Epoch 10/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 8.9052 - mae: 2.9183\n",
      "Epoch 10: val_loss improved from 9.09966 to 8.57365, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 8.9052 - mae: 2.9183 - val_loss: 8.5736 - val_mae: 2.8554\n",
      "Epoch 11/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 8.3337 - mae: 2.8075\n",
      "Epoch 11: val_loss improved from 8.57365 to 8.07404, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 8.3337 - mae: 2.8075 - val_loss: 8.0740 - val_mae: 2.7547\n",
      "Epoch 12/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 7.7957 - mae: 2.6975\n",
      "Epoch 12: val_loss improved from 8.07404 to 7.60057, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 7.7957 - mae: 2.6975 - val_loss: 7.6006 - val_mae: 2.6543\n",
      "Epoch 13/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 7.2869 - mae: 2.5877\n",
      "Epoch 13: val_loss improved from 7.60057 to 7.15295, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 7.2869 - mae: 2.5877 - val_loss: 7.1530 - val_mae: 2.5541\n",
      "Epoch 14/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 6.8105 - mae: 2.4789\n",
      "Epoch 14: val_loss improved from 7.15295 to 6.73093, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 6.8105 - mae: 2.4789 - val_loss: 6.7309 - val_mae: 2.4544\n",
      "Epoch 15/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 6.3635 - mae: 2.3714\n",
      "Epoch 15: val_loss improved from 6.73093 to 6.33425, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 6.3635 - mae: 2.3714 - val_loss: 6.3342 - val_mae: 2.3558\n",
      "Epoch 16/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 5.9495 - mae: 2.2667\n",
      "Epoch 16: val_loss improved from 6.33425 to 5.96262, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 5.9495 - mae: 2.2667 - val_loss: 5.9626 - val_mae: 2.2595\n",
      "Epoch 17/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 5.5623 - mae: 2.1649\n",
      "Epoch 17: val_loss improved from 5.96262 to 5.61562, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 5.5623 - mae: 2.1649 - val_loss: 5.6156 - val_mae: 2.1667\n",
      "Epoch 18/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 5.2069 - mae: 2.0678\n",
      "Epoch 18: val_loss improved from 5.61562 to 5.29265, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 5.2069 - mae: 2.0678 - val_loss: 5.2927 - val_mae: 2.0793\n",
      "Epoch 19/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 4.8738 - mae: 1.9754\n",
      "Epoch 19: val_loss improved from 5.29265 to 4.99308, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 4.8738 - mae: 1.9754 - val_loss: 4.9931 - val_mae: 1.9984\n",
      "Epoch 20/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 4.5736 - mae: 1.8906\n",
      "Epoch 20: val_loss improved from 4.99308 to 4.71608, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 4.5736 - mae: 1.8906 - val_loss: 4.7161 - val_mae: 1.9244\n",
      "Epoch 21/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 4.2962 - mae: 1.8120\n",
      "Epoch 21: val_loss improved from 4.71608 to 4.46068, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 4.2962 - mae: 1.8120 - val_loss: 4.4607 - val_mae: 1.8572\n",
      "Epoch 22/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 4.0466 - mae: 1.7415\n",
      "Epoch 22: val_loss improved from 4.46068 to 4.22577, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 4.0466 - mae: 1.7415 - val_loss: 4.2258 - val_mae: 1.7968\n",
      "Epoch 23/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.8190 - mae: 1.6775\n",
      "Epoch 23: val_loss improved from 4.22577 to 4.01017, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 3.8190 - mae: 1.6775 - val_loss: 4.0102 - val_mae: 1.7425\n",
      "Epoch 24/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 3.6141 - mae: 1.6201\n",
      "Epoch 24: val_loss improved from 4.01017 to 3.81253, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 3.6141 - mae: 1.6201 - val_loss: 3.8125 - val_mae: 1.6934\n",
      "Epoch 25/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.4325 - mae: 1.5712\n",
      "Epoch 25: val_loss improved from 3.81253 to 3.63142, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 3.4325 - mae: 1.5712 - val_loss: 3.6314 - val_mae: 1.6484\n",
      "Epoch 26/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.2671 - mae: 1.5272\n",
      "Epoch 26: val_loss improved from 3.63142 to 3.46541, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 3.2671 - mae: 1.5272 - val_loss: 3.4654 - val_mae: 1.6067\n",
      "Epoch 27/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.1200 - mae: 1.4891\n",
      "Epoch 27: val_loss improved from 3.46541 to 3.31291, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 3.1200 - mae: 1.4891 - val_loss: 3.3129 - val_mae: 1.5679\n",
      "Epoch 28/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 2.9890 - mae: 1.4550\n",
      "Epoch 28: val_loss improved from 3.31291 to 3.17238, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 2.9890 - mae: 1.4550 - val_loss: 3.1724 - val_mae: 1.5311\n",
      "Epoch 29/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.8692 - mae: 1.4236\n",
      "Epoch 29: val_loss improved from 3.17238 to 3.04227, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 2.8692 - mae: 1.4236 - val_loss: 3.0423 - val_mae: 1.4960\n",
      "Epoch 30/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.7612 - mae: 1.3951\n",
      "Epoch 30: val_loss improved from 3.04227 to 2.92113, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 2.7612 - mae: 1.3951 - val_loss: 2.9211 - val_mae: 1.4620\n",
      "Epoch 31/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 2.6615 - mae: 1.3685\n",
      "Epoch 31: val_loss improved from 2.92113 to 2.80758, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 2.6615 - mae: 1.3685 - val_loss: 2.8076 - val_mae: 1.4287\n",
      "Epoch 32/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.5704 - mae: 1.3425\n",
      "Epoch 32: val_loss improved from 2.80758 to 2.70045, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 2.5704 - mae: 1.3425 - val_loss: 2.7005 - val_mae: 1.3960\n",
      "Epoch 33/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.4860 - mae: 1.3179\n",
      "Epoch 33: val_loss improved from 2.70045 to 2.59871, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 2.4860 - mae: 1.3179 - val_loss: 2.5987 - val_mae: 1.3639\n",
      "Epoch 34/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.4048 - mae: 1.2935\n",
      "Epoch 34: val_loss improved from 2.59871 to 2.50161, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 2.4048 - mae: 1.2935 - val_loss: 2.5016 - val_mae: 1.3326\n",
      "Epoch 35/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.3258 - mae: 1.2696\n",
      "Epoch 35: val_loss improved from 2.50161 to 2.40864, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 2.3258 - mae: 1.2696 - val_loss: 2.4086 - val_mae: 1.3024\n",
      "Epoch 36/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.2521 - mae: 1.2456\n",
      "Epoch 36: val_loss improved from 2.40864 to 2.31950, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 2.2521 - mae: 1.2456 - val_loss: 2.3195 - val_mae: 1.2737\n",
      "Epoch 37/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.1796 - mae: 1.2226\n",
      "Epoch 37: val_loss improved from 2.31950 to 2.23410, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2.1796 - mae: 1.2226 - val_loss: 2.2341 - val_mae: 1.2465\n",
      "Epoch 38/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.1100 - mae: 1.2006\n",
      "Epoch 38: val_loss improved from 2.23410 to 2.15253, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 2.1100 - mae: 1.2006 - val_loss: 2.1525 - val_mae: 1.2208\n",
      "Epoch 39/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.0383 - mae: 1.1771\n",
      "Epoch 39: val_loss improved from 2.15253 to 2.07504, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 2.0383 - mae: 1.1771 - val_loss: 2.0750 - val_mae: 1.1967\n",
      "Epoch 40/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.9700 - mae: 1.1548\n",
      "Epoch 40: val_loss improved from 2.07504 to 2.00193, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1.9700 - mae: 1.1548 - val_loss: 2.0019 - val_mae: 1.1739\n",
      "Epoch 41/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.9071 - mae: 1.1338\n",
      "Epoch 41: val_loss improved from 2.00193 to 1.93351, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 1.9071 - mae: 1.1338 - val_loss: 1.9335 - val_mae: 1.1522\n",
      "Epoch 42/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.8425 - mae: 1.1124\n",
      "Epoch 42: val_loss improved from 1.93351 to 1.87006, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1.8425 - mae: 1.1124 - val_loss: 1.8701 - val_mae: 1.1318\n",
      "Epoch 43/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.7859 - mae: 1.0940\n",
      "Epoch 43: val_loss improved from 1.87006 to 1.81180, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.7859 - mae: 1.0940 - val_loss: 1.8118 - val_mae: 1.1125\n",
      "Epoch 44/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.7296 - mae: 1.0750\n",
      "Epoch 44: val_loss improved from 1.81180 to 1.75883, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 1.7296 - mae: 1.0750 - val_loss: 1.7588 - val_mae: 1.0946\n",
      "Epoch 45/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.6786 - mae: 1.0573\n",
      "Epoch 45: val_loss improved from 1.75883 to 1.71117, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.6786 - mae: 1.0573 - val_loss: 1.7112 - val_mae: 1.0779\n",
      "Epoch 46/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.6317 - mae: 1.0413\n",
      "Epoch 46: val_loss improved from 1.71117 to 1.66876, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.6317 - mae: 1.0413 - val_loss: 1.6688 - val_mae: 1.0625\n",
      "Epoch 47/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.5878 - mae: 1.0261\n",
      "Epoch 47: val_loss improved from 1.66876 to 1.63146, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.5878 - mae: 1.0261 - val_loss: 1.6315 - val_mae: 1.0486\n",
      "Epoch 48/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.5471 - mae: 1.0111\n",
      "Epoch 48: val_loss improved from 1.63146 to 1.59907, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1.5471 - mae: 1.0111 - val_loss: 1.5991 - val_mae: 1.0362\n",
      "Epoch 49/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.5108 - mae: 0.9982\n",
      "Epoch 49: val_loss improved from 1.59907 to 1.57129, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1.5108 - mae: 0.9982 - val_loss: 1.5713 - val_mae: 1.0252\n",
      "Epoch 50/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.4798 - mae: 0.9868\n",
      "Epoch 50: val_loss improved from 1.57129 to 1.54765, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.4798 - mae: 0.9868 - val_loss: 1.5477 - val_mae: 1.0156\n",
      "Epoch 51/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.4542 - mae: 0.9773\n",
      "Epoch 51: val_loss improved from 1.54765 to 1.52755, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.4542 - mae: 0.9773 - val_loss: 1.5275 - val_mae: 1.0074\n",
      "Epoch 52/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.4316 - mae: 0.9685\n",
      "Epoch 52: val_loss improved from 1.52755 to 1.51022, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1.4316 - mae: 0.9685 - val_loss: 1.5102 - val_mae: 1.0002\n",
      "Epoch 53/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.4111 - mae: 0.9605\n",
      "Epoch 53: val_loss improved from 1.51022 to 1.49485, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.4111 - mae: 0.9605 - val_loss: 1.4948 - val_mae: 0.9939\n",
      "Epoch 54/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.3927 - mae: 0.9541\n",
      "Epoch 54: val_loss improved from 1.49485 to 1.48051, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.3927 - mae: 0.9541 - val_loss: 1.4805 - val_mae: 0.9881\n",
      "Epoch 55/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.3786 - mae: 0.9487\n",
      "Epoch 55: val_loss improved from 1.48051 to 1.46635, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1.3786 - mae: 0.9487 - val_loss: 1.4663 - val_mae: 0.9825\n",
      "Epoch 56/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.3633 - mae: 0.9433\n",
      "Epoch 56: val_loss improved from 1.46635 to 1.45153, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.3633 - mae: 0.9433 - val_loss: 1.4515 - val_mae: 0.9768\n",
      "Epoch 57/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.3488 - mae: 0.9382\n",
      "Epoch 57: val_loss improved from 1.45153 to 1.43537, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.3488 - mae: 0.9382 - val_loss: 1.4354 - val_mae: 0.9708\n",
      "Epoch 58/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.3321 - mae: 0.9322\n",
      "Epoch 58: val_loss improved from 1.43537 to 1.41736, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.3321 - mae: 0.9322 - val_loss: 1.4174 - val_mae: 0.9643\n",
      "Epoch 59/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.3164 - mae: 0.9266\n",
      "Epoch 59: val_loss improved from 1.41736 to 1.39716, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.3164 - mae: 0.9266 - val_loss: 1.3972 - val_mae: 0.9570\n",
      "Epoch 60/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.2996 - mae: 0.9207\n",
      "Epoch 60: val_loss improved from 1.39716 to 1.37464, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.2996 - mae: 0.9207 - val_loss: 1.3746 - val_mae: 0.9490\n",
      "Epoch 61/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.2800 - mae: 0.9131\n",
      "Epoch 61: val_loss improved from 1.37464 to 1.34985, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1.2800 - mae: 0.9131 - val_loss: 1.3499 - val_mae: 0.9401\n",
      "Epoch 62/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.2553 - mae: 0.9044\n",
      "Epoch 62: val_loss improved from 1.34985 to 1.32306, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.2553 - mae: 0.9044 - val_loss: 1.3231 - val_mae: 0.9306\n",
      "Epoch 63/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.2343 - mae: 0.8969\n",
      "Epoch 63: val_loss improved from 1.32306 to 1.29457, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1.2343 - mae: 0.8969 - val_loss: 1.2946 - val_mae: 0.9204\n",
      "Epoch 64/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.2109 - mae: 0.8881\n",
      "Epoch 64: val_loss improved from 1.29457 to 1.26480, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.2109 - mae: 0.8881 - val_loss: 1.2648 - val_mae: 0.9098\n",
      "Epoch 65/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.1875 - mae: 0.8796\n",
      "Epoch 65: val_loss improved from 1.26480 to 1.23423, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.1875 - mae: 0.8796 - val_loss: 1.2342 - val_mae: 0.8989\n",
      "Epoch 66/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1621 - mae: 0.8703\n",
      "Epoch 66: val_loss improved from 1.23423 to 1.20328, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.1621 - mae: 0.8703 - val_loss: 1.2033 - val_mae: 0.8878\n",
      "Epoch 67/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1372 - mae: 0.8614\n",
      "Epoch 67: val_loss improved from 1.20328 to 1.17233, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.1372 - mae: 0.8614 - val_loss: 1.1723 - val_mae: 0.8767\n",
      "Epoch 68/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1104 - mae: 0.8513\n",
      "Epoch 68: val_loss improved from 1.17233 to 1.14173, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.1104 - mae: 0.8513 - val_loss: 1.1417 - val_mae: 0.8657\n",
      "Epoch 69/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0853 - mae: 0.8419\n",
      "Epoch 69: val_loss improved from 1.14173 to 1.11171, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.0853 - mae: 0.8419 - val_loss: 1.1117 - val_mae: 0.8549\n",
      "Epoch 70/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0628 - mae: 0.8332\n",
      "Epoch 70: val_loss improved from 1.11171 to 1.08242, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.0628 - mae: 0.8332 - val_loss: 1.0824 - val_mae: 0.8442\n",
      "Epoch 71/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0371 - mae: 0.8233\n",
      "Epoch 71: val_loss improved from 1.08242 to 1.05394, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.0371 - mae: 0.8233 - val_loss: 1.0539 - val_mae: 0.8337\n",
      "Epoch 72/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.0157 - mae: 0.8152\n",
      "Epoch 72: val_loss improved from 1.05394 to 1.02628, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.0157 - mae: 0.8152 - val_loss: 1.0263 - val_mae: 0.8234\n",
      "Epoch 73/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.9917 - mae: 0.8055\n",
      "Epoch 73: val_loss improved from 1.02628 to 0.99939, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.9917 - mae: 0.8055 - val_loss: 0.9994 - val_mae: 0.8132\n",
      "Epoch 74/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.9696 - mae: 0.7968\n",
      "Epoch 74: val_loss improved from 0.99939 to 0.97321, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.9696 - mae: 0.7968 - val_loss: 0.9732 - val_mae: 0.8031\n",
      "Epoch 75/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.9449 - mae: 0.7866\n",
      "Epoch 75: val_loss improved from 0.97321 to 0.94770, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.9449 - mae: 0.7866 - val_loss: 0.9477 - val_mae: 0.7931\n",
      "Epoch 76/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.9254 - mae: 0.7789\n",
      "Epoch 76: val_loss improved from 0.94770 to 0.92277, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.9254 - mae: 0.7789 - val_loss: 0.9228 - val_mae: 0.7830\n",
      "Epoch 77/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.9033 - mae: 0.7691\n",
      "Epoch 77: val_loss improved from 0.92277 to 0.89838, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.9033 - mae: 0.7691 - val_loss: 0.8984 - val_mae: 0.7730\n",
      "Epoch 78/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.8821 - mae: 0.7603\n",
      "Epoch 78: val_loss improved from 0.89838 to 0.87450, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.8821 - mae: 0.7603 - val_loss: 0.8745 - val_mae: 0.7630\n",
      "Epoch 79/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8611 - mae: 0.7511\n",
      "Epoch 79: val_loss improved from 0.87450 to 0.85113, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8611 - mae: 0.7511 - val_loss: 0.8511 - val_mae: 0.7529\n",
      "Epoch 80/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.8374 - mae: 0.7402\n",
      "Epoch 80: val_loss improved from 0.85113 to 0.82829, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8374 - mae: 0.7402 - val_loss: 0.8283 - val_mae: 0.7429\n",
      "Epoch 81/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.8206 - mae: 0.7326\n",
      "Epoch 81: val_loss improved from 0.82829 to 0.80596, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.8206 - mae: 0.7326 - val_loss: 0.8060 - val_mae: 0.7329\n",
      "Epoch 82/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.7988 - mae: 0.7224\n",
      "Epoch 82: val_loss improved from 0.80596 to 0.78415, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.7988 - mae: 0.7224 - val_loss: 0.7842 - val_mae: 0.7230\n",
      "Epoch 83/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7782 - mae: 0.7127\n",
      "Epoch 83: val_loss improved from 0.78415 to 0.76287, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.7782 - mae: 0.7127 - val_loss: 0.7629 - val_mae: 0.7131\n",
      "Epoch 84/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7572 - mae: 0.7026\n",
      "Epoch 84: val_loss improved from 0.76287 to 0.74209, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.7572 - mae: 0.7026 - val_loss: 0.7421 - val_mae: 0.7033\n",
      "Epoch 85/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.7407 - mae: 0.6946\n",
      "Epoch 85: val_loss improved from 0.74209 to 0.72177, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.7407 - mae: 0.6946 - val_loss: 0.7218 - val_mae: 0.6936\n",
      "Epoch 86/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.7208 - mae: 0.6853\n",
      "Epoch 86: val_loss improved from 0.72177 to 0.70188, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.7208 - mae: 0.6853 - val_loss: 0.7019 - val_mae: 0.6839\n",
      "Epoch 87/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.7034 - mae: 0.6760\n",
      "Epoch 87: val_loss improved from 0.70188 to 0.68238, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.7034 - mae: 0.6760 - val_loss: 0.6824 - val_mae: 0.6742\n",
      "Epoch 88/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.6851 - mae: 0.6671\n",
      "Epoch 88: val_loss improved from 0.68238 to 0.66321, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.6851 - mae: 0.6671 - val_loss: 0.6632 - val_mae: 0.6645\n",
      "Epoch 89/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6687 - mae: 0.6586\n",
      "Epoch 89: val_loss improved from 0.66321 to 0.64435, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.6687 - mae: 0.6586 - val_loss: 0.6444 - val_mae: 0.6547\n",
      "Epoch 90/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6503 - mae: 0.6486\n",
      "Epoch 90: val_loss improved from 0.64435 to 0.62581, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.6503 - mae: 0.6486 - val_loss: 0.6258 - val_mae: 0.6450\n",
      "Epoch 91/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6343 - mae: 0.6405\n",
      "Epoch 91: val_loss improved from 0.62581 to 0.60758, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.6343 - mae: 0.6405 - val_loss: 0.6076 - val_mae: 0.6351\n",
      "Epoch 92/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6175 - mae: 0.6308\n",
      "Epoch 92: val_loss improved from 0.60758 to 0.58969, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.6175 - mae: 0.6308 - val_loss: 0.5897 - val_mae: 0.6253\n",
      "Epoch 93/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6009 - mae: 0.6214\n",
      "Epoch 93: val_loss improved from 0.58969 to 0.57219, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.6009 - mae: 0.6214 - val_loss: 0.5722 - val_mae: 0.6154\n",
      "Epoch 94/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.5854 - mae: 0.6128\n",
      "Epoch 94: val_loss improved from 0.57219 to 0.55512, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.5854 - mae: 0.6128 - val_loss: 0.5551 - val_mae: 0.6056\n",
      "Epoch 95/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5692 - mae: 0.6036\n",
      "Epoch 95: val_loss improved from 0.55512 to 0.53852, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.5692 - mae: 0.6036 - val_loss: 0.5385 - val_mae: 0.5959\n",
      "Epoch 96/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5540 - mae: 0.5943\n",
      "Epoch 96: val_loss improved from 0.53852 to 0.52243, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.5540 - mae: 0.5943 - val_loss: 0.5224 - val_mae: 0.5862\n",
      "Epoch 97/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.5399 - mae: 0.5857\n",
      "Epoch 97: val_loss improved from 0.52243 to 0.50685, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.5399 - mae: 0.5857 - val_loss: 0.5068 - val_mae: 0.5767\n",
      "Epoch 98/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.5271 - mae: 0.5783\n",
      "Epoch 98: val_loss improved from 0.50685 to 0.49176, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.5271 - mae: 0.5783 - val_loss: 0.4918 - val_mae: 0.5674\n",
      "Epoch 99/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5147 - mae: 0.5707\n",
      "Epoch 99: val_loss improved from 0.49176 to 0.47718, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.5147 - mae: 0.5707 - val_loss: 0.4772 - val_mae: 0.5582\n",
      "Epoch 100/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5000 - mae: 0.5612\n",
      "Epoch 100: val_loss improved from 0.47718 to 0.46308, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.5000 - mae: 0.5612 - val_loss: 0.4631 - val_mae: 0.5492\n",
      "Epoch 101/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4871 - mae: 0.5537\n",
      "Epoch 101: val_loss improved from 0.46308 to 0.44947, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.4871 - mae: 0.5537 - val_loss: 0.4495 - val_mae: 0.5404\n",
      "Epoch 102/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4766 - mae: 0.5465\n",
      "Epoch 102: val_loss improved from 0.44947 to 0.43635, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.4766 - mae: 0.5465 - val_loss: 0.4363 - val_mae: 0.5318\n",
      "Epoch 103/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.4656 - mae: 0.5393\n",
      "Epoch 103: val_loss improved from 0.43635 to 0.42373, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.4656 - mae: 0.5393 - val_loss: 0.4237 - val_mae: 0.5234\n",
      "Epoch 104/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4549 - mae: 0.5321\n",
      "Epoch 104: val_loss improved from 0.42373 to 0.41164, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.4549 - mae: 0.5321 - val_loss: 0.4116 - val_mae: 0.5153\n",
      "Epoch 105/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4438 - mae: 0.5249\n",
      "Epoch 105: val_loss improved from 0.41164 to 0.40006, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.4438 - mae: 0.5249 - val_loss: 0.4001 - val_mae: 0.5075\n",
      "Epoch 106/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4327 - mae: 0.5176\n",
      "Epoch 106: val_loss improved from 0.40006 to 0.38901, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.4327 - mae: 0.5176 - val_loss: 0.3890 - val_mae: 0.5000\n",
      "Epoch 107/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4244 - mae: 0.5118\n",
      "Epoch 107: val_loss improved from 0.38901 to 0.37848, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.4244 - mae: 0.5118 - val_loss: 0.3785 - val_mae: 0.4928\n",
      "Epoch 108/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4148 - mae: 0.5054\n",
      "Epoch 108: val_loss improved from 0.37848 to 0.36843, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.4148 - mae: 0.5054 - val_loss: 0.3684 - val_mae: 0.4858\n",
      "Epoch 109/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4060 - mae: 0.4985\n",
      "Epoch 109: val_loss improved from 0.36843 to 0.35884, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.4060 - mae: 0.4985 - val_loss: 0.3588 - val_mae: 0.4792\n",
      "Epoch 110/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.3991 - mae: 0.4942\n",
      "Epoch 110: val_loss improved from 0.35884 to 0.34967, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3991 - mae: 0.4942 - val_loss: 0.3497 - val_mae: 0.4728\n",
      "Epoch 111/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3914 - mae: 0.4888\n",
      "Epoch 111: val_loss improved from 0.34967 to 0.34088, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3914 - mae: 0.4888 - val_loss: 0.3409 - val_mae: 0.4666\n",
      "Epoch 112/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.3826 - mae: 0.4827\n",
      "Epoch 112: val_loss improved from 0.34088 to 0.33246, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.3826 - mae: 0.4827 - val_loss: 0.3325 - val_mae: 0.4606\n",
      "Epoch 113/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3751 - mae: 0.4775\n",
      "Epoch 113: val_loss improved from 0.33246 to 0.32440, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.3751 - mae: 0.4775 - val_loss: 0.3244 - val_mae: 0.4548\n",
      "Epoch 114/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3698 - mae: 0.4733\n",
      "Epoch 114: val_loss improved from 0.32440 to 0.31669, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3698 - mae: 0.4733 - val_loss: 0.3167 - val_mae: 0.4493\n",
      "Epoch 115/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3617 - mae: 0.4676\n",
      "Epoch 115: val_loss improved from 0.31669 to 0.30936, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3617 - mae: 0.4676 - val_loss: 0.3094 - val_mae: 0.4440\n",
      "Epoch 116/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3572 - mae: 0.4645\n",
      "Epoch 116: val_loss improved from 0.30936 to 0.30239, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.3572 - mae: 0.4645 - val_loss: 0.3024 - val_mae: 0.4389\n",
      "Epoch 117/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3509 - mae: 0.4600\n",
      "Epoch 117: val_loss improved from 0.30239 to 0.29578, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.3509 - mae: 0.4600 - val_loss: 0.2958 - val_mae: 0.4340\n",
      "Epoch 118/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3454 - mae: 0.4560\n",
      "Epoch 118: val_loss improved from 0.29578 to 0.28954, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3454 - mae: 0.4560 - val_loss: 0.2895 - val_mae: 0.4294\n",
      "Epoch 119/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3406 - mae: 0.4527\n",
      "Epoch 119: val_loss improved from 0.28954 to 0.28364, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.3406 - mae: 0.4527 - val_loss: 0.2836 - val_mae: 0.4250\n",
      "Epoch 120/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3343 - mae: 0.4478\n",
      "Epoch 120: val_loss improved from 0.28364 to 0.27809, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3343 - mae: 0.4478 - val_loss: 0.2781 - val_mae: 0.4209\n",
      "Epoch 121/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3298 - mae: 0.4450\n",
      "Epoch 121: val_loss improved from 0.27809 to 0.27286, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.3298 - mae: 0.4450 - val_loss: 0.2729 - val_mae: 0.4170\n",
      "Epoch 122/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3248 - mae: 0.4412\n",
      "Epoch 122: val_loss improved from 0.27286 to 0.26794, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.3248 - mae: 0.4412 - val_loss: 0.2679 - val_mae: 0.4132\n",
      "Epoch 123/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3237 - mae: 0.4395\n",
      "Epoch 123: val_loss improved from 0.26794 to 0.26332, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.3237 - mae: 0.4395 - val_loss: 0.2633 - val_mae: 0.4097\n",
      "Epoch 124/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3190 - mae: 0.4361\n",
      "Epoch 124: val_loss improved from 0.26332 to 0.25899, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.3190 - mae: 0.4361 - val_loss: 0.2590 - val_mae: 0.4064\n",
      "Epoch 125/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3161 - mae: 0.4333\n",
      "Epoch 125: val_loss improved from 0.25899 to 0.25491, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3161 - mae: 0.4333 - val_loss: 0.2549 - val_mae: 0.4033\n",
      "Epoch 126/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3116 - mae: 0.4302\n",
      "Epoch 126: val_loss improved from 0.25491 to 0.25108, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.3116 - mae: 0.4302 - val_loss: 0.2511 - val_mae: 0.4003\n",
      "Epoch 127/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3069 - mae: 0.4270\n",
      "Epoch 127: val_loss improved from 0.25108 to 0.24747, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3069 - mae: 0.4270 - val_loss: 0.2475 - val_mae: 0.3974\n",
      "Epoch 128/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3060 - mae: 0.4257\n",
      "Epoch 128: val_loss improved from 0.24747 to 0.24406, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.3060 - mae: 0.4257 - val_loss: 0.2441 - val_mae: 0.3947\n",
      "Epoch 129/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3033 - mae: 0.4235\n",
      "Epoch 129: val_loss improved from 0.24406 to 0.24085, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.3033 - mae: 0.4235 - val_loss: 0.2408 - val_mae: 0.3921\n",
      "Epoch 130/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2983 - mae: 0.4205\n",
      "Epoch 130: val_loss improved from 0.24085 to 0.23781, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2983 - mae: 0.4205 - val_loss: 0.2378 - val_mae: 0.3896\n",
      "Epoch 131/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2998 - mae: 0.4208\n",
      "Epoch 131: val_loss improved from 0.23781 to 0.23494, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2998 - mae: 0.4208 - val_loss: 0.2349 - val_mae: 0.3873\n",
      "Epoch 132/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2956 - mae: 0.4178\n",
      "Epoch 132: val_loss improved from 0.23494 to 0.23224, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2956 - mae: 0.4178 - val_loss: 0.2322 - val_mae: 0.3850\n",
      "Epoch 133/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2931 - mae: 0.4160\n",
      "Epoch 133: val_loss improved from 0.23224 to 0.22969, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2931 - mae: 0.4160 - val_loss: 0.2297 - val_mae: 0.3829\n",
      "Epoch 134/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2902 - mae: 0.4138\n",
      "Epoch 134: val_loss improved from 0.22969 to 0.22728, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2902 - mae: 0.4138 - val_loss: 0.2273 - val_mae: 0.3809\n",
      "Epoch 135/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2881 - mae: 0.4120\n",
      "Epoch 135: val_loss improved from 0.22728 to 0.22501, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.2881 - mae: 0.4120 - val_loss: 0.2250 - val_mae: 0.3790\n",
      "Epoch 136/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2867 - mae: 0.4112\n",
      "Epoch 136: val_loss improved from 0.22501 to 0.22287, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2867 - mae: 0.4112 - val_loss: 0.2229 - val_mae: 0.3771\n",
      "Epoch 137/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2858 - mae: 0.4103\n",
      "Epoch 137: val_loss improved from 0.22287 to 0.22086, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2858 - mae: 0.4103 - val_loss: 0.2209 - val_mae: 0.3754\n",
      "Epoch 138/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2838 - mae: 0.4087\n",
      "Epoch 138: val_loss improved from 0.22086 to 0.21898, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.2838 - mae: 0.4087 - val_loss: 0.2190 - val_mae: 0.3738\n",
      "Epoch 139/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2803 - mae: 0.4067\n",
      "Epoch 139: val_loss improved from 0.21898 to 0.21722, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2803 - mae: 0.4067 - val_loss: 0.2172 - val_mae: 0.3723\n",
      "Epoch 140/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2799 - mae: 0.4062\n",
      "Epoch 140: val_loss improved from 0.21722 to 0.21557, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2799 - mae: 0.4062 - val_loss: 0.2156 - val_mae: 0.3709\n",
      "Epoch 141/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2799 - mae: 0.4059\n",
      "Epoch 141: val_loss improved from 0.21557 to 0.21402, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2799 - mae: 0.4059 - val_loss: 0.2140 - val_mae: 0.3695\n",
      "Epoch 142/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2748 - mae: 0.4030\n",
      "Epoch 142: val_loss improved from 0.21402 to 0.21257, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.2748 - mae: 0.4030 - val_loss: 0.2126 - val_mae: 0.3683\n",
      "Epoch 143/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2753 - mae: 0.4028\n",
      "Epoch 143: val_loss improved from 0.21257 to 0.21121, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2753 - mae: 0.4028 - val_loss: 0.2112 - val_mae: 0.3671\n",
      "Epoch 144/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2742 - mae: 0.4023\n",
      "Epoch 144: val_loss improved from 0.21121 to 0.20992, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2742 - mae: 0.4023 - val_loss: 0.2099 - val_mae: 0.3659\n",
      "Epoch 145/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2743 - mae: 0.4022\n",
      "Epoch 145: val_loss improved from 0.20992 to 0.20872, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2743 - mae: 0.4022 - val_loss: 0.2087 - val_mae: 0.3648\n",
      "Epoch 146/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2740 - mae: 0.4016\n",
      "Epoch 146: val_loss improved from 0.20872 to 0.20760, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2740 - mae: 0.4016 - val_loss: 0.2076 - val_mae: 0.3638\n",
      "Epoch 147/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2713 - mae: 0.3997\n",
      "Epoch 147: val_loss improved from 0.20760 to 0.20655, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2713 - mae: 0.3997 - val_loss: 0.2065 - val_mae: 0.3629\n",
      "Epoch 148/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2713 - mae: 0.3996\n",
      "Epoch 148: val_loss improved from 0.20655 to 0.20557, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2713 - mae: 0.3996 - val_loss: 0.2056 - val_mae: 0.3620\n",
      "Epoch 149/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2700 - mae: 0.3987\n",
      "Epoch 149: val_loss improved from 0.20557 to 0.20467, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2700 - mae: 0.3987 - val_loss: 0.2047 - val_mae: 0.3611\n",
      "Epoch 150/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2676 - mae: 0.3977\n",
      "Epoch 150: val_loss improved from 0.20467 to 0.20382, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2038 - val_mae: 0.3603\n",
      "Epoch 151/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2673 - mae: 0.3967\n",
      "Epoch 151: val_loss improved from 0.20382 to 0.20305, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2673 - mae: 0.3967 - val_loss: 0.2030 - val_mae: 0.3596\n",
      "Epoch 152/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2676 - mae: 0.3968\n",
      "Epoch 152: val_loss improved from 0.20305 to 0.20233, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2676 - mae: 0.3968 - val_loss: 0.2023 - val_mae: 0.3589\n",
      "Epoch 153/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2652 - mae: 0.3952\n",
      "Epoch 153: val_loss improved from 0.20233 to 0.20166, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.2652 - mae: 0.3952 - val_loss: 0.2017 - val_mae: 0.3583\n",
      "Epoch 154/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2647 - mae: 0.3950\n",
      "Epoch 154: val_loss improved from 0.20166 to 0.20105, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2647 - mae: 0.3950 - val_loss: 0.2011 - val_mae: 0.3577\n",
      "Epoch 155/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2625 - mae: 0.3934\n",
      "Epoch 155: val_loss improved from 0.20105 to 0.20048, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2625 - mae: 0.3934 - val_loss: 0.2005 - val_mae: 0.3571\n",
      "Epoch 156/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2637 - mae: 0.3940\n",
      "Epoch 156: val_loss improved from 0.20048 to 0.19995, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2637 - mae: 0.3940 - val_loss: 0.1999 - val_mae: 0.3566\n",
      "Epoch 157/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2613 - mae: 0.3927\n",
      "Epoch 157: val_loss improved from 0.19995 to 0.19945, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2613 - mae: 0.3927 - val_loss: 0.1994 - val_mae: 0.3561\n",
      "Epoch 158/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2608 - mae: 0.3920\n",
      "Epoch 158: val_loss improved from 0.19945 to 0.19897, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.2608 - mae: 0.3920 - val_loss: 0.1990 - val_mae: 0.3556\n",
      "Epoch 159/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2588 - mae: 0.3905\n",
      "Epoch 159: val_loss improved from 0.19897 to 0.19852, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2588 - mae: 0.3905 - val_loss: 0.1985 - val_mae: 0.3552\n",
      "Epoch 160/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2607 - mae: 0.3918\n",
      "Epoch 160: val_loss improved from 0.19852 to 0.19808, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2607 - mae: 0.3918 - val_loss: 0.1981 - val_mae: 0.3547\n",
      "Epoch 161/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2590 - mae: 0.3908\n",
      "Epoch 161: val_loss improved from 0.19808 to 0.19766, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2590 - mae: 0.3908 - val_loss: 0.1977 - val_mae: 0.3543\n",
      "Epoch 162/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2581 - mae: 0.3901\n",
      "Epoch 162: val_loss improved from 0.19766 to 0.19724, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2581 - mae: 0.3901 - val_loss: 0.1972 - val_mae: 0.3539\n",
      "Epoch 163/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2582 - mae: 0.3897\n",
      "Epoch 163: val_loss improved from 0.19724 to 0.19683, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2582 - mae: 0.3897 - val_loss: 0.1968 - val_mae: 0.3534\n",
      "Epoch 164/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2574 - mae: 0.3893\n",
      "Epoch 164: val_loss improved from 0.19683 to 0.19644, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2574 - mae: 0.3893 - val_loss: 0.1964 - val_mae: 0.3530\n",
      "Epoch 165/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2557 - mae: 0.3883\n",
      "Epoch 165: val_loss improved from 0.19644 to 0.19606, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2557 - mae: 0.3883 - val_loss: 0.1961 - val_mae: 0.3527\n",
      "Epoch 166/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2559 - mae: 0.3882\n",
      "Epoch 166: val_loss improved from 0.19606 to 0.19570, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2559 - mae: 0.3882 - val_loss: 0.1957 - val_mae: 0.3523\n",
      "Epoch 167/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2547 - mae: 0.3877\n",
      "Epoch 167: val_loss improved from 0.19570 to 0.19534, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2547 - mae: 0.3877 - val_loss: 0.1953 - val_mae: 0.3519\n",
      "Epoch 168/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2541 - mae: 0.3873\n",
      "Epoch 168: val_loss improved from 0.19534 to 0.19501, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2541 - mae: 0.3873 - val_loss: 0.1950 - val_mae: 0.3515\n",
      "Epoch 169/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2545 - mae: 0.3877\n",
      "Epoch 169: val_loss improved from 0.19501 to 0.19467, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2545 - mae: 0.3877 - val_loss: 0.1947 - val_mae: 0.3512\n",
      "Epoch 170/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2547 - mae: 0.3877\n",
      "Epoch 170: val_loss improved from 0.19467 to 0.19435, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.2547 - mae: 0.3877 - val_loss: 0.1944 - val_mae: 0.3508\n",
      "Epoch 171/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2537 - mae: 0.3869\n",
      "Epoch 171: val_loss improved from 0.19435 to 0.19402, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2537 - mae: 0.3869 - val_loss: 0.1940 - val_mae: 0.3505\n",
      "Epoch 172/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2526 - mae: 0.3862\n",
      "Epoch 172: val_loss improved from 0.19402 to 0.19367, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2526 - mae: 0.3862 - val_loss: 0.1937 - val_mae: 0.3501\n",
      "Epoch 173/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2531 - mae: 0.3862\n",
      "Epoch 173: val_loss improved from 0.19367 to 0.19333, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2531 - mae: 0.3862 - val_loss: 0.1933 - val_mae: 0.3498\n",
      "Epoch 174/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2492 - mae: 0.3839\n",
      "Epoch 174: val_loss improved from 0.19333 to 0.19299, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2492 - mae: 0.3839 - val_loss: 0.1930 - val_mae: 0.3494\n",
      "Epoch 175/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2502 - mae: 0.3844\n",
      "Epoch 175: val_loss improved from 0.19299 to 0.19266, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2502 - mae: 0.3844 - val_loss: 0.1927 - val_mae: 0.3491\n",
      "Epoch 176/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2490 - mae: 0.3836\n",
      "Epoch 176: val_loss improved from 0.19266 to 0.19232, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2490 - mae: 0.3836 - val_loss: 0.1923 - val_mae: 0.3487\n",
      "Epoch 177/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2491 - mae: 0.3836\n",
      "Epoch 177: val_loss improved from 0.19232 to 0.19199, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2491 - mae: 0.3836 - val_loss: 0.1920 - val_mae: 0.3484\n",
      "Epoch 178/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2479 - mae: 0.3832\n",
      "Epoch 178: val_loss improved from 0.19199 to 0.19165, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2479 - mae: 0.3832 - val_loss: 0.1917 - val_mae: 0.3481\n",
      "Epoch 179/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2484 - mae: 0.3830\n",
      "Epoch 179: val_loss improved from 0.19165 to 0.19132, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2484 - mae: 0.3830 - val_loss: 0.1913 - val_mae: 0.3477\n",
      "Epoch 180/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2466 - mae: 0.3822\n",
      "Epoch 180: val_loss improved from 0.19132 to 0.19100, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2466 - mae: 0.3822 - val_loss: 0.1910 - val_mae: 0.3474\n",
      "Epoch 181/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2466 - mae: 0.3823\n",
      "Epoch 181: val_loss improved from 0.19100 to 0.19067, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2466 - mae: 0.3823 - val_loss: 0.1907 - val_mae: 0.3471\n",
      "Epoch 182/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2476 - mae: 0.3825\n",
      "Epoch 182: val_loss improved from 0.19067 to 0.19035, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.2476 - mae: 0.3825 - val_loss: 0.1903 - val_mae: 0.3467\n",
      "Epoch 183/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2453 - mae: 0.3812\n",
      "Epoch 183: val_loss improved from 0.19035 to 0.19002, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2453 - mae: 0.3812 - val_loss: 0.1900 - val_mae: 0.3464\n",
      "Epoch 184/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2456 - mae: 0.3814\n",
      "Epoch 184: val_loss improved from 0.19002 to 0.18972, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.2456 - mae: 0.3814 - val_loss: 0.1897 - val_mae: 0.3461\n",
      "Epoch 185/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2451 - mae: 0.3811\n",
      "Epoch 185: val_loss improved from 0.18972 to 0.18943, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2451 - mae: 0.3811 - val_loss: 0.1894 - val_mae: 0.3458\n",
      "Epoch 186/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2440 - mae: 0.3802\n",
      "Epoch 186: val_loss improved from 0.18943 to 0.18916, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2440 - mae: 0.3802 - val_loss: 0.1892 - val_mae: 0.3456\n",
      "Epoch 187/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2448 - mae: 0.3801\n",
      "Epoch 187: val_loss improved from 0.18916 to 0.18889, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2448 - mae: 0.3801 - val_loss: 0.1889 - val_mae: 0.3453\n",
      "Epoch 188/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2415 - mae: 0.3789\n",
      "Epoch 188: val_loss improved from 0.18889 to 0.18862, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2415 - mae: 0.3789 - val_loss: 0.1886 - val_mae: 0.3450\n",
      "Epoch 189/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2411 - mae: 0.3784\n",
      "Epoch 189: val_loss improved from 0.18862 to 0.18835, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2411 - mae: 0.3784 - val_loss: 0.1883 - val_mae: 0.3447\n",
      "Epoch 190/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2421 - mae: 0.3787\n",
      "Epoch 190: val_loss improved from 0.18835 to 0.18808, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2421 - mae: 0.3787 - val_loss: 0.1881 - val_mae: 0.3445\n",
      "Epoch 191/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2427 - mae: 0.3789\n",
      "Epoch 191: val_loss improved from 0.18808 to 0.18781, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2427 - mae: 0.3789 - val_loss: 0.1878 - val_mae: 0.3442\n",
      "Epoch 192/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2415 - mae: 0.3786\n",
      "Epoch 192: val_loss improved from 0.18781 to 0.18753, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2415 - mae: 0.3786 - val_loss: 0.1875 - val_mae: 0.3439\n",
      "Epoch 193/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2422 - mae: 0.3786\n",
      "Epoch 193: val_loss improved from 0.18753 to 0.18723, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2422 - mae: 0.3786 - val_loss: 0.1872 - val_mae: 0.3437\n",
      "Epoch 194/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2409 - mae: 0.3777\n",
      "Epoch 194: val_loss improved from 0.18723 to 0.18694, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2409 - mae: 0.3777 - val_loss: 0.1869 - val_mae: 0.3434\n",
      "Epoch 195/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2402 - mae: 0.3777\n",
      "Epoch 195: val_loss improved from 0.18694 to 0.18665, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2402 - mae: 0.3777 - val_loss: 0.1867 - val_mae: 0.3431\n",
      "Epoch 196/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2395 - mae: 0.3772\n",
      "Epoch 196: val_loss improved from 0.18665 to 0.18637, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2395 - mae: 0.3772 - val_loss: 0.1864 - val_mae: 0.3429\n",
      "Epoch 197/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2412 - mae: 0.3783\n",
      "Epoch 197: val_loss improved from 0.18637 to 0.18608, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2412 - mae: 0.3783 - val_loss: 0.1861 - val_mae: 0.3426\n",
      "Epoch 198/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2379 - mae: 0.3759\n",
      "Epoch 198: val_loss improved from 0.18608 to 0.18582, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2379 - mae: 0.3759 - val_loss: 0.1858 - val_mae: 0.3423\n",
      "Epoch 199/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2386 - mae: 0.3764\n",
      "Epoch 199: val_loss improved from 0.18582 to 0.18558, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2386 - mae: 0.3764 - val_loss: 0.1856 - val_mae: 0.3421\n",
      "Epoch 200/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2383 - mae: 0.3761\n",
      "Epoch 200: val_loss improved from 0.18558 to 0.18537, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2383 - mae: 0.3761 - val_loss: 0.1854 - val_mae: 0.3419\n",
      "Epoch 201/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2377 - mae: 0.3759\n",
      "Epoch 201: val_loss improved from 0.18537 to 0.18516, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2377 - mae: 0.3759 - val_loss: 0.1852 - val_mae: 0.3416\n",
      "Epoch 202/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2384 - mae: 0.3760\n",
      "Epoch 202: val_loss improved from 0.18516 to 0.18496, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2384 - mae: 0.3760 - val_loss: 0.1850 - val_mae: 0.3414\n",
      "Epoch 203/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2369 - mae: 0.3752\n",
      "Epoch 203: val_loss improved from 0.18496 to 0.18475, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2369 - mae: 0.3752 - val_loss: 0.1847 - val_mae: 0.3412\n",
      "Epoch 204/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2347 - mae: 0.3740\n",
      "Epoch 204: val_loss improved from 0.18475 to 0.18453, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2347 - mae: 0.3740 - val_loss: 0.1845 - val_mae: 0.3410\n",
      "Epoch 205/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2361 - mae: 0.3745\n",
      "Epoch 205: val_loss improved from 0.18453 to 0.18431, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.2361 - mae: 0.3745 - val_loss: 0.1843 - val_mae: 0.3407\n",
      "Epoch 206/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2354 - mae: 0.3741\n",
      "Epoch 206: val_loss improved from 0.18431 to 0.18406, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2354 - mae: 0.3741 - val_loss: 0.1841 - val_mae: 0.3405\n",
      "Epoch 207/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2355 - mae: 0.3741\n",
      "Epoch 207: val_loss improved from 0.18406 to 0.18381, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2355 - mae: 0.3741 - val_loss: 0.1838 - val_mae: 0.3402\n",
      "Epoch 208/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2352 - mae: 0.3736\n",
      "Epoch 208: val_loss improved from 0.18381 to 0.18354, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2352 - mae: 0.3736 - val_loss: 0.1835 - val_mae: 0.3400\n",
      "Epoch 209/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2339 - mae: 0.3729\n",
      "Epoch 209: val_loss improved from 0.18354 to 0.18326, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2339 - mae: 0.3729 - val_loss: 0.1833 - val_mae: 0.3397\n",
      "Epoch 210/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2344 - mae: 0.3733\n",
      "Epoch 210: val_loss improved from 0.18326 to 0.18297, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2344 - mae: 0.3733 - val_loss: 0.1830 - val_mae: 0.3394\n",
      "Epoch 211/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2345 - mae: 0.3735\n",
      "Epoch 211: val_loss improved from 0.18297 to 0.18270, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2345 - mae: 0.3735 - val_loss: 0.1827 - val_mae: 0.3392\n",
      "Epoch 212/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2332 - mae: 0.3728\n",
      "Epoch 212: val_loss improved from 0.18270 to 0.18244, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2332 - mae: 0.3728 - val_loss: 0.1824 - val_mae: 0.3389\n",
      "Epoch 213/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2322 - mae: 0.3718\n",
      "Epoch 213: val_loss improved from 0.18244 to 0.18219, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2322 - mae: 0.3718 - val_loss: 0.1822 - val_mae: 0.3387\n",
      "Epoch 214/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2328 - mae: 0.3724\n",
      "Epoch 214: val_loss improved from 0.18219 to 0.18198, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.2328 - mae: 0.3724 - val_loss: 0.1820 - val_mae: 0.3385\n",
      "Epoch 215/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2316 - mae: 0.3713\n",
      "Epoch 215: val_loss improved from 0.18198 to 0.18180, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.2316 - mae: 0.3713 - val_loss: 0.1818 - val_mae: 0.3383\n",
      "Epoch 216/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2318 - mae: 0.3715\n",
      "Epoch 216: val_loss improved from 0.18180 to 0.18164, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2318 - mae: 0.3715 - val_loss: 0.1816 - val_mae: 0.3381\n",
      "Epoch 217/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2327 - mae: 0.3723\n",
      "Epoch 217: val_loss improved from 0.18164 to 0.18148, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2327 - mae: 0.3723 - val_loss: 0.1815 - val_mae: 0.3379\n",
      "Epoch 218/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2304 - mae: 0.3707\n",
      "Epoch 218: val_loss improved from 0.18148 to 0.18131, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2304 - mae: 0.3707 - val_loss: 0.1813 - val_mae: 0.3378\n",
      "Epoch 219/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2308 - mae: 0.3708\n",
      "Epoch 219: val_loss improved from 0.18131 to 0.18113, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2308 - mae: 0.3708 - val_loss: 0.1811 - val_mae: 0.3376\n",
      "Epoch 220/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2302 - mae: 0.3701\n",
      "Epoch 220: val_loss improved from 0.18113 to 0.18093, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2302 - mae: 0.3701 - val_loss: 0.1809 - val_mae: 0.3374\n",
      "Epoch 221/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2296 - mae: 0.3697\n",
      "Epoch 221: val_loss improved from 0.18093 to 0.18074, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2296 - mae: 0.3697 - val_loss: 0.1807 - val_mae: 0.3372\n",
      "Epoch 222/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2284 - mae: 0.3694\n",
      "Epoch 222: val_loss improved from 0.18074 to 0.18057, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2284 - mae: 0.3694 - val_loss: 0.1806 - val_mae: 0.3370\n",
      "Epoch 223/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2284 - mae: 0.3693\n",
      "Epoch 223: val_loss improved from 0.18057 to 0.18037, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2284 - mae: 0.3693 - val_loss: 0.1804 - val_mae: 0.3368\n",
      "Epoch 224/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2294 - mae: 0.3696\n",
      "Epoch 224: val_loss improved from 0.18037 to 0.18018, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.2294 - mae: 0.3696 - val_loss: 0.1802 - val_mae: 0.3366\n",
      "Epoch 225/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2293 - mae: 0.3696\n",
      "Epoch 225: val_loss improved from 0.18018 to 0.17996, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2293 - mae: 0.3696 - val_loss: 0.1800 - val_mae: 0.3364\n",
      "Epoch 226/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2273 - mae: 0.3684\n",
      "Epoch 226: val_loss improved from 0.17996 to 0.17974, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2273 - mae: 0.3684 - val_loss: 0.1797 - val_mae: 0.3362\n",
      "Epoch 227/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2285 - mae: 0.3689\n",
      "Epoch 227: val_loss improved from 0.17974 to 0.17952, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2285 - mae: 0.3689 - val_loss: 0.1795 - val_mae: 0.3360\n",
      "Epoch 228/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2273 - mae: 0.3685\n",
      "Epoch 228: val_loss improved from 0.17952 to 0.17930, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2273 - mae: 0.3685 - val_loss: 0.1793 - val_mae: 0.3358\n",
      "Epoch 229/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2270 - mae: 0.3682\n",
      "Epoch 229: val_loss improved from 0.17930 to 0.17910, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2270 - mae: 0.3682 - val_loss: 0.1791 - val_mae: 0.3356\n",
      "Epoch 230/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2272 - mae: 0.3683\n",
      "Epoch 230: val_loss improved from 0.17910 to 0.17890, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2272 - mae: 0.3683 - val_loss: 0.1789 - val_mae: 0.3354\n",
      "Epoch 231/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2270 - mae: 0.3682\n",
      "Epoch 231: val_loss improved from 0.17890 to 0.17871, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2270 - mae: 0.3682 - val_loss: 0.1787 - val_mae: 0.3352\n",
      "Epoch 232/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2261 - mae: 0.3677\n",
      "Epoch 232: val_loss improved from 0.17871 to 0.17853, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2261 - mae: 0.3677 - val_loss: 0.1785 - val_mae: 0.3350\n",
      "Epoch 233/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2259 - mae: 0.3674\n",
      "Epoch 233: val_loss improved from 0.17853 to 0.17836, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2259 - mae: 0.3674 - val_loss: 0.1784 - val_mae: 0.3348\n",
      "Epoch 234/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2251 - mae: 0.3669\n",
      "Epoch 234: val_loss improved from 0.17836 to 0.17819, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2251 - mae: 0.3669 - val_loss: 0.1782 - val_mae: 0.3346\n",
      "Epoch 235/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2261 - mae: 0.3675\n",
      "Epoch 235: val_loss improved from 0.17819 to 0.17801, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2261 - mae: 0.3675 - val_loss: 0.1780 - val_mae: 0.3344\n",
      "Epoch 236/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2250 - mae: 0.3668\n",
      "Epoch 236: val_loss improved from 0.17801 to 0.17784, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2250 - mae: 0.3668 - val_loss: 0.1778 - val_mae: 0.3343\n",
      "Epoch 237/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2251 - mae: 0.3670\n",
      "Epoch 237: val_loss improved from 0.17784 to 0.17767, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2251 - mae: 0.3670 - val_loss: 0.1777 - val_mae: 0.3341\n",
      "Epoch 238/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2228 - mae: 0.3653\n",
      "Epoch 238: val_loss improved from 0.17767 to 0.17751, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2228 - mae: 0.3653 - val_loss: 0.1775 - val_mae: 0.3339\n",
      "Epoch 239/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2237 - mae: 0.3661\n",
      "Epoch 239: val_loss improved from 0.17751 to 0.17736, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2237 - mae: 0.3661 - val_loss: 0.1774 - val_mae: 0.3338\n",
      "Epoch 240/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2234 - mae: 0.3656\n",
      "Epoch 240: val_loss improved from 0.17736 to 0.17721, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2234 - mae: 0.3656 - val_loss: 0.1772 - val_mae: 0.3336\n",
      "Epoch 241/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2235 - mae: 0.3654\n",
      "Epoch 241: val_loss improved from 0.17721 to 0.17705, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2235 - mae: 0.3654 - val_loss: 0.1770 - val_mae: 0.3334\n",
      "Epoch 242/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2228 - mae: 0.3652\n",
      "Epoch 242: val_loss improved from 0.17705 to 0.17685, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2228 - mae: 0.3652 - val_loss: 0.1769 - val_mae: 0.3332\n",
      "Epoch 243/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2234 - mae: 0.3657\n",
      "Epoch 243: val_loss improved from 0.17685 to 0.17665, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2234 - mae: 0.3657 - val_loss: 0.1767 - val_mae: 0.3331\n",
      "Epoch 244/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2227 - mae: 0.3655\n",
      "Epoch 244: val_loss improved from 0.17665 to 0.17643, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2227 - mae: 0.3655 - val_loss: 0.1764 - val_mae: 0.3329\n",
      "Epoch 245/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2220 - mae: 0.3647\n",
      "Epoch 245: val_loss improved from 0.17643 to 0.17623, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2220 - mae: 0.3647 - val_loss: 0.1762 - val_mae: 0.3327\n",
      "Epoch 246/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2218 - mae: 0.3646\n",
      "Epoch 246: val_loss improved from 0.17623 to 0.17605, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.2218 - mae: 0.3646 - val_loss: 0.1761 - val_mae: 0.3325\n",
      "Epoch 247/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2219 - mae: 0.3646\n",
      "Epoch 247: val_loss improved from 0.17605 to 0.17590, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2219 - mae: 0.3646 - val_loss: 0.1759 - val_mae: 0.3323\n",
      "Epoch 248/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2212 - mae: 0.3640\n",
      "Epoch 248: val_loss improved from 0.17590 to 0.17575, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2212 - mae: 0.3640 - val_loss: 0.1757 - val_mae: 0.3322\n",
      "Epoch 249/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2214 - mae: 0.3641\n",
      "Epoch 249: val_loss improved from 0.17575 to 0.17563, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2214 - mae: 0.3641 - val_loss: 0.1756 - val_mae: 0.3320\n",
      "Epoch 250/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2216 - mae: 0.3648\n",
      "Epoch 250: val_loss improved from 0.17563 to 0.17550, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2216 - mae: 0.3648 - val_loss: 0.1755 - val_mae: 0.3319\n",
      "Epoch 251/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2204 - mae: 0.3637\n",
      "Epoch 251: val_loss improved from 0.17550 to 0.17538, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.2204 - mae: 0.3637 - val_loss: 0.1754 - val_mae: 0.3318\n",
      "Epoch 252/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2202 - mae: 0.3635\n",
      "Epoch 252: val_loss improved from 0.17538 to 0.17526, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2202 - mae: 0.3635 - val_loss: 0.1753 - val_mae: 0.3316\n",
      "Epoch 253/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2192 - mae: 0.3627\n",
      "Epoch 253: val_loss improved from 0.17526 to 0.17514, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2192 - mae: 0.3627 - val_loss: 0.1751 - val_mae: 0.3315\n",
      "Epoch 254/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2199 - mae: 0.3631\n",
      "Epoch 254: val_loss improved from 0.17514 to 0.17501, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2199 - mae: 0.3631 - val_loss: 0.1750 - val_mae: 0.3314\n",
      "Epoch 255/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2193 - mae: 0.3627\n",
      "Epoch 255: val_loss improved from 0.17501 to 0.17487, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2193 - mae: 0.3627 - val_loss: 0.1749 - val_mae: 0.3312\n",
      "Epoch 256/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2190 - mae: 0.3626\n",
      "Epoch 256: val_loss improved from 0.17487 to 0.17471, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2190 - mae: 0.3626 - val_loss: 0.1747 - val_mae: 0.3311\n",
      "Epoch 257/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2196 - mae: 0.3628\n",
      "Epoch 257: val_loss improved from 0.17471 to 0.17453, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2196 - mae: 0.3628 - val_loss: 0.1745 - val_mae: 0.3309\n",
      "Epoch 258/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2185 - mae: 0.3619\n",
      "Epoch 258: val_loss improved from 0.17453 to 0.17436, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2185 - mae: 0.3619 - val_loss: 0.1744 - val_mae: 0.3307\n",
      "Epoch 259/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2177 - mae: 0.3616\n",
      "Epoch 259: val_loss improved from 0.17436 to 0.17421, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.2177 - mae: 0.3616 - val_loss: 0.1742 - val_mae: 0.3306\n",
      "Epoch 260/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2196 - mae: 0.3626\n",
      "Epoch 260: val_loss improved from 0.17421 to 0.17406, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2196 - mae: 0.3626 - val_loss: 0.1741 - val_mae: 0.3304\n",
      "Epoch 261/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2182 - mae: 0.3619\n",
      "Epoch 261: val_loss improved from 0.17406 to 0.17391, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2182 - mae: 0.3619 - val_loss: 0.1739 - val_mae: 0.3303\n",
      "Epoch 262/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2183 - mae: 0.3623\n",
      "Epoch 262: val_loss improved from 0.17391 to 0.17376, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2183 - mae: 0.3623 - val_loss: 0.1738 - val_mae: 0.3301\n",
      "Epoch 263/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2175 - mae: 0.3614\n",
      "Epoch 263: val_loss improved from 0.17376 to 0.17363, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2175 - mae: 0.3614 - val_loss: 0.1736 - val_mae: 0.3300\n",
      "Epoch 264/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2169 - mae: 0.3616\n",
      "Epoch 264: val_loss improved from 0.17363 to 0.17351, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2169 - mae: 0.3616 - val_loss: 0.1735 - val_mae: 0.3299\n",
      "Epoch 265/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2169 - mae: 0.3612\n",
      "Epoch 265: val_loss improved from 0.17351 to 0.17340, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2169 - mae: 0.3612 - val_loss: 0.1734 - val_mae: 0.3297\n",
      "Epoch 266/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2178 - mae: 0.3618\n",
      "Epoch 266: val_loss improved from 0.17340 to 0.17328, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2178 - mae: 0.3618 - val_loss: 0.1733 - val_mae: 0.3296\n",
      "Epoch 267/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2168 - mae: 0.3610\n",
      "Epoch 267: val_loss improved from 0.17328 to 0.17317, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2168 - mae: 0.3610 - val_loss: 0.1732 - val_mae: 0.3295\n",
      "Epoch 268/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2168 - mae: 0.3609\n",
      "Epoch 268: val_loss improved from 0.17317 to 0.17308, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2168 - mae: 0.3609 - val_loss: 0.1731 - val_mae: 0.3294\n",
      "Epoch 269/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2163 - mae: 0.3607\n",
      "Epoch 269: val_loss improved from 0.17308 to 0.17299, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2163 - mae: 0.3607 - val_loss: 0.1730 - val_mae: 0.3293\n",
      "Epoch 270/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2156 - mae: 0.3600\n",
      "Epoch 270: val_loss improved from 0.17299 to 0.17287, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2156 - mae: 0.3600 - val_loss: 0.1729 - val_mae: 0.3291\n",
      "Epoch 271/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2153 - mae: 0.3600\n",
      "Epoch 271: val_loss improved from 0.17287 to 0.17274, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2153 - mae: 0.3600 - val_loss: 0.1727 - val_mae: 0.3290\n",
      "Epoch 272/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2141 - mae: 0.3589\n",
      "Epoch 272: val_loss improved from 0.17274 to 0.17260, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2141 - mae: 0.3589 - val_loss: 0.1726 - val_mae: 0.3288\n",
      "Epoch 273/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2152 - mae: 0.3597\n",
      "Epoch 273: val_loss improved from 0.17260 to 0.17242, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2152 - mae: 0.3597 - val_loss: 0.1724 - val_mae: 0.3287\n",
      "Epoch 274/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2166 - mae: 0.3611\n",
      "Epoch 274: val_loss improved from 0.17242 to 0.17223, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2166 - mae: 0.3611 - val_loss: 0.1722 - val_mae: 0.3285\n",
      "Epoch 275/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2156 - mae: 0.3599\n",
      "Epoch 275: val_loss improved from 0.17223 to 0.17206, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.2156 - mae: 0.3599 - val_loss: 0.1721 - val_mae: 0.3284\n",
      "Epoch 276/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2143 - mae: 0.3591\n",
      "Epoch 276: val_loss improved from 0.17206 to 0.17190, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2143 - mae: 0.3591 - val_loss: 0.1719 - val_mae: 0.3282\n",
      "Epoch 277/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2150 - mae: 0.3597\n",
      "Epoch 277: val_loss improved from 0.17190 to 0.17175, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2150 - mae: 0.3597 - val_loss: 0.1717 - val_mae: 0.3281\n",
      "Epoch 278/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2129 - mae: 0.3583\n",
      "Epoch 278: val_loss improved from 0.17175 to 0.17164, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2129 - mae: 0.3583 - val_loss: 0.1716 - val_mae: 0.3279\n",
      "Epoch 279/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2141 - mae: 0.3589\n",
      "Epoch 279: val_loss improved from 0.17164 to 0.17154, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2141 - mae: 0.3589 - val_loss: 0.1715 - val_mae: 0.3278\n",
      "Epoch 280/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2137 - mae: 0.3589\n",
      "Epoch 280: val_loss improved from 0.17154 to 0.17144, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2137 - mae: 0.3589 - val_loss: 0.1714 - val_mae: 0.3277\n",
      "Epoch 281/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2126 - mae: 0.3581\n",
      "Epoch 281: val_loss improved from 0.17144 to 0.17137, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2126 - mae: 0.3581 - val_loss: 0.1714 - val_mae: 0.3276\n",
      "Epoch 282/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2129 - mae: 0.3585\n",
      "Epoch 282: val_loss improved from 0.17137 to 0.17127, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2129 - mae: 0.3585 - val_loss: 0.1713 - val_mae: 0.3275\n",
      "Epoch 283/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2126 - mae: 0.3577\n",
      "Epoch 283: val_loss improved from 0.17127 to 0.17116, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2126 - mae: 0.3577 - val_loss: 0.1712 - val_mae: 0.3274\n",
      "Epoch 284/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2128 - mae: 0.3581\n",
      "Epoch 284: val_loss improved from 0.17116 to 0.17104, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2128 - mae: 0.3581 - val_loss: 0.1710 - val_mae: 0.3273\n",
      "Epoch 285/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2127 - mae: 0.3578\n",
      "Epoch 285: val_loss improved from 0.17104 to 0.17091, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2127 - mae: 0.3578 - val_loss: 0.1709 - val_mae: 0.3272\n",
      "Epoch 286/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2128 - mae: 0.3582\n",
      "Epoch 286: val_loss improved from 0.17091 to 0.17078, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2128 - mae: 0.3582 - val_loss: 0.1708 - val_mae: 0.3270\n",
      "Epoch 287/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2111 - mae: 0.3570\n",
      "Epoch 287: val_loss improved from 0.17078 to 0.17065, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2111 - mae: 0.3570 - val_loss: 0.1706 - val_mae: 0.3269\n",
      "Epoch 288/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2126 - mae: 0.3582\n",
      "Epoch 288: val_loss improved from 0.17065 to 0.17050, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2126 - mae: 0.3582 - val_loss: 0.1705 - val_mae: 0.3268\n",
      "Epoch 289/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2125 - mae: 0.3577\n",
      "Epoch 289: val_loss improved from 0.17050 to 0.17035, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2125 - mae: 0.3577 - val_loss: 0.1704 - val_mae: 0.3266\n",
      "Epoch 290/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2112 - mae: 0.3571\n",
      "Epoch 290: val_loss improved from 0.17035 to 0.17022, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2112 - mae: 0.3571 - val_loss: 0.1702 - val_mae: 0.3265\n",
      "Epoch 291/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2112 - mae: 0.3571\n",
      "Epoch 291: val_loss improved from 0.17022 to 0.17008, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.2112 - mae: 0.3571 - val_loss: 0.1701 - val_mae: 0.3264\n",
      "Epoch 292/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2106 - mae: 0.3565\n",
      "Epoch 292: val_loss improved from 0.17008 to 0.16995, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2106 - mae: 0.3565 - val_loss: 0.1700 - val_mae: 0.3262\n",
      "Epoch 293/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2104 - mae: 0.3565\n",
      "Epoch 293: val_loss improved from 0.16995 to 0.16984, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2104 - mae: 0.3565 - val_loss: 0.1698 - val_mae: 0.3261\n",
      "Epoch 294/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2102 - mae: 0.3564\n",
      "Epoch 294: val_loss improved from 0.16984 to 0.16973, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2102 - mae: 0.3564 - val_loss: 0.1697 - val_mae: 0.3260\n",
      "Epoch 295/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2110 - mae: 0.3566\n",
      "Epoch 295: val_loss improved from 0.16973 to 0.16964, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2110 - mae: 0.3566 - val_loss: 0.1696 - val_mae: 0.3259\n",
      "Epoch 296/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2100 - mae: 0.3559\n",
      "Epoch 296: val_loss improved from 0.16964 to 0.16957, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2100 - mae: 0.3559 - val_loss: 0.1696 - val_mae: 0.3258\n",
      "Epoch 297/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2105 - mae: 0.3562\n",
      "Epoch 297: val_loss improved from 0.16957 to 0.16950, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2105 - mae: 0.3562 - val_loss: 0.1695 - val_mae: 0.3257\n",
      "Epoch 298/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2103 - mae: 0.3564\n",
      "Epoch 298: val_loss improved from 0.16950 to 0.16943, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2103 - mae: 0.3564 - val_loss: 0.1694 - val_mae: 0.3257\n",
      "Epoch 299/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2101 - mae: 0.3566\n",
      "Epoch 299: val_loss improved from 0.16943 to 0.16933, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.2101 - mae: 0.3566 - val_loss: 0.1693 - val_mae: 0.3256\n",
      "Epoch 300/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2083 - mae: 0.3549\n",
      "Epoch 300: val_loss improved from 0.16933 to 0.16925, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2083 - mae: 0.3549 - val_loss: 0.1692 - val_mae: 0.3255\n",
      "Epoch 301/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2098 - mae: 0.3563\n",
      "Epoch 301: val_loss improved from 0.16925 to 0.16914, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2098 - mae: 0.3563 - val_loss: 0.1691 - val_mae: 0.3253\n",
      "Epoch 302/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2093 - mae: 0.3556\n",
      "Epoch 302: val_loss improved from 0.16914 to 0.16904, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2093 - mae: 0.3556 - val_loss: 0.1690 - val_mae: 0.3252\n",
      "Epoch 303/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2091 - mae: 0.3557\n",
      "Epoch 303: val_loss improved from 0.16904 to 0.16893, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2091 - mae: 0.3557 - val_loss: 0.1689 - val_mae: 0.3251\n",
      "Epoch 304/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2089 - mae: 0.3552\n",
      "Epoch 304: val_loss improved from 0.16893 to 0.16881, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2089 - mae: 0.3552 - val_loss: 0.1688 - val_mae: 0.3250\n",
      "Epoch 305/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2089 - mae: 0.3555\n",
      "Epoch 305: val_loss improved from 0.16881 to 0.16868, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2089 - mae: 0.3555 - val_loss: 0.1687 - val_mae: 0.3249\n",
      "Epoch 306/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2074 - mae: 0.3544\n",
      "Epoch 306: val_loss improved from 0.16868 to 0.16857, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.2074 - mae: 0.3544 - val_loss: 0.1686 - val_mae: 0.3248\n",
      "Epoch 307/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2080 - mae: 0.3549\n",
      "Epoch 307: val_loss improved from 0.16857 to 0.16846, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2080 - mae: 0.3549 - val_loss: 0.1685 - val_mae: 0.3246\n",
      "Epoch 308/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2090 - mae: 0.3555\n",
      "Epoch 308: val_loss improved from 0.16846 to 0.16834, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.2090 - mae: 0.3555 - val_loss: 0.1683 - val_mae: 0.3245\n",
      "Epoch 309/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2080 - mae: 0.3549\n",
      "Epoch 309: val_loss improved from 0.16834 to 0.16824, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2080 - mae: 0.3549 - val_loss: 0.1682 - val_mae: 0.3244\n",
      "Epoch 310/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2074 - mae: 0.3542\n",
      "Epoch 310: val_loss improved from 0.16824 to 0.16815, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2074 - mae: 0.3542 - val_loss: 0.1681 - val_mae: 0.3243\n",
      "Epoch 311/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2073 - mae: 0.3542\n",
      "Epoch 311: val_loss improved from 0.16815 to 0.16807, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2073 - mae: 0.3542 - val_loss: 0.1681 - val_mae: 0.3242\n",
      "Epoch 312/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2068 - mae: 0.3537\n",
      "Epoch 312: val_loss improved from 0.16807 to 0.16799, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.2068 - mae: 0.3537 - val_loss: 0.1680 - val_mae: 0.3241\n",
      "Epoch 313/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2069 - mae: 0.3537\n",
      "Epoch 313: val_loss improved from 0.16799 to 0.16791, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.2069 - mae: 0.3537 - val_loss: 0.1679 - val_mae: 0.3240\n",
      "Epoch 314/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2061 - mae: 0.3531\n",
      "Epoch 314: val_loss improved from 0.16791 to 0.16781, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2061 - mae: 0.3531 - val_loss: 0.1678 - val_mae: 0.3239\n",
      "Epoch 315/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2054 - mae: 0.3531\n",
      "Epoch 315: val_loss improved from 0.16781 to 0.16771, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2054 - mae: 0.3531 - val_loss: 0.1677 - val_mae: 0.3238\n",
      "Epoch 316/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2065 - mae: 0.3536\n",
      "Epoch 316: val_loss improved from 0.16771 to 0.16761, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2065 - mae: 0.3536 - val_loss: 0.1676 - val_mae: 0.3237\n",
      "Epoch 317/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2059 - mae: 0.3532\n",
      "Epoch 317: val_loss improved from 0.16761 to 0.16749, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.2059 - mae: 0.3532 - val_loss: 0.1675 - val_mae: 0.3236\n",
      "Epoch 318/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2054 - mae: 0.3527\n",
      "Epoch 318: val_loss improved from 0.16749 to 0.16739, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2054 - mae: 0.3527 - val_loss: 0.1674 - val_mae: 0.3235\n",
      "Epoch 319/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2058 - mae: 0.3526\n",
      "Epoch 319: val_loss improved from 0.16739 to 0.16729, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2058 - mae: 0.3526 - val_loss: 0.1673 - val_mae: 0.3234\n",
      "Epoch 320/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2059 - mae: 0.3532\n",
      "Epoch 320: val_loss improved from 0.16729 to 0.16717, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2059 - mae: 0.3532 - val_loss: 0.1672 - val_mae: 0.3233\n",
      "Epoch 321/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2056 - mae: 0.3528\n",
      "Epoch 321: val_loss improved from 0.16717 to 0.16706, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.2056 - mae: 0.3528 - val_loss: 0.1671 - val_mae: 0.3232\n",
      "Epoch 322/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2061 - mae: 0.3531\n",
      "Epoch 322: val_loss improved from 0.16706 to 0.16695, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2061 - mae: 0.3531 - val_loss: 0.1669 - val_mae: 0.3231\n",
      "Epoch 323/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2045 - mae: 0.3522\n",
      "Epoch 323: val_loss improved from 0.16695 to 0.16685, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2045 - mae: 0.3522 - val_loss: 0.1669 - val_mae: 0.3230\n",
      "Epoch 324/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2061 - mae: 0.3532\n",
      "Epoch 324: val_loss improved from 0.16685 to 0.16677, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2061 - mae: 0.3532 - val_loss: 0.1668 - val_mae: 0.3229\n",
      "Epoch 325/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2047 - mae: 0.3521\n",
      "Epoch 325: val_loss improved from 0.16677 to 0.16671, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2047 - mae: 0.3521 - val_loss: 0.1667 - val_mae: 0.3228\n",
      "Epoch 326/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2047 - mae: 0.3523\n",
      "Epoch 326: val_loss improved from 0.16671 to 0.16664, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2047 - mae: 0.3523 - val_loss: 0.1666 - val_mae: 0.3227\n",
      "Epoch 327/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2037 - mae: 0.3517\n",
      "Epoch 327: val_loss improved from 0.16664 to 0.16657, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2037 - mae: 0.3517 - val_loss: 0.1666 - val_mae: 0.3227\n",
      "Epoch 328/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2046 - mae: 0.3522\n",
      "Epoch 328: val_loss improved from 0.16657 to 0.16649, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2046 - mae: 0.3522 - val_loss: 0.1665 - val_mae: 0.3226\n",
      "Epoch 329/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2043 - mae: 0.3516\n",
      "Epoch 329: val_loss improved from 0.16649 to 0.16638, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2043 - mae: 0.3516 - val_loss: 0.1664 - val_mae: 0.3225\n",
      "Epoch 330/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2043 - mae: 0.3517\n",
      "Epoch 330: val_loss improved from 0.16638 to 0.16628, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2043 - mae: 0.3517 - val_loss: 0.1663 - val_mae: 0.3224\n",
      "Epoch 331/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2028 - mae: 0.3508\n",
      "Epoch 331: val_loss improved from 0.16628 to 0.16618, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.2028 - mae: 0.3508 - val_loss: 0.1662 - val_mae: 0.3223\n",
      "Epoch 332/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2034 - mae: 0.3511\n",
      "Epoch 332: val_loss improved from 0.16618 to 0.16609, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2034 - mae: 0.3511 - val_loss: 0.1661 - val_mae: 0.3222\n",
      "Epoch 333/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2029 - mae: 0.3506\n",
      "Epoch 333: val_loss improved from 0.16609 to 0.16600, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2029 - mae: 0.3506 - val_loss: 0.1660 - val_mae: 0.3221\n",
      "Epoch 334/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2032 - mae: 0.3510\n",
      "Epoch 334: val_loss improved from 0.16600 to 0.16592, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2032 - mae: 0.3510 - val_loss: 0.1659 - val_mae: 0.3220\n",
      "Epoch 335/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2030 - mae: 0.3510\n",
      "Epoch 335: val_loss improved from 0.16592 to 0.16582, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2030 - mae: 0.3510 - val_loss: 0.1658 - val_mae: 0.3219\n",
      "Epoch 336/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2022 - mae: 0.3503\n",
      "Epoch 336: val_loss improved from 0.16582 to 0.16570, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2022 - mae: 0.3503 - val_loss: 0.1657 - val_mae: 0.3218\n",
      "Epoch 337/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2028 - mae: 0.3509\n",
      "Epoch 337: val_loss improved from 0.16570 to 0.16558, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2028 - mae: 0.3509 - val_loss: 0.1656 - val_mae: 0.3217\n",
      "Epoch 338/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2034 - mae: 0.3511\n",
      "Epoch 338: val_loss improved from 0.16558 to 0.16545, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2034 - mae: 0.3511 - val_loss: 0.1654 - val_mae: 0.3216\n",
      "Epoch 339/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2031 - mae: 0.3511\n",
      "Epoch 339: val_loss improved from 0.16545 to 0.16533, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.2031 - mae: 0.3511 - val_loss: 0.1653 - val_mae: 0.3215\n",
      "Epoch 340/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2015 - mae: 0.3503\n",
      "Epoch 340: val_loss improved from 0.16533 to 0.16523, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2015 - mae: 0.3503 - val_loss: 0.1652 - val_mae: 0.3214\n",
      "Epoch 341/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2025 - mae: 0.3510\n",
      "Epoch 341: val_loss improved from 0.16523 to 0.16513, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2025 - mae: 0.3510 - val_loss: 0.1651 - val_mae: 0.3213\n",
      "Epoch 342/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2022 - mae: 0.3505\n",
      "Epoch 342: val_loss improved from 0.16513 to 0.16505, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2022 - mae: 0.3505 - val_loss: 0.1651 - val_mae: 0.3212\n",
      "Epoch 343/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2015 - mae: 0.3500\n",
      "Epoch 343: val_loss improved from 0.16505 to 0.16499, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2015 - mae: 0.3500 - val_loss: 0.1650 - val_mae: 0.3211\n",
      "Epoch 344/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2013 - mae: 0.3496\n",
      "Epoch 344: val_loss improved from 0.16499 to 0.16492, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2013 - mae: 0.3496 - val_loss: 0.1649 - val_mae: 0.3210\n",
      "Epoch 345/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2016 - mae: 0.3500\n",
      "Epoch 345: val_loss improved from 0.16492 to 0.16485, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.2016 - mae: 0.3500 - val_loss: 0.1648 - val_mae: 0.3209\n",
      "Epoch 346/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2009 - mae: 0.3497\n",
      "Epoch 346: val_loss improved from 0.16485 to 0.16477, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2009 - mae: 0.3497 - val_loss: 0.1648 - val_mae: 0.3208\n",
      "Epoch 347/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2007 - mae: 0.3495\n",
      "Epoch 347: val_loss improved from 0.16477 to 0.16467, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2007 - mae: 0.3495 - val_loss: 0.1647 - val_mae: 0.3207\n",
      "Epoch 348/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2000 - mae: 0.3489\n",
      "Epoch 348: val_loss improved from 0.16467 to 0.16458, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.2000 - mae: 0.3489 - val_loss: 0.1646 - val_mae: 0.3206\n",
      "Epoch 349/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2018 - mae: 0.3502\n",
      "Epoch 349: val_loss improved from 0.16458 to 0.16447, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2018 - mae: 0.3502 - val_loss: 0.1645 - val_mae: 0.3205\n",
      "Epoch 350/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2001 - mae: 0.3489\n",
      "Epoch 350: val_loss improved from 0.16447 to 0.16435, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2001 - mae: 0.3489 - val_loss: 0.1644 - val_mae: 0.3204\n",
      "Epoch 351/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2001 - mae: 0.3491\n",
      "Epoch 351: val_loss improved from 0.16435 to 0.16424, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.2001 - mae: 0.3491 - val_loss: 0.1642 - val_mae: 0.3203\n",
      "Epoch 352/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1994 - mae: 0.3479\n",
      "Epoch 352: val_loss improved from 0.16424 to 0.16413, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1994 - mae: 0.3479 - val_loss: 0.1641 - val_mae: 0.3202\n",
      "Epoch 353/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2005 - mae: 0.3488\n",
      "Epoch 353: val_loss improved from 0.16413 to 0.16402, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.2005 - mae: 0.3488 - val_loss: 0.1640 - val_mae: 0.3201\n",
      "Epoch 354/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1996 - mae: 0.3481\n",
      "Epoch 354: val_loss improved from 0.16402 to 0.16391, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1996 - mae: 0.3481 - val_loss: 0.1639 - val_mae: 0.3200\n",
      "Epoch 355/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1996 - mae: 0.3486\n",
      "Epoch 355: val_loss improved from 0.16391 to 0.16381, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1996 - mae: 0.3486 - val_loss: 0.1638 - val_mae: 0.3199\n",
      "Epoch 356/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1992 - mae: 0.3480\n",
      "Epoch 356: val_loss improved from 0.16381 to 0.16373, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1992 - mae: 0.3480 - val_loss: 0.1637 - val_mae: 0.3198\n",
      "Epoch 357/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1991 - mae: 0.3478\n",
      "Epoch 357: val_loss improved from 0.16373 to 0.16366, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1991 - mae: 0.3478 - val_loss: 0.1637 - val_mae: 0.3197\n",
      "Epoch 358/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1989 - mae: 0.3479\n",
      "Epoch 358: val_loss improved from 0.16366 to 0.16360, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1989 - mae: 0.3479 - val_loss: 0.1636 - val_mae: 0.3197\n",
      "Epoch 359/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1989 - mae: 0.3478\n",
      "Epoch 359: val_loss improved from 0.16360 to 0.16355, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1989 - mae: 0.3478 - val_loss: 0.1636 - val_mae: 0.3196\n",
      "Epoch 360/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1992 - mae: 0.3480\n",
      "Epoch 360: val_loss improved from 0.16355 to 0.16350, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1992 - mae: 0.3480 - val_loss: 0.1635 - val_mae: 0.3195\n",
      "Epoch 361/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1984 - mae: 0.3474\n",
      "Epoch 361: val_loss improved from 0.16350 to 0.16345, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1984 - mae: 0.3474 - val_loss: 0.1634 - val_mae: 0.3195\n",
      "Epoch 362/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1985 - mae: 0.3475\n",
      "Epoch 362: val_loss improved from 0.16345 to 0.16338, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1985 - mae: 0.3475 - val_loss: 0.1634 - val_mae: 0.3194\n",
      "Epoch 363/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1989 - mae: 0.3477\n",
      "Epoch 363: val_loss improved from 0.16338 to 0.16329, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1989 - mae: 0.3477 - val_loss: 0.1633 - val_mae: 0.3193\n",
      "Epoch 364/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1985 - mae: 0.3476\n",
      "Epoch 364: val_loss improved from 0.16329 to 0.16319, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1985 - mae: 0.3476 - val_loss: 0.1632 - val_mae: 0.3192\n",
      "Epoch 365/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1983 - mae: 0.3471\n",
      "Epoch 365: val_loss improved from 0.16319 to 0.16307, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1983 - mae: 0.3471 - val_loss: 0.1631 - val_mae: 0.3191\n",
      "Epoch 366/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1981 - mae: 0.3473\n",
      "Epoch 366: val_loss improved from 0.16307 to 0.16297, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1981 - mae: 0.3473 - val_loss: 0.1630 - val_mae: 0.3190\n",
      "Epoch 367/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1978 - mae: 0.3471\n",
      "Epoch 367: val_loss improved from 0.16297 to 0.16287, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1978 - mae: 0.3471 - val_loss: 0.1629 - val_mae: 0.3189\n",
      "Epoch 368/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1976 - mae: 0.3468\n",
      "Epoch 368: val_loss improved from 0.16287 to 0.16278, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1976 - mae: 0.3468 - val_loss: 0.1628 - val_mae: 0.3189\n",
      "Epoch 369/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1979 - mae: 0.3471\n",
      "Epoch 369: val_loss improved from 0.16278 to 0.16268, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1979 - mae: 0.3471 - val_loss: 0.1627 - val_mae: 0.3188\n",
      "Epoch 370/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1975 - mae: 0.3469\n",
      "Epoch 370: val_loss improved from 0.16268 to 0.16258, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1975 - mae: 0.3469 - val_loss: 0.1626 - val_mae: 0.3187\n",
      "Epoch 371/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1978 - mae: 0.3469\n",
      "Epoch 371: val_loss improved from 0.16258 to 0.16249, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1978 - mae: 0.3469 - val_loss: 0.1625 - val_mae: 0.3186\n",
      "Epoch 372/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1974 - mae: 0.3468\n",
      "Epoch 372: val_loss improved from 0.16249 to 0.16242, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1974 - mae: 0.3468 - val_loss: 0.1624 - val_mae: 0.3185\n",
      "Epoch 373/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1970 - mae: 0.3467\n",
      "Epoch 373: val_loss improved from 0.16242 to 0.16236, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1970 - mae: 0.3467 - val_loss: 0.1624 - val_mae: 0.3184\n",
      "Epoch 374/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1969 - mae: 0.3464\n",
      "Epoch 374: val_loss improved from 0.16236 to 0.16228, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1969 - mae: 0.3464 - val_loss: 0.1623 - val_mae: 0.3183\n",
      "Epoch 375/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1972 - mae: 0.3468\n",
      "Epoch 375: val_loss improved from 0.16228 to 0.16220, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1972 - mae: 0.3468 - val_loss: 0.1622 - val_mae: 0.3183\n",
      "Epoch 376/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1958 - mae: 0.3458\n",
      "Epoch 376: val_loss improved from 0.16220 to 0.16213, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1958 - mae: 0.3458 - val_loss: 0.1621 - val_mae: 0.3182\n",
      "Epoch 377/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1965 - mae: 0.3463\n",
      "Epoch 377: val_loss improved from 0.16213 to 0.16204, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1965 - mae: 0.3463 - val_loss: 0.1620 - val_mae: 0.3181\n",
      "Epoch 378/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1967 - mae: 0.3462\n",
      "Epoch 378: val_loss improved from 0.16204 to 0.16192, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1967 - mae: 0.3462 - val_loss: 0.1619 - val_mae: 0.3180\n",
      "Epoch 379/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1960 - mae: 0.3457\n",
      "Epoch 379: val_loss improved from 0.16192 to 0.16180, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1960 - mae: 0.3457 - val_loss: 0.1618 - val_mae: 0.3179\n",
      "Epoch 380/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1968 - mae: 0.3463\n",
      "Epoch 380: val_loss improved from 0.16180 to 0.16168, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1968 - mae: 0.3463 - val_loss: 0.1617 - val_mae: 0.3178\n",
      "Epoch 381/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1964 - mae: 0.3459\n",
      "Epoch 381: val_loss improved from 0.16168 to 0.16158, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1964 - mae: 0.3459 - val_loss: 0.1616 - val_mae: 0.3177\n",
      "Epoch 382/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1960 - mae: 0.3454\n",
      "Epoch 382: val_loss improved from 0.16158 to 0.16148, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1960 - mae: 0.3454 - val_loss: 0.1615 - val_mae: 0.3176\n",
      "Epoch 383/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1957 - mae: 0.3456\n",
      "Epoch 383: val_loss improved from 0.16148 to 0.16141, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1957 - mae: 0.3456 - val_loss: 0.1614 - val_mae: 0.3175\n",
      "Epoch 384/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1951 - mae: 0.3447\n",
      "Epoch 384: val_loss improved from 0.16141 to 0.16135, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1951 - mae: 0.3447 - val_loss: 0.1614 - val_mae: 0.3174\n",
      "Epoch 385/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1963 - mae: 0.3457\n",
      "Epoch 385: val_loss improved from 0.16135 to 0.16129, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1963 - mae: 0.3457 - val_loss: 0.1613 - val_mae: 0.3174\n",
      "Epoch 386/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1958 - mae: 0.3455\n",
      "Epoch 386: val_loss improved from 0.16129 to 0.16122, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1958 - mae: 0.3455 - val_loss: 0.1612 - val_mae: 0.3173\n",
      "Epoch 387/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1951 - mae: 0.3453\n",
      "Epoch 387: val_loss improved from 0.16122 to 0.16113, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1951 - mae: 0.3453 - val_loss: 0.1611 - val_mae: 0.3172\n",
      "Epoch 388/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1958 - mae: 0.3455\n",
      "Epoch 388: val_loss improved from 0.16113 to 0.16103, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1958 - mae: 0.3455 - val_loss: 0.1610 - val_mae: 0.3171\n",
      "Epoch 389/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1964 - mae: 0.3459\n",
      "Epoch 389: val_loss improved from 0.16103 to 0.16093, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1964 - mae: 0.3459 - val_loss: 0.1609 - val_mae: 0.3170\n",
      "Epoch 390/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1943 - mae: 0.3446\n",
      "Epoch 390: val_loss improved from 0.16093 to 0.16086, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1943 - mae: 0.3446 - val_loss: 0.1609 - val_mae: 0.3169\n",
      "Epoch 391/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1950 - mae: 0.3450\n",
      "Epoch 391: val_loss improved from 0.16086 to 0.16080, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1950 - mae: 0.3450 - val_loss: 0.1608 - val_mae: 0.3169\n",
      "Epoch 392/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1954 - mae: 0.3454\n",
      "Epoch 392: val_loss improved from 0.16080 to 0.16076, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1954 - mae: 0.3454 - val_loss: 0.1608 - val_mae: 0.3168\n",
      "Epoch 393/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1948 - mae: 0.3450\n",
      "Epoch 393: val_loss improved from 0.16076 to 0.16072, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1948 - mae: 0.3450 - val_loss: 0.1607 - val_mae: 0.3167\n",
      "Epoch 394/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1944 - mae: 0.3444\n",
      "Epoch 394: val_loss improved from 0.16072 to 0.16066, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1944 - mae: 0.3444 - val_loss: 0.1607 - val_mae: 0.3167\n",
      "Epoch 395/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1944 - mae: 0.3443\n",
      "Epoch 395: val_loss improved from 0.16066 to 0.16060, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.1944 - mae: 0.3443 - val_loss: 0.1606 - val_mae: 0.3166\n",
      "Epoch 396/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1940 - mae: 0.3441\n",
      "Epoch 396: val_loss improved from 0.16060 to 0.16054, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1940 - mae: 0.3441 - val_loss: 0.1605 - val_mae: 0.3165\n",
      "Epoch 397/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1938 - mae: 0.3443\n",
      "Epoch 397: val_loss improved from 0.16054 to 0.16046, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1938 - mae: 0.3443 - val_loss: 0.1605 - val_mae: 0.3164\n",
      "Epoch 398/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1935 - mae: 0.3436\n",
      "Epoch 398: val_loss improved from 0.16046 to 0.16037, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1935 - mae: 0.3436 - val_loss: 0.1604 - val_mae: 0.3164\n",
      "Epoch 399/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1941 - mae: 0.3443\n",
      "Epoch 399: val_loss improved from 0.16037 to 0.16028, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1941 - mae: 0.3443 - val_loss: 0.1603 - val_mae: 0.3163\n",
      "Epoch 400/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1944 - mae: 0.3446\n",
      "Epoch 400: val_loss improved from 0.16028 to 0.16019, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1944 - mae: 0.3446 - val_loss: 0.1602 - val_mae: 0.3162\n",
      "Epoch 401/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1927 - mae: 0.3433\n",
      "Epoch 401: val_loss improved from 0.16019 to 0.16013, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1927 - mae: 0.3433 - val_loss: 0.1601 - val_mae: 0.3161\n",
      "Epoch 402/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1944 - mae: 0.3444\n",
      "Epoch 402: val_loss improved from 0.16013 to 0.16006, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1944 - mae: 0.3444 - val_loss: 0.1601 - val_mae: 0.3160\n",
      "Epoch 403/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1932 - mae: 0.3435\n",
      "Epoch 403: val_loss improved from 0.16006 to 0.16000, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1932 - mae: 0.3435 - val_loss: 0.1600 - val_mae: 0.3160\n",
      "Epoch 404/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1923 - mae: 0.3428\n",
      "Epoch 404: val_loss improved from 0.16000 to 0.15994, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1923 - mae: 0.3428 - val_loss: 0.1599 - val_mae: 0.3159\n",
      "Epoch 405/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1931 - mae: 0.3434\n",
      "Epoch 405: val_loss improved from 0.15994 to 0.15988, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1931 - mae: 0.3434 - val_loss: 0.1599 - val_mae: 0.3158\n",
      "Epoch 406/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1925 - mae: 0.3427\n",
      "Epoch 406: val_loss improved from 0.15988 to 0.15982, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1925 - mae: 0.3427 - val_loss: 0.1598 - val_mae: 0.3158\n",
      "Epoch 407/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1922 - mae: 0.3431\n",
      "Epoch 407: val_loss improved from 0.15982 to 0.15975, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1922 - mae: 0.3431 - val_loss: 0.1597 - val_mae: 0.3157\n",
      "Epoch 408/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1927 - mae: 0.3431\n",
      "Epoch 408: val_loss improved from 0.15975 to 0.15965, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1927 - mae: 0.3431 - val_loss: 0.1597 - val_mae: 0.3156\n",
      "Epoch 409/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1923 - mae: 0.3426\n",
      "Epoch 409: val_loss improved from 0.15965 to 0.15954, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1923 - mae: 0.3426 - val_loss: 0.1595 - val_mae: 0.3155\n",
      "Epoch 410/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1923 - mae: 0.3427\n",
      "Epoch 410: val_loss improved from 0.15954 to 0.15943, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1923 - mae: 0.3427 - val_loss: 0.1594 - val_mae: 0.3154\n",
      "Epoch 411/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1914 - mae: 0.3423\n",
      "Epoch 411: val_loss improved from 0.15943 to 0.15934, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1914 - mae: 0.3423 - val_loss: 0.1593 - val_mae: 0.3153\n",
      "Epoch 412/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1925 - mae: 0.3432\n",
      "Epoch 412: val_loss improved from 0.15934 to 0.15925, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1925 - mae: 0.3432 - val_loss: 0.1592 - val_mae: 0.3152\n",
      "Epoch 413/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1922 - mae: 0.3428\n",
      "Epoch 413: val_loss improved from 0.15925 to 0.15914, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1922 - mae: 0.3428 - val_loss: 0.1591 - val_mae: 0.3152\n",
      "Epoch 414/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1919 - mae: 0.3426\n",
      "Epoch 414: val_loss improved from 0.15914 to 0.15906, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1919 - mae: 0.3426 - val_loss: 0.1591 - val_mae: 0.3151\n",
      "Epoch 415/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1914 - mae: 0.3422\n",
      "Epoch 415: val_loss improved from 0.15906 to 0.15897, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1914 - mae: 0.3422 - val_loss: 0.1590 - val_mae: 0.3150\n",
      "Epoch 416/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1918 - mae: 0.3424\n",
      "Epoch 416: val_loss improved from 0.15897 to 0.15888, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1918 - mae: 0.3424 - val_loss: 0.1589 - val_mae: 0.3149\n",
      "Epoch 417/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1922 - mae: 0.3426\n",
      "Epoch 417: val_loss improved from 0.15888 to 0.15880, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1922 - mae: 0.3426 - val_loss: 0.1588 - val_mae: 0.3148\n",
      "Epoch 418/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1912 - mae: 0.3419\n",
      "Epoch 418: val_loss improved from 0.15880 to 0.15871, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1912 - mae: 0.3419 - val_loss: 0.1587 - val_mae: 0.3147\n",
      "Epoch 419/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1921 - mae: 0.3425\n",
      "Epoch 419: val_loss improved from 0.15871 to 0.15864, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1921 - mae: 0.3425 - val_loss: 0.1586 - val_mae: 0.3147\n",
      "Epoch 420/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1914 - mae: 0.3421\n",
      "Epoch 420: val_loss improved from 0.15864 to 0.15856, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1914 - mae: 0.3421 - val_loss: 0.1586 - val_mae: 0.3146\n",
      "Epoch 421/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1904 - mae: 0.3410\n",
      "Epoch 421: val_loss improved from 0.15856 to 0.15848, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1904 - mae: 0.3410 - val_loss: 0.1585 - val_mae: 0.3145\n",
      "Epoch 422/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1907 - mae: 0.3416\n",
      "Epoch 422: val_loss improved from 0.15848 to 0.15843, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1907 - mae: 0.3416 - val_loss: 0.1584 - val_mae: 0.3144\n",
      "Epoch 423/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1907 - mae: 0.3417\n",
      "Epoch 423: val_loss improved from 0.15843 to 0.15840, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1907 - mae: 0.3417 - val_loss: 0.1584 - val_mae: 0.3144\n",
      "Epoch 424/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1912 - mae: 0.3417\n",
      "Epoch 424: val_loss improved from 0.15840 to 0.15834, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1912 - mae: 0.3417 - val_loss: 0.1583 - val_mae: 0.3143\n",
      "Epoch 425/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1905 - mae: 0.3415\n",
      "Epoch 425: val_loss improved from 0.15834 to 0.15828, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1905 - mae: 0.3415 - val_loss: 0.1583 - val_mae: 0.3142\n",
      "Epoch 426/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1910 - mae: 0.3419\n",
      "Epoch 426: val_loss improved from 0.15828 to 0.15821, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1910 - mae: 0.3419 - val_loss: 0.1582 - val_mae: 0.3142\n",
      "Epoch 427/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1905 - mae: 0.3413\n",
      "Epoch 427: val_loss improved from 0.15821 to 0.15812, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1905 - mae: 0.3413 - val_loss: 0.1581 - val_mae: 0.3141\n",
      "Epoch 428/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1902 - mae: 0.3410\n",
      "Epoch 428: val_loss improved from 0.15812 to 0.15805, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1902 - mae: 0.3410 - val_loss: 0.1580 - val_mae: 0.3140\n",
      "Epoch 429/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1898 - mae: 0.3407\n",
      "Epoch 429: val_loss improved from 0.15805 to 0.15797, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1898 - mae: 0.3407 - val_loss: 0.1580 - val_mae: 0.3139\n",
      "Epoch 430/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1901 - mae: 0.3411\n",
      "Epoch 430: val_loss improved from 0.15797 to 0.15790, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1901 - mae: 0.3411 - val_loss: 0.1579 - val_mae: 0.3138\n",
      "Epoch 431/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1896 - mae: 0.3409\n",
      "Epoch 431: val_loss improved from 0.15790 to 0.15781, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1896 - mae: 0.3409 - val_loss: 0.1578 - val_mae: 0.3138\n",
      "Epoch 432/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1900 - mae: 0.3412\n",
      "Epoch 432: val_loss improved from 0.15781 to 0.15773, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1900 - mae: 0.3412 - val_loss: 0.1577 - val_mae: 0.3137\n",
      "Epoch 433/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1892 - mae: 0.3404\n",
      "Epoch 433: val_loss improved from 0.15773 to 0.15763, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1892 - mae: 0.3404 - val_loss: 0.1576 - val_mae: 0.3136\n",
      "Epoch 434/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1902 - mae: 0.3413\n",
      "Epoch 434: val_loss improved from 0.15763 to 0.15754, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1902 - mae: 0.3413 - val_loss: 0.1575 - val_mae: 0.3135\n",
      "Epoch 435/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1891 - mae: 0.3402\n",
      "Epoch 435: val_loss improved from 0.15754 to 0.15747, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1891 - mae: 0.3402 - val_loss: 0.1575 - val_mae: 0.3134\n",
      "Epoch 436/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1891 - mae: 0.3401\n",
      "Epoch 436: val_loss improved from 0.15747 to 0.15743, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1891 - mae: 0.3401 - val_loss: 0.1574 - val_mae: 0.3134\n",
      "Epoch 437/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1893 - mae: 0.3408\n",
      "Epoch 437: val_loss improved from 0.15743 to 0.15740, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1893 - mae: 0.3408 - val_loss: 0.1574 - val_mae: 0.3133\n",
      "Epoch 438/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1889 - mae: 0.3402\n",
      "Epoch 438: val_loss improved from 0.15740 to 0.15736, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1889 - mae: 0.3402 - val_loss: 0.1574 - val_mae: 0.3132\n",
      "Epoch 439/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1889 - mae: 0.3402\n",
      "Epoch 439: val_loss improved from 0.15736 to 0.15732, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1889 - mae: 0.3402 - val_loss: 0.1573 - val_mae: 0.3132\n",
      "Epoch 440/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1885 - mae: 0.3399\n",
      "Epoch 440: val_loss improved from 0.15732 to 0.15727, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1885 - mae: 0.3399 - val_loss: 0.1573 - val_mae: 0.3131\n",
      "Epoch 441/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1880 - mae: 0.3392\n",
      "Epoch 441: val_loss improved from 0.15727 to 0.15719, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1880 - mae: 0.3392 - val_loss: 0.1572 - val_mae: 0.3131\n",
      "Epoch 442/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1886 - mae: 0.3398\n",
      "Epoch 442: val_loss improved from 0.15719 to 0.15709, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1886 - mae: 0.3398 - val_loss: 0.1571 - val_mae: 0.3130\n",
      "Epoch 443/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1888 - mae: 0.3398\n",
      "Epoch 443: val_loss improved from 0.15709 to 0.15699, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1888 - mae: 0.3398 - val_loss: 0.1570 - val_mae: 0.3129\n",
      "Epoch 444/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1873 - mae: 0.3389\n",
      "Epoch 444: val_loss improved from 0.15699 to 0.15691, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1873 - mae: 0.3389 - val_loss: 0.1569 - val_mae: 0.3128\n",
      "Epoch 445/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1880 - mae: 0.3392\n",
      "Epoch 445: val_loss improved from 0.15691 to 0.15683, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1880 - mae: 0.3392 - val_loss: 0.1568 - val_mae: 0.3127\n",
      "Epoch 446/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1876 - mae: 0.3391\n",
      "Epoch 446: val_loss improved from 0.15683 to 0.15676, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1876 - mae: 0.3391 - val_loss: 0.1568 - val_mae: 0.3127\n",
      "Epoch 447/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1869 - mae: 0.3382\n",
      "Epoch 447: val_loss improved from 0.15676 to 0.15669, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1869 - mae: 0.3382 - val_loss: 0.1567 - val_mae: 0.3126\n",
      "Epoch 448/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1875 - mae: 0.3391\n",
      "Epoch 448: val_loss improved from 0.15669 to 0.15663, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1875 - mae: 0.3391 - val_loss: 0.1566 - val_mae: 0.3125\n",
      "Epoch 449/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1873 - mae: 0.3385\n",
      "Epoch 449: val_loss improved from 0.15663 to 0.15656, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1873 - mae: 0.3385 - val_loss: 0.1566 - val_mae: 0.3125\n",
      "Epoch 450/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1876 - mae: 0.3391\n",
      "Epoch 450: val_loss improved from 0.15656 to 0.15649, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1876 - mae: 0.3391 - val_loss: 0.1565 - val_mae: 0.3124\n",
      "Epoch 451/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1874 - mae: 0.3386\n",
      "Epoch 451: val_loss improved from 0.15649 to 0.15640, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1874 - mae: 0.3386 - val_loss: 0.1564 - val_mae: 0.3123\n",
      "Epoch 452/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1876 - mae: 0.3389\n",
      "Epoch 452: val_loss improved from 0.15640 to 0.15630, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1876 - mae: 0.3389 - val_loss: 0.1563 - val_mae: 0.3122\n",
      "Epoch 453/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1868 - mae: 0.3381\n",
      "Epoch 453: val_loss improved from 0.15630 to 0.15621, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1868 - mae: 0.3381 - val_loss: 0.1562 - val_mae: 0.3121\n",
      "Epoch 454/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1860 - mae: 0.3378\n",
      "Epoch 454: val_loss improved from 0.15621 to 0.15612, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1860 - mae: 0.3378 - val_loss: 0.1561 - val_mae: 0.3121\n",
      "Epoch 455/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1873 - mae: 0.3386\n",
      "Epoch 455: val_loss improved from 0.15612 to 0.15605, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1873 - mae: 0.3386 - val_loss: 0.1561 - val_mae: 0.3120\n",
      "Epoch 456/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1873 - mae: 0.3389\n",
      "Epoch 456: val_loss improved from 0.15605 to 0.15598, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1873 - mae: 0.3389 - val_loss: 0.1560 - val_mae: 0.3119\n",
      "Epoch 457/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1868 - mae: 0.3388\n",
      "Epoch 457: val_loss improved from 0.15598 to 0.15590, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1868 - mae: 0.3388 - val_loss: 0.1559 - val_mae: 0.3118\n",
      "Epoch 458/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1866 - mae: 0.3382\n",
      "Epoch 458: val_loss improved from 0.15590 to 0.15581, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1866 - mae: 0.3382 - val_loss: 0.1558 - val_mae: 0.3117\n",
      "Epoch 459/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1865 - mae: 0.3384\n",
      "Epoch 459: val_loss improved from 0.15581 to 0.15573, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1865 - mae: 0.3384 - val_loss: 0.1557 - val_mae: 0.3116\n",
      "Epoch 460/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1862 - mae: 0.3381\n",
      "Epoch 460: val_loss improved from 0.15573 to 0.15565, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1862 - mae: 0.3381 - val_loss: 0.1557 - val_mae: 0.3116\n",
      "Epoch 461/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1860 - mae: 0.3378\n",
      "Epoch 461: val_loss improved from 0.15565 to 0.15557, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1860 - mae: 0.3378 - val_loss: 0.1556 - val_mae: 0.3115\n",
      "Epoch 462/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1860 - mae: 0.3379\n",
      "Epoch 462: val_loss improved from 0.15557 to 0.15550, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1860 - mae: 0.3379 - val_loss: 0.1555 - val_mae: 0.3114\n",
      "Epoch 463/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1868 - mae: 0.3381\n",
      "Epoch 463: val_loss improved from 0.15550 to 0.15541, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1868 - mae: 0.3381 - val_loss: 0.1554 - val_mae: 0.3113\n",
      "Epoch 464/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1859 - mae: 0.3378\n",
      "Epoch 464: val_loss improved from 0.15541 to 0.15532, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1859 - mae: 0.3378 - val_loss: 0.1553 - val_mae: 0.3112\n",
      "Epoch 465/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1867 - mae: 0.3384\n",
      "Epoch 465: val_loss improved from 0.15532 to 0.15523, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1867 - mae: 0.3384 - val_loss: 0.1552 - val_mae: 0.3112\n",
      "Epoch 466/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1847 - mae: 0.3368\n",
      "Epoch 466: val_loss improved from 0.15523 to 0.15517, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1847 - mae: 0.3368 - val_loss: 0.1552 - val_mae: 0.3111\n",
      "Epoch 467/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1855 - mae: 0.3374\n",
      "Epoch 467: val_loss improved from 0.15517 to 0.15512, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1855 - mae: 0.3374 - val_loss: 0.1551 - val_mae: 0.3110\n",
      "Epoch 468/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1861 - mae: 0.3381\n",
      "Epoch 468: val_loss improved from 0.15512 to 0.15509, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1861 - mae: 0.3381 - val_loss: 0.1551 - val_mae: 0.3110\n",
      "Epoch 469/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1849 - mae: 0.3370\n",
      "Epoch 469: val_loss improved from 0.15509 to 0.15507, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1849 - mae: 0.3370 - val_loss: 0.1551 - val_mae: 0.3110\n",
      "Epoch 470/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1858 - mae: 0.3375\n",
      "Epoch 470: val_loss improved from 0.15507 to 0.15504, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1858 - mae: 0.3375 - val_loss: 0.1550 - val_mae: 0.3109\n",
      "Epoch 471/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1853 - mae: 0.3371\n",
      "Epoch 471: val_loss improved from 0.15504 to 0.15499, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1853 - mae: 0.3371 - val_loss: 0.1550 - val_mae: 0.3108\n",
      "Epoch 472/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1852 - mae: 0.3371\n",
      "Epoch 472: val_loss improved from 0.15499 to 0.15491, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1852 - mae: 0.3371 - val_loss: 0.1549 - val_mae: 0.3108\n",
      "Epoch 473/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1851 - mae: 0.3370\n",
      "Epoch 473: val_loss improved from 0.15491 to 0.15481, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1851 - mae: 0.3370 - val_loss: 0.1548 - val_mae: 0.3107\n",
      "Epoch 474/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1850 - mae: 0.3368\n",
      "Epoch 474: val_loss improved from 0.15481 to 0.15472, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1850 - mae: 0.3368 - val_loss: 0.1547 - val_mae: 0.3106\n",
      "Epoch 475/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1849 - mae: 0.3369\n",
      "Epoch 475: val_loss improved from 0.15472 to 0.15463, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1849 - mae: 0.3369 - val_loss: 0.1546 - val_mae: 0.3105\n",
      "Epoch 476/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1846 - mae: 0.3368\n",
      "Epoch 476: val_loss improved from 0.15463 to 0.15453, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1846 - mae: 0.3368 - val_loss: 0.1545 - val_mae: 0.3104\n",
      "Epoch 477/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1850 - mae: 0.3370\n",
      "Epoch 477: val_loss improved from 0.15453 to 0.15445, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1850 - mae: 0.3370 - val_loss: 0.1545 - val_mae: 0.3103\n",
      "Epoch 478/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1843 - mae: 0.3364\n",
      "Epoch 478: val_loss improved from 0.15445 to 0.15438, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1843 - mae: 0.3364 - val_loss: 0.1544 - val_mae: 0.3103\n",
      "Epoch 479/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1846 - mae: 0.3367\n",
      "Epoch 479: val_loss improved from 0.15438 to 0.15430, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1846 - mae: 0.3367 - val_loss: 0.1543 - val_mae: 0.3102\n",
      "Epoch 480/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1845 - mae: 0.3365\n",
      "Epoch 480: val_loss improved from 0.15430 to 0.15424, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1845 - mae: 0.3365 - val_loss: 0.1542 - val_mae: 0.3101\n",
      "Epoch 481/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1843 - mae: 0.3363\n",
      "Epoch 481: val_loss improved from 0.15424 to 0.15420, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1843 - mae: 0.3363 - val_loss: 0.1542 - val_mae: 0.3100\n",
      "Epoch 482/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1842 - mae: 0.3362\n",
      "Epoch 482: val_loss improved from 0.15420 to 0.15416, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1842 - mae: 0.3362 - val_loss: 0.1542 - val_mae: 0.3100\n",
      "Epoch 483/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1837 - mae: 0.3358\n",
      "Epoch 483: val_loss improved from 0.15416 to 0.15411, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1837 - mae: 0.3358 - val_loss: 0.1541 - val_mae: 0.3099\n",
      "Epoch 484/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1845 - mae: 0.3366\n",
      "Epoch 484: val_loss improved from 0.15411 to 0.15406, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1845 - mae: 0.3366 - val_loss: 0.1541 - val_mae: 0.3099\n",
      "Epoch 485/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1834 - mae: 0.3353\n",
      "Epoch 485: val_loss improved from 0.15406 to 0.15399, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1834 - mae: 0.3353 - val_loss: 0.1540 - val_mae: 0.3098\n",
      "Epoch 486/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1836 - mae: 0.3359\n",
      "Epoch 486: val_loss improved from 0.15399 to 0.15392, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1836 - mae: 0.3359 - val_loss: 0.1539 - val_mae: 0.3097\n",
      "Epoch 487/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1836 - mae: 0.3358\n",
      "Epoch 487: val_loss improved from 0.15392 to 0.15384, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1836 - mae: 0.3358 - val_loss: 0.1538 - val_mae: 0.3097\n",
      "Epoch 488/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1840 - mae: 0.3359\n",
      "Epoch 488: val_loss improved from 0.15384 to 0.15376, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1840 - mae: 0.3359 - val_loss: 0.1538 - val_mae: 0.3096\n",
      "Epoch 489/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1832 - mae: 0.3354\n",
      "Epoch 489: val_loss improved from 0.15376 to 0.15368, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1832 - mae: 0.3354 - val_loss: 0.1537 - val_mae: 0.3095\n",
      "Epoch 490/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1829 - mae: 0.3354\n",
      "Epoch 490: val_loss improved from 0.15368 to 0.15359, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1829 - mae: 0.3354 - val_loss: 0.1536 - val_mae: 0.3094\n",
      "Epoch 491/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1831 - mae: 0.3354\n",
      "Epoch 491: val_loss improved from 0.15359 to 0.15349, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1831 - mae: 0.3354 - val_loss: 0.1535 - val_mae: 0.3093\n",
      "Epoch 492/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1837 - mae: 0.3357\n",
      "Epoch 492: val_loss improved from 0.15349 to 0.15340, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1837 - mae: 0.3357 - val_loss: 0.1534 - val_mae: 0.3093\n",
      "Epoch 493/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1821 - mae: 0.3348\n",
      "Epoch 493: val_loss improved from 0.15340 to 0.15334, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1821 - mae: 0.3348 - val_loss: 0.1533 - val_mae: 0.3092\n",
      "Epoch 494/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1830 - mae: 0.3355\n",
      "Epoch 494: val_loss improved from 0.15334 to 0.15330, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1830 - mae: 0.3355 - val_loss: 0.1533 - val_mae: 0.3091\n",
      "Epoch 495/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1827 - mae: 0.3348\n",
      "Epoch 495: val_loss improved from 0.15330 to 0.15327, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1827 - mae: 0.3348 - val_loss: 0.1533 - val_mae: 0.3091\n",
      "Epoch 496/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1830 - mae: 0.3354\n",
      "Epoch 496: val_loss improved from 0.15327 to 0.15323, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1830 - mae: 0.3354 - val_loss: 0.1532 - val_mae: 0.3090\n",
      "Epoch 497/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1823 - mae: 0.3348\n",
      "Epoch 497: val_loss improved from 0.15323 to 0.15317, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1823 - mae: 0.3348 - val_loss: 0.1532 - val_mae: 0.3090\n",
      "Epoch 498/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1815 - mae: 0.3341\n",
      "Epoch 498: val_loss improved from 0.15317 to 0.15309, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1815 - mae: 0.3341 - val_loss: 0.1531 - val_mae: 0.3089\n",
      "Epoch 499/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1828 - mae: 0.3348\n",
      "Epoch 499: val_loss improved from 0.15309 to 0.15299, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1828 - mae: 0.3348 - val_loss: 0.1530 - val_mae: 0.3088\n",
      "Epoch 500/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1824 - mae: 0.3350\n",
      "Epoch 500: val_loss improved from 0.15299 to 0.15286, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1824 - mae: 0.3350 - val_loss: 0.1529 - val_mae: 0.3087\n",
      "Epoch 501/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1824 - mae: 0.3350\n",
      "Epoch 501: val_loss improved from 0.15286 to 0.15275, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1824 - mae: 0.3350 - val_loss: 0.1528 - val_mae: 0.3086\n",
      "Epoch 502/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1827 - mae: 0.3352\n",
      "Epoch 502: val_loss improved from 0.15275 to 0.15266, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1827 - mae: 0.3352 - val_loss: 0.1527 - val_mae: 0.3085\n",
      "Epoch 503/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1817 - mae: 0.3344\n",
      "Epoch 503: val_loss improved from 0.15266 to 0.15258, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1817 - mae: 0.3344 - val_loss: 0.1526 - val_mae: 0.3084\n",
      "Epoch 504/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1823 - mae: 0.3348\n",
      "Epoch 504: val_loss improved from 0.15258 to 0.15252, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1823 - mae: 0.3348 - val_loss: 0.1525 - val_mae: 0.3083\n",
      "Epoch 505/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1816 - mae: 0.3342\n",
      "Epoch 505: val_loss improved from 0.15252 to 0.15246, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1816 - mae: 0.3342 - val_loss: 0.1525 - val_mae: 0.3083\n",
      "Epoch 506/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1822 - mae: 0.3347\n",
      "Epoch 506: val_loss improved from 0.15246 to 0.15242, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1822 - mae: 0.3347 - val_loss: 0.1524 - val_mae: 0.3082\n",
      "Epoch 507/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1820 - mae: 0.3344\n",
      "Epoch 507: val_loss improved from 0.15242 to 0.15237, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1820 - mae: 0.3344 - val_loss: 0.1524 - val_mae: 0.3082\n",
      "Epoch 508/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1813 - mae: 0.3339\n",
      "Epoch 508: val_loss improved from 0.15237 to 0.15231, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1813 - mae: 0.3339 - val_loss: 0.1523 - val_mae: 0.3081\n",
      "Epoch 509/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1806 - mae: 0.3334\n",
      "Epoch 509: val_loss improved from 0.15231 to 0.15227, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1806 - mae: 0.3334 - val_loss: 0.1523 - val_mae: 0.3080\n",
      "Epoch 510/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1816 - mae: 0.3345\n",
      "Epoch 510: val_loss improved from 0.15227 to 0.15220, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1816 - mae: 0.3345 - val_loss: 0.1522 - val_mae: 0.3080\n",
      "Epoch 511/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1805 - mae: 0.3334\n",
      "Epoch 511: val_loss improved from 0.15220 to 0.15212, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1805 - mae: 0.3334 - val_loss: 0.1521 - val_mae: 0.3079\n",
      "Epoch 512/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1811 - mae: 0.3336\n",
      "Epoch 512: val_loss improved from 0.15212 to 0.15205, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1811 - mae: 0.3336 - val_loss: 0.1520 - val_mae: 0.3078\n",
      "Epoch 513/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1809 - mae: 0.3335\n",
      "Epoch 513: val_loss improved from 0.15205 to 0.15198, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1809 - mae: 0.3335 - val_loss: 0.1520 - val_mae: 0.3077\n",
      "Epoch 514/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1808 - mae: 0.3334\n",
      "Epoch 514: val_loss improved from 0.15198 to 0.15192, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1808 - mae: 0.3334 - val_loss: 0.1519 - val_mae: 0.3077\n",
      "Epoch 515/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1805 - mae: 0.3329\n",
      "Epoch 515: val_loss improved from 0.15192 to 0.15187, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1805 - mae: 0.3329 - val_loss: 0.1519 - val_mae: 0.3076\n",
      "Epoch 516/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1806 - mae: 0.3331\n",
      "Epoch 516: val_loss improved from 0.15187 to 0.15181, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1806 - mae: 0.3331 - val_loss: 0.1518 - val_mae: 0.3076\n",
      "Epoch 517/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1808 - mae: 0.3334\n",
      "Epoch 517: val_loss improved from 0.15181 to 0.15175, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1808 - mae: 0.3334 - val_loss: 0.1518 - val_mae: 0.3075\n",
      "Epoch 518/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1803 - mae: 0.3331\n",
      "Epoch 518: val_loss improved from 0.15175 to 0.15169, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1803 - mae: 0.3331 - val_loss: 0.1517 - val_mae: 0.3074\n",
      "Epoch 519/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1812 - mae: 0.3339\n",
      "Epoch 519: val_loss improved from 0.15169 to 0.15161, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1812 - mae: 0.3339 - val_loss: 0.1516 - val_mae: 0.3073\n",
      "Epoch 520/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1807 - mae: 0.3333\n",
      "Epoch 520: val_loss improved from 0.15161 to 0.15152, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1807 - mae: 0.3333 - val_loss: 0.1515 - val_mae: 0.3073\n",
      "Epoch 521/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1803 - mae: 0.3331\n",
      "Epoch 521: val_loss improved from 0.15152 to 0.15143, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1803 - mae: 0.3331 - val_loss: 0.1514 - val_mae: 0.3072\n",
      "Epoch 522/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1800 - mae: 0.3328\n",
      "Epoch 522: val_loss improved from 0.15143 to 0.15136, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1800 - mae: 0.3328 - val_loss: 0.1514 - val_mae: 0.3071\n",
      "Epoch 523/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1804 - mae: 0.3333\n",
      "Epoch 523: val_loss improved from 0.15136 to 0.15130, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1804 - mae: 0.3333 - val_loss: 0.1513 - val_mae: 0.3071\n",
      "Epoch 524/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1796 - mae: 0.3325\n",
      "Epoch 524: val_loss improved from 0.15130 to 0.15124, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1796 - mae: 0.3325 - val_loss: 0.1512 - val_mae: 0.3070\n",
      "Epoch 525/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1794 - mae: 0.3324\n",
      "Epoch 525: val_loss improved from 0.15124 to 0.15119, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1794 - mae: 0.3324 - val_loss: 0.1512 - val_mae: 0.3069\n",
      "Epoch 526/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1793 - mae: 0.3323\n",
      "Epoch 526: val_loss improved from 0.15119 to 0.15111, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1793 - mae: 0.3323 - val_loss: 0.1511 - val_mae: 0.3068\n",
      "Epoch 527/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1791 - mae: 0.3319\n",
      "Epoch 527: val_loss improved from 0.15111 to 0.15103, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1791 - mae: 0.3319 - val_loss: 0.1510 - val_mae: 0.3068\n",
      "Epoch 528/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1786 - mae: 0.3314\n",
      "Epoch 528: val_loss improved from 0.15103 to 0.15095, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1786 - mae: 0.3314 - val_loss: 0.1509 - val_mae: 0.3067\n",
      "Epoch 529/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1788 - mae: 0.3321\n",
      "Epoch 529: val_loss improved from 0.15095 to 0.15088, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1788 - mae: 0.3321 - val_loss: 0.1509 - val_mae: 0.3066\n",
      "Epoch 530/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1800 - mae: 0.3329\n",
      "Epoch 530: val_loss improved from 0.15088 to 0.15082, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1800 - mae: 0.3329 - val_loss: 0.1508 - val_mae: 0.3066\n",
      "Epoch 531/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1791 - mae: 0.3323\n",
      "Epoch 531: val_loss improved from 0.15082 to 0.15077, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1791 - mae: 0.3323 - val_loss: 0.1508 - val_mae: 0.3065\n",
      "Epoch 532/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1790 - mae: 0.3322\n",
      "Epoch 532: val_loss improved from 0.15077 to 0.15069, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1790 - mae: 0.3322 - val_loss: 0.1507 - val_mae: 0.3064\n",
      "Epoch 533/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1797 - mae: 0.3326\n",
      "Epoch 533: val_loss improved from 0.15069 to 0.15061, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1797 - mae: 0.3326 - val_loss: 0.1506 - val_mae: 0.3063\n",
      "Epoch 534/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1783 - mae: 0.3314\n",
      "Epoch 534: val_loss improved from 0.15061 to 0.15053, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1783 - mae: 0.3314 - val_loss: 0.1505 - val_mae: 0.3063\n",
      "Epoch 535/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1793 - mae: 0.3321\n",
      "Epoch 535: val_loss improved from 0.15053 to 0.15046, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.1793 - mae: 0.3321 - val_loss: 0.1505 - val_mae: 0.3062\n",
      "Epoch 536/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1786 - mae: 0.3315\n",
      "Epoch 536: val_loss improved from 0.15046 to 0.15041, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1786 - mae: 0.3315 - val_loss: 0.1504 - val_mae: 0.3061\n",
      "Epoch 537/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1791 - mae: 0.3320\n",
      "Epoch 537: val_loss improved from 0.15041 to 0.15035, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1791 - mae: 0.3320 - val_loss: 0.1504 - val_mae: 0.3061\n",
      "Epoch 538/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1799 - mae: 0.3328\n",
      "Epoch 538: val_loss improved from 0.15035 to 0.15027, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1799 - mae: 0.3328 - val_loss: 0.1503 - val_mae: 0.3060\n",
      "Epoch 539/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1784 - mae: 0.3313\n",
      "Epoch 539: val_loss improved from 0.15027 to 0.15019, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1784 - mae: 0.3313 - val_loss: 0.1502 - val_mae: 0.3059\n",
      "Epoch 540/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1787 - mae: 0.3317\n",
      "Epoch 540: val_loss improved from 0.15019 to 0.15012, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1787 - mae: 0.3317 - val_loss: 0.1501 - val_mae: 0.3058\n",
      "Epoch 541/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1782 - mae: 0.3313\n",
      "Epoch 541: val_loss improved from 0.15012 to 0.15006, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1782 - mae: 0.3313 - val_loss: 0.1501 - val_mae: 0.3058\n",
      "Epoch 542/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1784 - mae: 0.3314\n",
      "Epoch 542: val_loss improved from 0.15006 to 0.15001, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1784 - mae: 0.3314 - val_loss: 0.1500 - val_mae: 0.3057\n",
      "Epoch 543/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1783 - mae: 0.3313\n",
      "Epoch 543: val_loss improved from 0.15001 to 0.14995, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1783 - mae: 0.3313 - val_loss: 0.1499 - val_mae: 0.3056\n",
      "Epoch 544/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1785 - mae: 0.3314\n",
      "Epoch 544: val_loss improved from 0.14995 to 0.14987, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1785 - mae: 0.3314 - val_loss: 0.1499 - val_mae: 0.3056\n",
      "Epoch 545/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1777 - mae: 0.3308\n",
      "Epoch 545: val_loss improved from 0.14987 to 0.14980, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1777 - mae: 0.3308 - val_loss: 0.1498 - val_mae: 0.3055\n",
      "Epoch 546/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1775 - mae: 0.3307\n",
      "Epoch 546: val_loss improved from 0.14980 to 0.14973, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1775 - mae: 0.3307 - val_loss: 0.1497 - val_mae: 0.3054\n",
      "Epoch 547/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1774 - mae: 0.3308\n",
      "Epoch 547: val_loss improved from 0.14973 to 0.14967, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1774 - mae: 0.3308 - val_loss: 0.1497 - val_mae: 0.3053\n",
      "Epoch 548/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1778 - mae: 0.3311\n",
      "Epoch 548: val_loss improved from 0.14967 to 0.14960, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1778 - mae: 0.3311 - val_loss: 0.1496 - val_mae: 0.3053\n",
      "Epoch 549/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1781 - mae: 0.3310\n",
      "Epoch 549: val_loss improved from 0.14960 to 0.14952, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1781 - mae: 0.3310 - val_loss: 0.1495 - val_mae: 0.3052\n",
      "Epoch 550/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1777 - mae: 0.3309\n",
      "Epoch 550: val_loss improved from 0.14952 to 0.14946, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1777 - mae: 0.3309 - val_loss: 0.1495 - val_mae: 0.3051\n",
      "Epoch 551/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1775 - mae: 0.3306\n",
      "Epoch 551: val_loss improved from 0.14946 to 0.14940, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1775 - mae: 0.3306 - val_loss: 0.1494 - val_mae: 0.3051\n",
      "Epoch 552/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1771 - mae: 0.3304\n",
      "Epoch 552: val_loss improved from 0.14940 to 0.14935, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1771 - mae: 0.3304 - val_loss: 0.1493 - val_mae: 0.3050\n",
      "Epoch 553/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1774 - mae: 0.3305\n",
      "Epoch 553: val_loss improved from 0.14935 to 0.14929, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1774 - mae: 0.3305 - val_loss: 0.1493 - val_mae: 0.3050\n",
      "Epoch 554/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1768 - mae: 0.3303\n",
      "Epoch 554: val_loss improved from 0.14929 to 0.14926, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1768 - mae: 0.3303 - val_loss: 0.1493 - val_mae: 0.3049\n",
      "Epoch 555/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1769 - mae: 0.3302\n",
      "Epoch 555: val_loss improved from 0.14926 to 0.14921, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1769 - mae: 0.3302 - val_loss: 0.1492 - val_mae: 0.3049\n",
      "Epoch 556/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1769 - mae: 0.3302\n",
      "Epoch 556: val_loss improved from 0.14921 to 0.14914, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1769 - mae: 0.3302 - val_loss: 0.1491 - val_mae: 0.3048\n",
      "Epoch 557/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1768 - mae: 0.3303\n",
      "Epoch 557: val_loss improved from 0.14914 to 0.14905, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1768 - mae: 0.3303 - val_loss: 0.1491 - val_mae: 0.3047\n",
      "Epoch 558/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1770 - mae: 0.3301\n",
      "Epoch 558: val_loss improved from 0.14905 to 0.14897, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1770 - mae: 0.3301 - val_loss: 0.1490 - val_mae: 0.3046\n",
      "Epoch 559/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1769 - mae: 0.3304\n",
      "Epoch 559: val_loss improved from 0.14897 to 0.14888, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1769 - mae: 0.3304 - val_loss: 0.1489 - val_mae: 0.3045\n",
      "Epoch 560/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1762 - mae: 0.3296\n",
      "Epoch 560: val_loss improved from 0.14888 to 0.14880, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1762 - mae: 0.3296 - val_loss: 0.1488 - val_mae: 0.3045\n",
      "Epoch 561/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1764 - mae: 0.3299\n",
      "Epoch 561: val_loss improved from 0.14880 to 0.14873, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1764 - mae: 0.3299 - val_loss: 0.1487 - val_mae: 0.3044\n",
      "Epoch 562/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1761 - mae: 0.3295\n",
      "Epoch 562: val_loss improved from 0.14873 to 0.14868, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1761 - mae: 0.3295 - val_loss: 0.1487 - val_mae: 0.3043\n",
      "Epoch 563/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1765 - mae: 0.3297\n",
      "Epoch 563: val_loss improved from 0.14868 to 0.14860, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1765 - mae: 0.3297 - val_loss: 0.1486 - val_mae: 0.3042\n",
      "Epoch 564/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1767 - mae: 0.3299\n",
      "Epoch 564: val_loss improved from 0.14860 to 0.14853, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1767 - mae: 0.3299 - val_loss: 0.1485 - val_mae: 0.3042\n",
      "Epoch 565/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1768 - mae: 0.3299\n",
      "Epoch 565: val_loss improved from 0.14853 to 0.14845, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1768 - mae: 0.3299 - val_loss: 0.1484 - val_mae: 0.3041\n",
      "Epoch 566/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1764 - mae: 0.3302\n",
      "Epoch 566: val_loss improved from 0.14845 to 0.14836, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1764 - mae: 0.3302 - val_loss: 0.1484 - val_mae: 0.3040\n",
      "Epoch 567/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1756 - mae: 0.3290\n",
      "Epoch 567: val_loss improved from 0.14836 to 0.14828, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1756 - mae: 0.3290 - val_loss: 0.1483 - val_mae: 0.3039\n",
      "Epoch 568/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1759 - mae: 0.3294\n",
      "Epoch 568: val_loss improved from 0.14828 to 0.14821, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1759 - mae: 0.3294 - val_loss: 0.1482 - val_mae: 0.3038\n",
      "Epoch 569/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1755 - mae: 0.3290\n",
      "Epoch 569: val_loss improved from 0.14821 to 0.14815, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1755 - mae: 0.3290 - val_loss: 0.1481 - val_mae: 0.3038\n",
      "Epoch 570/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1756 - mae: 0.3289\n",
      "Epoch 570: val_loss improved from 0.14815 to 0.14809, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1756 - mae: 0.3289 - val_loss: 0.1481 - val_mae: 0.3037\n",
      "Epoch 571/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1750 - mae: 0.3282\n",
      "Epoch 571: val_loss improved from 0.14809 to 0.14804, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1750 - mae: 0.3282 - val_loss: 0.1480 - val_mae: 0.3037\n",
      "Epoch 572/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1751 - mae: 0.3288\n",
      "Epoch 572: val_loss improved from 0.14804 to 0.14801, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1751 - mae: 0.3288 - val_loss: 0.1480 - val_mae: 0.3036\n",
      "Epoch 573/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1749 - mae: 0.3284\n",
      "Epoch 573: val_loss improved from 0.14801 to 0.14796, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1749 - mae: 0.3284 - val_loss: 0.1480 - val_mae: 0.3036\n",
      "Epoch 574/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1753 - mae: 0.3288\n",
      "Epoch 574: val_loss improved from 0.14796 to 0.14793, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1753 - mae: 0.3288 - val_loss: 0.1479 - val_mae: 0.3035\n",
      "Epoch 575/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1751 - mae: 0.3284\n",
      "Epoch 575: val_loss improved from 0.14793 to 0.14788, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1751 - mae: 0.3284 - val_loss: 0.1479 - val_mae: 0.3035\n",
      "Epoch 576/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1744 - mae: 0.3280\n",
      "Epoch 576: val_loss improved from 0.14788 to 0.14783, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1744 - mae: 0.3280 - val_loss: 0.1478 - val_mae: 0.3034\n",
      "Epoch 577/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1750 - mae: 0.3284\n",
      "Epoch 577: val_loss improved from 0.14783 to 0.14776, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1750 - mae: 0.3284 - val_loss: 0.1478 - val_mae: 0.3034\n",
      "Epoch 578/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1742 - mae: 0.3277\n",
      "Epoch 578: val_loss improved from 0.14776 to 0.14768, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1742 - mae: 0.3277 - val_loss: 0.1477 - val_mae: 0.3033\n",
      "Epoch 579/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1751 - mae: 0.3286\n",
      "Epoch 579: val_loss improved from 0.14768 to 0.14757, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1751 - mae: 0.3286 - val_loss: 0.1476 - val_mae: 0.3032\n",
      "Epoch 580/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1747 - mae: 0.3283\n",
      "Epoch 580: val_loss improved from 0.14757 to 0.14749, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1747 - mae: 0.3283 - val_loss: 0.1475 - val_mae: 0.3031\n",
      "Epoch 581/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1747 - mae: 0.3283\n",
      "Epoch 581: val_loss improved from 0.14749 to 0.14744, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1747 - mae: 0.3283 - val_loss: 0.1474 - val_mae: 0.3031\n",
      "Epoch 582/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1749 - mae: 0.3283\n",
      "Epoch 582: val_loss improved from 0.14744 to 0.14740, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1749 - mae: 0.3283 - val_loss: 0.1474 - val_mae: 0.3030\n",
      "Epoch 583/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1746 - mae: 0.3282\n",
      "Epoch 583: val_loss improved from 0.14740 to 0.14735, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1746 - mae: 0.3282 - val_loss: 0.1474 - val_mae: 0.3029\n",
      "Epoch 584/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1744 - mae: 0.3283\n",
      "Epoch 584: val_loss improved from 0.14735 to 0.14731, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1744 - mae: 0.3283 - val_loss: 0.1473 - val_mae: 0.3029\n",
      "Epoch 585/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1745 - mae: 0.3282\n",
      "Epoch 585: val_loss improved from 0.14731 to 0.14726, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1745 - mae: 0.3282 - val_loss: 0.1473 - val_mae: 0.3028\n",
      "Epoch 586/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1742 - mae: 0.3277\n",
      "Epoch 586: val_loss improved from 0.14726 to 0.14719, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1742 - mae: 0.3277 - val_loss: 0.1472 - val_mae: 0.3027\n",
      "Epoch 587/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1733 - mae: 0.3272\n",
      "Epoch 587: val_loss improved from 0.14719 to 0.14713, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1733 - mae: 0.3272 - val_loss: 0.1471 - val_mae: 0.3026\n",
      "Epoch 588/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1743 - mae: 0.3276\n",
      "Epoch 588: val_loss improved from 0.14713 to 0.14708, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1743 - mae: 0.3276 - val_loss: 0.1471 - val_mae: 0.3026\n",
      "Epoch 589/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1738 - mae: 0.3276\n",
      "Epoch 589: val_loss improved from 0.14708 to 0.14701, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1738 - mae: 0.3276 - val_loss: 0.1470 - val_mae: 0.3025\n",
      "Epoch 590/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1739 - mae: 0.3275\n",
      "Epoch 590: val_loss improved from 0.14701 to 0.14692, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1739 - mae: 0.3275 - val_loss: 0.1469 - val_mae: 0.3024\n",
      "Epoch 591/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1735 - mae: 0.3274\n",
      "Epoch 591: val_loss improved from 0.14692 to 0.14683, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1735 - mae: 0.3274 - val_loss: 0.1468 - val_mae: 0.3023\n",
      "Epoch 592/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1741 - mae: 0.3275\n",
      "Epoch 592: val_loss improved from 0.14683 to 0.14674, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1741 - mae: 0.3275 - val_loss: 0.1467 - val_mae: 0.3023\n",
      "Epoch 593/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1727 - mae: 0.3266\n",
      "Epoch 593: val_loss improved from 0.14674 to 0.14667, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1727 - mae: 0.3266 - val_loss: 0.1467 - val_mae: 0.3022\n",
      "Epoch 594/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1736 - mae: 0.3273\n",
      "Epoch 594: val_loss improved from 0.14667 to 0.14663, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1736 - mae: 0.3273 - val_loss: 0.1466 - val_mae: 0.3021\n",
      "Epoch 595/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1731 - mae: 0.3266\n",
      "Epoch 595: val_loss improved from 0.14663 to 0.14661, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1731 - mae: 0.3266 - val_loss: 0.1466 - val_mae: 0.3021\n",
      "Epoch 596/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1734 - mae: 0.3271\n",
      "Epoch 596: val_loss improved from 0.14661 to 0.14660, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1734 - mae: 0.3271 - val_loss: 0.1466 - val_mae: 0.3021\n",
      "Epoch 597/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1731 - mae: 0.3268\n",
      "Epoch 597: val_loss improved from 0.14660 to 0.14657, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1731 - mae: 0.3268 - val_loss: 0.1466 - val_mae: 0.3020\n",
      "Epoch 598/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1732 - mae: 0.3273\n",
      "Epoch 598: val_loss improved from 0.14657 to 0.14653, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1732 - mae: 0.3273 - val_loss: 0.1465 - val_mae: 0.3020\n",
      "Epoch 599/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1734 - mae: 0.3272\n",
      "Epoch 599: val_loss improved from 0.14653 to 0.14646, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1734 - mae: 0.3272 - val_loss: 0.1465 - val_mae: 0.3019\n",
      "Epoch 600/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1726 - mae: 0.3262\n",
      "Epoch 600: val_loss improved from 0.14646 to 0.14637, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1726 - mae: 0.3262 - val_loss: 0.1464 - val_mae: 0.3018\n",
      "Epoch 601/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1727 - mae: 0.3264\n",
      "Epoch 601: val_loss improved from 0.14637 to 0.14628, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1727 - mae: 0.3264 - val_loss: 0.1463 - val_mae: 0.3018\n",
      "Epoch 602/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1724 - mae: 0.3260\n",
      "Epoch 602: val_loss improved from 0.14628 to 0.14620, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1724 - mae: 0.3260 - val_loss: 0.1462 - val_mae: 0.3017\n",
      "Epoch 603/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1731 - mae: 0.3270\n",
      "Epoch 603: val_loss improved from 0.14620 to 0.14612, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1731 - mae: 0.3270 - val_loss: 0.1461 - val_mae: 0.3016\n",
      "Epoch 604/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1733 - mae: 0.3271\n",
      "Epoch 604: val_loss improved from 0.14612 to 0.14604, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1733 - mae: 0.3271 - val_loss: 0.1460 - val_mae: 0.3015\n",
      "Epoch 605/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1722 - mae: 0.3262\n",
      "Epoch 605: val_loss improved from 0.14604 to 0.14597, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1722 - mae: 0.3262 - val_loss: 0.1460 - val_mae: 0.3015\n",
      "Epoch 606/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1732 - mae: 0.3269\n",
      "Epoch 606: val_loss improved from 0.14597 to 0.14589, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1732 - mae: 0.3269 - val_loss: 0.1459 - val_mae: 0.3014\n",
      "Epoch 607/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1721 - mae: 0.3264\n",
      "Epoch 607: val_loss improved from 0.14589 to 0.14584, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1721 - mae: 0.3264 - val_loss: 0.1458 - val_mae: 0.3013\n",
      "Epoch 608/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1726 - mae: 0.3265\n",
      "Epoch 608: val_loss improved from 0.14584 to 0.14580, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1726 - mae: 0.3265 - val_loss: 0.1458 - val_mae: 0.3013\n",
      "Epoch 609/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1726 - mae: 0.3263\n",
      "Epoch 609: val_loss improved from 0.14580 to 0.14576, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1726 - mae: 0.3263 - val_loss: 0.1458 - val_mae: 0.3012\n",
      "Epoch 610/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1716 - mae: 0.3256\n",
      "Epoch 610: val_loss improved from 0.14576 to 0.14573, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1716 - mae: 0.3256 - val_loss: 0.1457 - val_mae: 0.3012\n",
      "Epoch 611/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1719 - mae: 0.3257\n",
      "Epoch 611: val_loss improved from 0.14573 to 0.14569, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1719 - mae: 0.3257 - val_loss: 0.1457 - val_mae: 0.3011\n",
      "Epoch 612/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1717 - mae: 0.3257\n",
      "Epoch 612: val_loss improved from 0.14569 to 0.14564, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1717 - mae: 0.3257 - val_loss: 0.1456 - val_mae: 0.3010\n",
      "Epoch 613/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1726 - mae: 0.3263\n",
      "Epoch 613: val_loss improved from 0.14564 to 0.14555, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1726 - mae: 0.3263 - val_loss: 0.1456 - val_mae: 0.3010\n",
      "Epoch 614/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1717 - mae: 0.3255\n",
      "Epoch 614: val_loss improved from 0.14555 to 0.14546, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1717 - mae: 0.3255 - val_loss: 0.1455 - val_mae: 0.3009\n",
      "Epoch 615/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1717 - mae: 0.3258\n",
      "Epoch 615: val_loss improved from 0.14546 to 0.14536, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1717 - mae: 0.3258 - val_loss: 0.1454 - val_mae: 0.3008\n",
      "Epoch 616/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1716 - mae: 0.3256\n",
      "Epoch 616: val_loss improved from 0.14536 to 0.14525, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1716 - mae: 0.3256 - val_loss: 0.1452 - val_mae: 0.3007\n",
      "Epoch 617/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1717 - mae: 0.3258\n",
      "Epoch 617: val_loss improved from 0.14525 to 0.14515, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1717 - mae: 0.3258 - val_loss: 0.1451 - val_mae: 0.3006\n",
      "Epoch 618/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1715 - mae: 0.3254\n",
      "Epoch 618: val_loss improved from 0.14515 to 0.14506, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1715 - mae: 0.3254 - val_loss: 0.1451 - val_mae: 0.3005\n",
      "Epoch 619/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1716 - mae: 0.3258\n",
      "Epoch 619: val_loss improved from 0.14506 to 0.14502, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1716 - mae: 0.3258 - val_loss: 0.1450 - val_mae: 0.3005\n",
      "Epoch 620/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1714 - mae: 0.3254\n",
      "Epoch 620: val_loss improved from 0.14502 to 0.14499, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1714 - mae: 0.3254 - val_loss: 0.1450 - val_mae: 0.3004\n",
      "Epoch 621/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1712 - mae: 0.3251\n",
      "Epoch 621: val_loss improved from 0.14499 to 0.14496, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1712 - mae: 0.3251 - val_loss: 0.1450 - val_mae: 0.3004\n",
      "Epoch 622/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1708 - mae: 0.3249\n",
      "Epoch 622: val_loss improved from 0.14496 to 0.14495, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1708 - mae: 0.3249 - val_loss: 0.1449 - val_mae: 0.3003\n",
      "Epoch 623/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1709 - mae: 0.3251\n",
      "Epoch 623: val_loss improved from 0.14495 to 0.14492, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1709 - mae: 0.3251 - val_loss: 0.1449 - val_mae: 0.3003\n",
      "Epoch 624/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1706 - mae: 0.3249\n",
      "Epoch 624: val_loss improved from 0.14492 to 0.14485, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1706 - mae: 0.3249 - val_loss: 0.1448 - val_mae: 0.3002\n",
      "Epoch 625/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1711 - mae: 0.3251\n",
      "Epoch 625: val_loss improved from 0.14485 to 0.14475, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1711 - mae: 0.3251 - val_loss: 0.1448 - val_mae: 0.3001\n",
      "Epoch 626/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1700 - mae: 0.3242\n",
      "Epoch 626: val_loss improved from 0.14475 to 0.14465, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1700 - mae: 0.3242 - val_loss: 0.1447 - val_mae: 0.3001\n",
      "Epoch 627/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1713 - mae: 0.3253\n",
      "Epoch 627: val_loss improved from 0.14465 to 0.14456, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1713 - mae: 0.3253 - val_loss: 0.1446 - val_mae: 0.3000\n",
      "Epoch 628/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1704 - mae: 0.3247\n",
      "Epoch 628: val_loss improved from 0.14456 to 0.14448, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1704 - mae: 0.3247 - val_loss: 0.1445 - val_mae: 0.2999\n",
      "Epoch 629/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1704 - mae: 0.3244\n",
      "Epoch 629: val_loss improved from 0.14448 to 0.14443, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1704 - mae: 0.3244 - val_loss: 0.1444 - val_mae: 0.2998\n",
      "Epoch 630/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1702 - mae: 0.3241\n",
      "Epoch 630: val_loss improved from 0.14443 to 0.14440, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1702 - mae: 0.3241 - val_loss: 0.1444 - val_mae: 0.2998\n",
      "Epoch 631/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1706 - mae: 0.3246\n",
      "Epoch 631: val_loss improved from 0.14440 to 0.14435, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1706 - mae: 0.3246 - val_loss: 0.1444 - val_mae: 0.2997\n",
      "Epoch 632/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1705 - mae: 0.3247\n",
      "Epoch 632: val_loss improved from 0.14435 to 0.14432, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1705 - mae: 0.3247 - val_loss: 0.1443 - val_mae: 0.2997\n",
      "Epoch 633/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1701 - mae: 0.3243\n",
      "Epoch 633: val_loss improved from 0.14432 to 0.14427, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1701 - mae: 0.3243 - val_loss: 0.1443 - val_mae: 0.2996\n",
      "Epoch 634/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1700 - mae: 0.3241\n",
      "Epoch 634: val_loss improved from 0.14427 to 0.14421, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1700 - mae: 0.3241 - val_loss: 0.1442 - val_mae: 0.2995\n",
      "Epoch 635/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1696 - mae: 0.3240\n",
      "Epoch 635: val_loss improved from 0.14421 to 0.14414, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1696 - mae: 0.3240 - val_loss: 0.1441 - val_mae: 0.2995\n",
      "Epoch 636/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1695 - mae: 0.3238\n",
      "Epoch 636: val_loss improved from 0.14414 to 0.14407, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1695 - mae: 0.3238 - val_loss: 0.1441 - val_mae: 0.2994\n",
      "Epoch 637/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1704 - mae: 0.3243\n",
      "Epoch 637: val_loss improved from 0.14407 to 0.14399, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1704 - mae: 0.3243 - val_loss: 0.1440 - val_mae: 0.2993\n",
      "Epoch 638/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1705 - mae: 0.3245\n",
      "Epoch 638: val_loss improved from 0.14399 to 0.14390, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1705 - mae: 0.3245 - val_loss: 0.1439 - val_mae: 0.2992\n",
      "Epoch 639/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1699 - mae: 0.3239\n",
      "Epoch 639: val_loss improved from 0.14390 to 0.14384, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1699 - mae: 0.3239 - val_loss: 0.1438 - val_mae: 0.2992\n",
      "Epoch 640/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1701 - mae: 0.3242\n",
      "Epoch 640: val_loss improved from 0.14384 to 0.14379, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1701 - mae: 0.3242 - val_loss: 0.1438 - val_mae: 0.2991\n",
      "Epoch 641/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1694 - mae: 0.3237\n",
      "Epoch 641: val_loss improved from 0.14379 to 0.14374, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1694 - mae: 0.3237 - val_loss: 0.1437 - val_mae: 0.2990\n",
      "Epoch 642/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1695 - mae: 0.3236\n",
      "Epoch 642: val_loss improved from 0.14374 to 0.14366, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1695 - mae: 0.3236 - val_loss: 0.1437 - val_mae: 0.2990\n",
      "Epoch 643/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1690 - mae: 0.3233\n",
      "Epoch 643: val_loss improved from 0.14366 to 0.14359, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1690 - mae: 0.3233 - val_loss: 0.1436 - val_mae: 0.2989\n",
      "Epoch 644/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1687 - mae: 0.3230\n",
      "Epoch 644: val_loss improved from 0.14359 to 0.14354, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1687 - mae: 0.3230 - val_loss: 0.1435 - val_mae: 0.2988\n",
      "Epoch 645/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1693 - mae: 0.3234\n",
      "Epoch 645: val_loss improved from 0.14354 to 0.14349, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1693 - mae: 0.3234 - val_loss: 0.1435 - val_mae: 0.2988\n",
      "Epoch 646/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1688 - mae: 0.3229\n",
      "Epoch 646: val_loss improved from 0.14349 to 0.14343, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1688 - mae: 0.3229 - val_loss: 0.1434 - val_mae: 0.2987\n",
      "Epoch 647/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1689 - mae: 0.3234\n",
      "Epoch 647: val_loss improved from 0.14343 to 0.14336, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1689 - mae: 0.3234 - val_loss: 0.1434 - val_mae: 0.2987\n",
      "Epoch 648/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1696 - mae: 0.3236\n",
      "Epoch 648: val_loss improved from 0.14336 to 0.14329, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1696 - mae: 0.3236 - val_loss: 0.1433 - val_mae: 0.2986\n",
      "Epoch 649/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1693 - mae: 0.3234\n",
      "Epoch 649: val_loss improved from 0.14329 to 0.14322, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1693 - mae: 0.3234 - val_loss: 0.1432 - val_mae: 0.2985\n",
      "Epoch 650/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1698 - mae: 0.3239\n",
      "Epoch 650: val_loss improved from 0.14322 to 0.14314, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1698 - mae: 0.3239 - val_loss: 0.1431 - val_mae: 0.2985\n",
      "Epoch 651/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1689 - mae: 0.3230\n",
      "Epoch 651: val_loss improved from 0.14314 to 0.14306, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1689 - mae: 0.3230 - val_loss: 0.1431 - val_mae: 0.2984\n",
      "Epoch 652/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1687 - mae: 0.3232\n",
      "Epoch 652: val_loss improved from 0.14306 to 0.14299, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1687 - mae: 0.3232 - val_loss: 0.1430 - val_mae: 0.2983\n",
      "Epoch 653/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1695 - mae: 0.3236\n",
      "Epoch 653: val_loss improved from 0.14299 to 0.14294, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1695 - mae: 0.3236 - val_loss: 0.1429 - val_mae: 0.2982\n",
      "Epoch 654/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1682 - mae: 0.3224\n",
      "Epoch 654: val_loss improved from 0.14294 to 0.14289, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1682 - mae: 0.3224 - val_loss: 0.1429 - val_mae: 0.2982\n",
      "Epoch 655/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1687 - mae: 0.3229\n",
      "Epoch 655: val_loss improved from 0.14289 to 0.14283, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1687 - mae: 0.3229 - val_loss: 0.1428 - val_mae: 0.2981\n",
      "Epoch 656/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1685 - mae: 0.3228\n",
      "Epoch 656: val_loss improved from 0.14283 to 0.14276, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1685 - mae: 0.3228 - val_loss: 0.1428 - val_mae: 0.2980\n",
      "Epoch 657/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1686 - mae: 0.3229\n",
      "Epoch 657: val_loss improved from 0.14276 to 0.14269, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1686 - mae: 0.3229 - val_loss: 0.1427 - val_mae: 0.2980\n",
      "Epoch 658/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1681 - mae: 0.3225\n",
      "Epoch 658: val_loss improved from 0.14269 to 0.14265, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1681 - mae: 0.3225 - val_loss: 0.1426 - val_mae: 0.2979\n",
      "Epoch 659/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1682 - mae: 0.3227\n",
      "Epoch 659: val_loss improved from 0.14265 to 0.14258, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1682 - mae: 0.3227 - val_loss: 0.1426 - val_mae: 0.2978\n",
      "Epoch 660/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1680 - mae: 0.3222\n",
      "Epoch 660: val_loss improved from 0.14258 to 0.14251, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1680 - mae: 0.3222 - val_loss: 0.1425 - val_mae: 0.2978\n",
      "Epoch 661/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1683 - mae: 0.3227\n",
      "Epoch 661: val_loss improved from 0.14251 to 0.14246, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1683 - mae: 0.3227 - val_loss: 0.1425 - val_mae: 0.2977\n",
      "Epoch 662/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1681 - mae: 0.3226\n",
      "Epoch 662: val_loss improved from 0.14246 to 0.14240, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1681 - mae: 0.3226 - val_loss: 0.1424 - val_mae: 0.2976\n",
      "Epoch 663/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1674 - mae: 0.3218\n",
      "Epoch 663: val_loss improved from 0.14240 to 0.14235, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1674 - mae: 0.3218 - val_loss: 0.1423 - val_mae: 0.2976\n",
      "Epoch 664/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1682 - mae: 0.3225\n",
      "Epoch 664: val_loss improved from 0.14235 to 0.14227, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1682 - mae: 0.3225 - val_loss: 0.1423 - val_mae: 0.2975\n",
      "Epoch 665/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1675 - mae: 0.3220\n",
      "Epoch 665: val_loss improved from 0.14227 to 0.14219, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1675 - mae: 0.3220 - val_loss: 0.1422 - val_mae: 0.2974\n",
      "Epoch 666/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1670 - mae: 0.3216\n",
      "Epoch 666: val_loss improved from 0.14219 to 0.14213, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1670 - mae: 0.3216 - val_loss: 0.1421 - val_mae: 0.2973\n",
      "Epoch 667/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1671 - mae: 0.3214\n",
      "Epoch 667: val_loss improved from 0.14213 to 0.14208, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1671 - mae: 0.3214 - val_loss: 0.1421 - val_mae: 0.2973\n",
      "Epoch 668/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1676 - mae: 0.3218\n",
      "Epoch 668: val_loss improved from 0.14208 to 0.14205, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1676 - mae: 0.3218 - val_loss: 0.1420 - val_mae: 0.2972\n",
      "Epoch 669/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1677 - mae: 0.3221\n",
      "Epoch 669: val_loss improved from 0.14205 to 0.14200, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1677 - mae: 0.3221 - val_loss: 0.1420 - val_mae: 0.2972\n",
      "Epoch 670/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1669 - mae: 0.3214\n",
      "Epoch 670: val_loss improved from 0.14200 to 0.14196, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1669 - mae: 0.3214 - val_loss: 0.1420 - val_mae: 0.2971\n",
      "Epoch 671/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1675 - mae: 0.3218\n",
      "Epoch 671: val_loss improved from 0.14196 to 0.14192, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1675 - mae: 0.3218 - val_loss: 0.1419 - val_mae: 0.2971\n",
      "Epoch 672/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1671 - mae: 0.3214\n",
      "Epoch 672: val_loss improved from 0.14192 to 0.14187, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1671 - mae: 0.3214 - val_loss: 0.1419 - val_mae: 0.2970\n",
      "Epoch 673/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1674 - mae: 0.3216\n",
      "Epoch 673: val_loss improved from 0.14187 to 0.14179, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1674 - mae: 0.3216 - val_loss: 0.1418 - val_mae: 0.2969\n",
      "Epoch 674/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1668 - mae: 0.3213\n",
      "Epoch 674: val_loss improved from 0.14179 to 0.14171, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1668 - mae: 0.3213 - val_loss: 0.1417 - val_mae: 0.2969\n",
      "Epoch 675/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1662 - mae: 0.3208\n",
      "Epoch 675: val_loss improved from 0.14171 to 0.14164, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1662 - mae: 0.3208 - val_loss: 0.1416 - val_mae: 0.2968\n",
      "Epoch 676/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1660 - mae: 0.3205\n",
      "Epoch 676: val_loss improved from 0.14164 to 0.14157, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1660 - mae: 0.3205 - val_loss: 0.1416 - val_mae: 0.2967\n",
      "Epoch 677/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1664 - mae: 0.3208\n",
      "Epoch 677: val_loss improved from 0.14157 to 0.14150, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1664 - mae: 0.3208 - val_loss: 0.1415 - val_mae: 0.2967\n",
      "Epoch 678/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1663 - mae: 0.3205\n",
      "Epoch 678: val_loss improved from 0.14150 to 0.14142, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1663 - mae: 0.3205 - val_loss: 0.1414 - val_mae: 0.2966\n",
      "Epoch 679/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1661 - mae: 0.3206\n",
      "Epoch 679: val_loss improved from 0.14142 to 0.14133, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1661 - mae: 0.3206 - val_loss: 0.1413 - val_mae: 0.2965\n",
      "Epoch 680/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1664 - mae: 0.3210\n",
      "Epoch 680: val_loss improved from 0.14133 to 0.14125, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1664 - mae: 0.3210 - val_loss: 0.1413 - val_mae: 0.2964\n",
      "Epoch 681/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1657 - mae: 0.3203\n",
      "Epoch 681: val_loss improved from 0.14125 to 0.14119, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1657 - mae: 0.3203 - val_loss: 0.1412 - val_mae: 0.2964\n",
      "Epoch 682/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1664 - mae: 0.3208\n",
      "Epoch 682: val_loss improved from 0.14119 to 0.14115, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1664 - mae: 0.3208 - val_loss: 0.1411 - val_mae: 0.2963\n",
      "Epoch 683/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1657 - mae: 0.3202\n",
      "Epoch 683: val_loss improved from 0.14115 to 0.14111, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1657 - mae: 0.3202 - val_loss: 0.1411 - val_mae: 0.2962\n",
      "Epoch 684/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1659 - mae: 0.3205\n",
      "Epoch 684: val_loss improved from 0.14111 to 0.14106, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1659 - mae: 0.3205 - val_loss: 0.1411 - val_mae: 0.2962\n",
      "Epoch 685/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1661 - mae: 0.3204\n",
      "Epoch 685: val_loss improved from 0.14106 to 0.14099, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1661 - mae: 0.3204 - val_loss: 0.1410 - val_mae: 0.2961\n",
      "Epoch 686/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1666 - mae: 0.3210\n",
      "Epoch 686: val_loss improved from 0.14099 to 0.14093, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1666 - mae: 0.3210 - val_loss: 0.1409 - val_mae: 0.2960\n",
      "Epoch 687/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1656 - mae: 0.3201\n",
      "Epoch 687: val_loss improved from 0.14093 to 0.14085, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1656 - mae: 0.3201 - val_loss: 0.1408 - val_mae: 0.2960\n",
      "Epoch 688/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1655 - mae: 0.3200\n",
      "Epoch 688: val_loss improved from 0.14085 to 0.14078, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1655 - mae: 0.3200 - val_loss: 0.1408 - val_mae: 0.2959\n",
      "Epoch 689/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1655 - mae: 0.3203\n",
      "Epoch 689: val_loss improved from 0.14078 to 0.14071, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1655 - mae: 0.3203 - val_loss: 0.1407 - val_mae: 0.2958\n",
      "Epoch 690/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1662 - mae: 0.3208\n",
      "Epoch 690: val_loss improved from 0.14071 to 0.14064, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1662 - mae: 0.3208 - val_loss: 0.1406 - val_mae: 0.2957\n",
      "Epoch 691/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1656 - mae: 0.3202\n",
      "Epoch 691: val_loss improved from 0.14064 to 0.14059, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1656 - mae: 0.3202 - val_loss: 0.1406 - val_mae: 0.2957\n",
      "Epoch 692/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1659 - mae: 0.3203\n",
      "Epoch 692: val_loss improved from 0.14059 to 0.14054, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1659 - mae: 0.3203 - val_loss: 0.1405 - val_mae: 0.2956\n",
      "Epoch 693/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1650 - mae: 0.3199\n",
      "Epoch 693: val_loss improved from 0.14054 to 0.14050, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1650 - mae: 0.3199 - val_loss: 0.1405 - val_mae: 0.2956\n",
      "Epoch 694/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1654 - mae: 0.3197\n",
      "Epoch 694: val_loss improved from 0.14050 to 0.14047, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1654 - mae: 0.3197 - val_loss: 0.1405 - val_mae: 0.2955\n",
      "Epoch 695/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1652 - mae: 0.3200\n",
      "Epoch 695: val_loss improved from 0.14047 to 0.14044, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1652 - mae: 0.3200 - val_loss: 0.1404 - val_mae: 0.2955\n",
      "Epoch 696/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1654 - mae: 0.3198\n",
      "Epoch 696: val_loss improved from 0.14044 to 0.14039, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1654 - mae: 0.3198 - val_loss: 0.1404 - val_mae: 0.2954\n",
      "Epoch 697/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1648 - mae: 0.3194\n",
      "Epoch 697: val_loss improved from 0.14039 to 0.14034, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1648 - mae: 0.3194 - val_loss: 0.1403 - val_mae: 0.2954\n",
      "Epoch 698/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1646 - mae: 0.3192\n",
      "Epoch 698: val_loss improved from 0.14034 to 0.14024, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1646 - mae: 0.3192 - val_loss: 0.1402 - val_mae: 0.2953\n",
      "Epoch 699/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1646 - mae: 0.3190\n",
      "Epoch 699: val_loss improved from 0.14024 to 0.14017, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1646 - mae: 0.3190 - val_loss: 0.1402 - val_mae: 0.2952\n",
      "Epoch 700/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1640 - mae: 0.3189\n",
      "Epoch 700: val_loss improved from 0.14017 to 0.14010, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1640 - mae: 0.3189 - val_loss: 0.1401 - val_mae: 0.2951\n",
      "Epoch 701/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1653 - mae: 0.3200\n",
      "Epoch 701: val_loss improved from 0.14010 to 0.14002, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1653 - mae: 0.3200 - val_loss: 0.1400 - val_mae: 0.2951\n",
      "Epoch 702/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1648 - mae: 0.3195\n",
      "Epoch 702: val_loss improved from 0.14002 to 0.13993, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1648 - mae: 0.3195 - val_loss: 0.1399 - val_mae: 0.2950\n",
      "Epoch 703/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1644 - mae: 0.3189\n",
      "Epoch 703: val_loss improved from 0.13993 to 0.13985, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1644 - mae: 0.3189 - val_loss: 0.1398 - val_mae: 0.2949\n",
      "Epoch 704/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1645 - mae: 0.3190\n",
      "Epoch 704: val_loss improved from 0.13985 to 0.13979, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1645 - mae: 0.3190 - val_loss: 0.1398 - val_mae: 0.2948\n",
      "Epoch 705/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1643 - mae: 0.3191\n",
      "Epoch 705: val_loss improved from 0.13979 to 0.13974, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1643 - mae: 0.3191 - val_loss: 0.1397 - val_mae: 0.2947\n",
      "Epoch 706/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1647 - mae: 0.3192\n",
      "Epoch 706: val_loss improved from 0.13974 to 0.13969, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1647 - mae: 0.3192 - val_loss: 0.1397 - val_mae: 0.2947\n",
      "Epoch 707/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1644 - mae: 0.3192\n",
      "Epoch 707: val_loss improved from 0.13969 to 0.13963, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1644 - mae: 0.3192 - val_loss: 0.1396 - val_mae: 0.2946\n",
      "Epoch 708/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1647 - mae: 0.3195\n",
      "Epoch 708: val_loss improved from 0.13963 to 0.13955, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1647 - mae: 0.3195 - val_loss: 0.1396 - val_mae: 0.2945\n",
      "Epoch 709/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1638 - mae: 0.3185\n",
      "Epoch 709: val_loss improved from 0.13955 to 0.13949, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1638 - mae: 0.3185 - val_loss: 0.1395 - val_mae: 0.2945\n",
      "Epoch 710/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1641 - mae: 0.3189\n",
      "Epoch 710: val_loss improved from 0.13949 to 0.13944, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1641 - mae: 0.3189 - val_loss: 0.1394 - val_mae: 0.2944\n",
      "Epoch 711/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1640 - mae: 0.3189\n",
      "Epoch 711: val_loss improved from 0.13944 to 0.13939, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1640 - mae: 0.3189 - val_loss: 0.1394 - val_mae: 0.2944\n",
      "Epoch 712/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1640 - mae: 0.3187\n",
      "Epoch 712: val_loss improved from 0.13939 to 0.13934, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1640 - mae: 0.3187 - val_loss: 0.1393 - val_mae: 0.2943\n",
      "Epoch 713/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1642 - mae: 0.3191\n",
      "Epoch 713: val_loss improved from 0.13934 to 0.13930, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1642 - mae: 0.3191 - val_loss: 0.1393 - val_mae: 0.2943\n",
      "Epoch 714/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1635 - mae: 0.3180\n",
      "Epoch 714: val_loss improved from 0.13930 to 0.13924, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1635 - mae: 0.3180 - val_loss: 0.1392 - val_mae: 0.2942\n",
      "Epoch 715/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1638 - mae: 0.3184\n",
      "Epoch 715: val_loss improved from 0.13924 to 0.13918, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1638 - mae: 0.3184 - val_loss: 0.1392 - val_mae: 0.2941\n",
      "Epoch 716/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1638 - mae: 0.3188\n",
      "Epoch 716: val_loss improved from 0.13918 to 0.13909, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1638 - mae: 0.3188 - val_loss: 0.1391 - val_mae: 0.2941\n",
      "Epoch 717/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1631 - mae: 0.3177\n",
      "Epoch 717: val_loss improved from 0.13909 to 0.13902, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1631 - mae: 0.3177 - val_loss: 0.1390 - val_mae: 0.2940\n",
      "Epoch 718/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1629 - mae: 0.3180\n",
      "Epoch 718: val_loss improved from 0.13902 to 0.13897, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1629 - mae: 0.3180 - val_loss: 0.1390 - val_mae: 0.2939\n",
      "Epoch 719/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1635 - mae: 0.3183\n",
      "Epoch 719: val_loss improved from 0.13897 to 0.13894, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1635 - mae: 0.3183 - val_loss: 0.1389 - val_mae: 0.2939\n",
      "Epoch 720/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1631 - mae: 0.3179\n",
      "Epoch 720: val_loss improved from 0.13894 to 0.13887, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1631 - mae: 0.3179 - val_loss: 0.1389 - val_mae: 0.2938\n",
      "Epoch 721/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1635 - mae: 0.3181\n",
      "Epoch 721: val_loss improved from 0.13887 to 0.13882, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1635 - mae: 0.3181 - val_loss: 0.1388 - val_mae: 0.2937\n",
      "Epoch 722/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1627 - mae: 0.3174\n",
      "Epoch 722: val_loss improved from 0.13882 to 0.13876, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1627 - mae: 0.3174 - val_loss: 0.1388 - val_mae: 0.2937\n",
      "Epoch 723/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1631 - mae: 0.3176\n",
      "Epoch 723: val_loss improved from 0.13876 to 0.13869, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1631 - mae: 0.3176 - val_loss: 0.1387 - val_mae: 0.2936\n",
      "Epoch 724/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1622 - mae: 0.3173\n",
      "Epoch 724: val_loss improved from 0.13869 to 0.13859, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1622 - mae: 0.3173 - val_loss: 0.1386 - val_mae: 0.2935\n",
      "Epoch 725/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1630 - mae: 0.3179\n",
      "Epoch 725: val_loss improved from 0.13859 to 0.13849, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1630 - mae: 0.3179 - val_loss: 0.1385 - val_mae: 0.2934\n",
      "Epoch 726/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1633 - mae: 0.3178\n",
      "Epoch 726: val_loss improved from 0.13849 to 0.13841, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1633 - mae: 0.3178 - val_loss: 0.1384 - val_mae: 0.2933\n",
      "Epoch 727/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1625 - mae: 0.3173\n",
      "Epoch 727: val_loss improved from 0.13841 to 0.13835, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1625 - mae: 0.3173 - val_loss: 0.1383 - val_mae: 0.2933\n",
      "Epoch 728/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1628 - mae: 0.3176\n",
      "Epoch 728: val_loss improved from 0.13835 to 0.13830, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1628 - mae: 0.3176 - val_loss: 0.1383 - val_mae: 0.2932\n",
      "Epoch 729/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1633 - mae: 0.3181\n",
      "Epoch 729: val_loss improved from 0.13830 to 0.13827, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1633 - mae: 0.3181 - val_loss: 0.1383 - val_mae: 0.2932\n",
      "Epoch 730/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1622 - mae: 0.3169\n",
      "Epoch 730: val_loss improved from 0.13827 to 0.13822, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1622 - mae: 0.3169 - val_loss: 0.1382 - val_mae: 0.2931\n",
      "Epoch 731/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1626 - mae: 0.3173\n",
      "Epoch 731: val_loss improved from 0.13822 to 0.13817, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1626 - mae: 0.3173 - val_loss: 0.1382 - val_mae: 0.2930\n",
      "Epoch 732/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1625 - mae: 0.3172\n",
      "Epoch 732: val_loss improved from 0.13817 to 0.13809, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1625 - mae: 0.3172 - val_loss: 0.1381 - val_mae: 0.2930\n",
      "Epoch 733/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1622 - mae: 0.3170\n",
      "Epoch 733: val_loss improved from 0.13809 to 0.13802, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1622 - mae: 0.3170 - val_loss: 0.1380 - val_mae: 0.2929\n",
      "Epoch 734/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1622 - mae: 0.3168\n",
      "Epoch 734: val_loss improved from 0.13802 to 0.13795, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1622 - mae: 0.3168 - val_loss: 0.1380 - val_mae: 0.2928\n",
      "Epoch 735/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1623 - mae: 0.3172\n",
      "Epoch 735: val_loss improved from 0.13795 to 0.13789, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1623 - mae: 0.3172 - val_loss: 0.1379 - val_mae: 0.2927\n",
      "Epoch 736/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1629 - mae: 0.3176\n",
      "Epoch 736: val_loss improved from 0.13789 to 0.13784, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1629 - mae: 0.3176 - val_loss: 0.1378 - val_mae: 0.2927\n",
      "Epoch 737/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1621 - mae: 0.3168\n",
      "Epoch 737: val_loss improved from 0.13784 to 0.13777, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1621 - mae: 0.3168 - val_loss: 0.1378 - val_mae: 0.2926\n",
      "Epoch 738/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1621 - mae: 0.3168\n",
      "Epoch 738: val_loss improved from 0.13777 to 0.13768, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1621 - mae: 0.3168 - val_loss: 0.1377 - val_mae: 0.2925\n",
      "Epoch 739/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1618 - mae: 0.3167\n",
      "Epoch 739: val_loss improved from 0.13768 to 0.13761, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1618 - mae: 0.3167 - val_loss: 0.1376 - val_mae: 0.2924\n",
      "Epoch 740/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1620 - mae: 0.3168\n",
      "Epoch 740: val_loss improved from 0.13761 to 0.13755, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1620 - mae: 0.3168 - val_loss: 0.1376 - val_mae: 0.2924\n",
      "Epoch 741/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1618 - mae: 0.3163\n",
      "Epoch 741: val_loss improved from 0.13755 to 0.13750, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1618 - mae: 0.3163 - val_loss: 0.1375 - val_mae: 0.2923\n",
      "Epoch 742/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1613 - mae: 0.3163\n",
      "Epoch 742: val_loss improved from 0.13750 to 0.13746, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1613 - mae: 0.3163 - val_loss: 0.1375 - val_mae: 0.2923\n",
      "Epoch 743/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1616 - mae: 0.3164\n",
      "Epoch 743: val_loss improved from 0.13746 to 0.13743, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1616 - mae: 0.3164 - val_loss: 0.1374 - val_mae: 0.2922\n",
      "Epoch 744/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1615 - mae: 0.3166\n",
      "Epoch 744: val_loss improved from 0.13743 to 0.13739, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1615 - mae: 0.3166 - val_loss: 0.1374 - val_mae: 0.2922\n",
      "Epoch 745/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1610 - mae: 0.3158\n",
      "Epoch 745: val_loss improved from 0.13739 to 0.13738, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1610 - mae: 0.3158 - val_loss: 0.1374 - val_mae: 0.2921\n",
      "Epoch 746/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1606 - mae: 0.3153\n",
      "Epoch 746: val_loss improved from 0.13738 to 0.13736, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1606 - mae: 0.3153 - val_loss: 0.1374 - val_mae: 0.2921\n",
      "Epoch 747/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1615 - mae: 0.3161\n",
      "Epoch 747: val_loss improved from 0.13736 to 0.13730, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1615 - mae: 0.3161 - val_loss: 0.1373 - val_mae: 0.2920\n",
      "Epoch 748/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1616 - mae: 0.3163\n",
      "Epoch 748: val_loss improved from 0.13730 to 0.13722, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1616 - mae: 0.3163 - val_loss: 0.1372 - val_mae: 0.2919\n",
      "Epoch 749/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1613 - mae: 0.3161\n",
      "Epoch 749: val_loss improved from 0.13722 to 0.13711, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1613 - mae: 0.3161 - val_loss: 0.1371 - val_mae: 0.2918\n",
      "Epoch 750/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1610 - mae: 0.3159\n",
      "Epoch 750: val_loss improved from 0.13711 to 0.13698, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1610 - mae: 0.3159 - val_loss: 0.1370 - val_mae: 0.2917\n",
      "Epoch 751/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1615 - mae: 0.3161\n",
      "Epoch 751: val_loss improved from 0.13698 to 0.13688, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1615 - mae: 0.3161 - val_loss: 0.1369 - val_mae: 0.2916\n",
      "Epoch 752/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1610 - mae: 0.3156\n",
      "Epoch 752: val_loss improved from 0.13688 to 0.13679, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1610 - mae: 0.3156 - val_loss: 0.1368 - val_mae: 0.2915\n",
      "Epoch 753/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1609 - mae: 0.3156\n",
      "Epoch 753: val_loss improved from 0.13679 to 0.13674, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1609 - mae: 0.3156 - val_loss: 0.1367 - val_mae: 0.2915\n",
      "Epoch 754/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1611 - mae: 0.3161\n",
      "Epoch 754: val_loss improved from 0.13674 to 0.13670, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1611 - mae: 0.3161 - val_loss: 0.1367 - val_mae: 0.2914\n",
      "Epoch 755/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1612 - mae: 0.3163\n",
      "Epoch 755: val_loss improved from 0.13670 to 0.13666, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1612 - mae: 0.3163 - val_loss: 0.1367 - val_mae: 0.2914\n",
      "Epoch 756/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1608 - mae: 0.3159\n",
      "Epoch 756: val_loss improved from 0.13666 to 0.13664, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1608 - mae: 0.3159 - val_loss: 0.1366 - val_mae: 0.2913\n",
      "Epoch 757/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1608 - mae: 0.3155\n",
      "Epoch 757: val_loss improved from 0.13664 to 0.13660, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1608 - mae: 0.3155 - val_loss: 0.1366 - val_mae: 0.2913\n",
      "Epoch 758/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1600 - mae: 0.3151\n",
      "Epoch 758: val_loss improved from 0.13660 to 0.13655, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1600 - mae: 0.3151 - val_loss: 0.1365 - val_mae: 0.2912\n",
      "Epoch 759/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1605 - mae: 0.3154\n",
      "Epoch 759: val_loss improved from 0.13655 to 0.13648, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1605 - mae: 0.3154 - val_loss: 0.1365 - val_mae: 0.2911\n",
      "Epoch 760/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1598 - mae: 0.3148\n",
      "Epoch 760: val_loss improved from 0.13648 to 0.13640, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1598 - mae: 0.3148 - val_loss: 0.1364 - val_mae: 0.2911\n",
      "Epoch 761/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1605 - mae: 0.3154\n",
      "Epoch 761: val_loss improved from 0.13640 to 0.13630, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1605 - mae: 0.3154 - val_loss: 0.1363 - val_mae: 0.2910\n",
      "Epoch 762/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1606 - mae: 0.3155\n",
      "Epoch 762: val_loss improved from 0.13630 to 0.13620, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1606 - mae: 0.3155 - val_loss: 0.1362 - val_mae: 0.2909\n",
      "Epoch 763/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1601 - mae: 0.3150\n",
      "Epoch 763: val_loss improved from 0.13620 to 0.13613, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1601 - mae: 0.3150 - val_loss: 0.1361 - val_mae: 0.2908\n",
      "Epoch 764/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1604 - mae: 0.3154\n",
      "Epoch 764: val_loss improved from 0.13613 to 0.13609, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1604 - mae: 0.3154 - val_loss: 0.1361 - val_mae: 0.2908\n",
      "Epoch 765/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1596 - mae: 0.3148\n",
      "Epoch 765: val_loss improved from 0.13609 to 0.13608, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1596 - mae: 0.3148 - val_loss: 0.1361 - val_mae: 0.2907\n",
      "Epoch 766/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1599 - mae: 0.3148\n",
      "Epoch 766: val_loss did not improve from 0.13608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1599 - mae: 0.3148 - val_loss: 0.1361 - val_mae: 0.2907\n",
      "Epoch 767/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1602 - mae: 0.3150\n",
      "Epoch 767: val_loss improved from 0.13608 to 0.13608, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1602 - mae: 0.3150 - val_loss: 0.1361 - val_mae: 0.2907\n",
      "Epoch 768/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1597 - mae: 0.3143\n",
      "Epoch 768: val_loss improved from 0.13608 to 0.13604, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1597 - mae: 0.3143 - val_loss: 0.1360 - val_mae: 0.2906\n",
      "Epoch 769/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1594 - mae: 0.3146\n",
      "Epoch 769: val_loss improved from 0.13604 to 0.13596, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1594 - mae: 0.3146 - val_loss: 0.1360 - val_mae: 0.2906\n",
      "Epoch 770/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1592 - mae: 0.3141\n",
      "Epoch 770: val_loss improved from 0.13596 to 0.13588, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1592 - mae: 0.3141 - val_loss: 0.1359 - val_mae: 0.2905\n",
      "Epoch 771/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1605 - mae: 0.3151\n",
      "Epoch 771: val_loss improved from 0.13588 to 0.13575, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1605 - mae: 0.3151 - val_loss: 0.1358 - val_mae: 0.2904\n",
      "Epoch 772/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1588 - mae: 0.3137\n",
      "Epoch 772: val_loss improved from 0.13575 to 0.13565, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1588 - mae: 0.3137 - val_loss: 0.1356 - val_mae: 0.2903\n",
      "Epoch 773/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1593 - mae: 0.3143\n",
      "Epoch 773: val_loss improved from 0.13565 to 0.13558, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1593 - mae: 0.3143 - val_loss: 0.1356 - val_mae: 0.2902\n",
      "Epoch 774/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1590 - mae: 0.3143\n",
      "Epoch 774: val_loss improved from 0.13558 to 0.13553, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1590 - mae: 0.3143 - val_loss: 0.1355 - val_mae: 0.2901\n",
      "Epoch 775/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1588 - mae: 0.3138\n",
      "Epoch 775: val_loss improved from 0.13553 to 0.13548, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1588 - mae: 0.3138 - val_loss: 0.1355 - val_mae: 0.2901\n",
      "Epoch 776/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1596 - mae: 0.3144\n",
      "Epoch 776: val_loss improved from 0.13548 to 0.13543, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1596 - mae: 0.3144 - val_loss: 0.1354 - val_mae: 0.2900\n",
      "Epoch 777/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1592 - mae: 0.3140\n",
      "Epoch 777: val_loss improved from 0.13543 to 0.13537, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1592 - mae: 0.3140 - val_loss: 0.1354 - val_mae: 0.2899\n",
      "Epoch 778/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1586 - mae: 0.3137\n",
      "Epoch 778: val_loss improved from 0.13537 to 0.13531, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1586 - mae: 0.3137 - val_loss: 0.1353 - val_mae: 0.2898\n",
      "Epoch 779/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1585 - mae: 0.3135\n",
      "Epoch 779: val_loss improved from 0.13531 to 0.13522, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1585 - mae: 0.3135 - val_loss: 0.1352 - val_mae: 0.2898\n",
      "Epoch 780/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1578 - mae: 0.3127\n",
      "Epoch 780: val_loss improved from 0.13522 to 0.13517, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.1578 - mae: 0.3127 - val_loss: 0.1352 - val_mae: 0.2897\n",
      "Epoch 781/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1586 - mae: 0.3133\n",
      "Epoch 781: val_loss improved from 0.13517 to 0.13512, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1586 - mae: 0.3133 - val_loss: 0.1351 - val_mae: 0.2896\n",
      "Epoch 782/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1588 - mae: 0.3138\n",
      "Epoch 782: val_loss improved from 0.13512 to 0.13508, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1588 - mae: 0.3138 - val_loss: 0.1351 - val_mae: 0.2896\n",
      "Epoch 783/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1588 - mae: 0.3138\n",
      "Epoch 783: val_loss improved from 0.13508 to 0.13506, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1588 - mae: 0.3138 - val_loss: 0.1351 - val_mae: 0.2896\n",
      "Epoch 784/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1585 - mae: 0.3136\n",
      "Epoch 784: val_loss improved from 0.13506 to 0.13500, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1585 - mae: 0.3136 - val_loss: 0.1350 - val_mae: 0.2895\n",
      "Epoch 785/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1584 - mae: 0.3135\n",
      "Epoch 785: val_loss improved from 0.13500 to 0.13492, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1584 - mae: 0.3135 - val_loss: 0.1349 - val_mae: 0.2894\n",
      "Epoch 786/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1583 - mae: 0.3133\n",
      "Epoch 786: val_loss improved from 0.13492 to 0.13484, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1583 - mae: 0.3133 - val_loss: 0.1348 - val_mae: 0.2893\n",
      "Epoch 787/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1583 - mae: 0.3134\n",
      "Epoch 787: val_loss improved from 0.13484 to 0.13477, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1583 - mae: 0.3134 - val_loss: 0.1348 - val_mae: 0.2893\n",
      "Epoch 788/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1581 - mae: 0.3130\n",
      "Epoch 788: val_loss improved from 0.13477 to 0.13470, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1581 - mae: 0.3130 - val_loss: 0.1347 - val_mae: 0.2892\n",
      "Epoch 789/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1581 - mae: 0.3130\n",
      "Epoch 789: val_loss improved from 0.13470 to 0.13464, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1581 - mae: 0.3130 - val_loss: 0.1346 - val_mae: 0.2891\n",
      "Epoch 790/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1583 - mae: 0.3132\n",
      "Epoch 790: val_loss improved from 0.13464 to 0.13458, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1583 - mae: 0.3132 - val_loss: 0.1346 - val_mae: 0.2890\n",
      "Epoch 791/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1586 - mae: 0.3137\n",
      "Epoch 791: val_loss improved from 0.13458 to 0.13453, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1586 - mae: 0.3137 - val_loss: 0.1345 - val_mae: 0.2890\n",
      "Epoch 792/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1577 - mae: 0.3127\n",
      "Epoch 792: val_loss improved from 0.13453 to 0.13450, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1577 - mae: 0.3127 - val_loss: 0.1345 - val_mae: 0.2889\n",
      "Epoch 793/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1580 - mae: 0.3128\n",
      "Epoch 793: val_loss improved from 0.13450 to 0.13443, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1580 - mae: 0.3128 - val_loss: 0.1344 - val_mae: 0.2888\n",
      "Epoch 794/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1570 - mae: 0.3120\n",
      "Epoch 794: val_loss improved from 0.13443 to 0.13437, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1570 - mae: 0.3120 - val_loss: 0.1344 - val_mae: 0.2888\n",
      "Epoch 795/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1572 - mae: 0.3122\n",
      "Epoch 795: val_loss improved from 0.13437 to 0.13430, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1572 - mae: 0.3122 - val_loss: 0.1343 - val_mae: 0.2887\n",
      "Epoch 796/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1573 - mae: 0.3123\n",
      "Epoch 796: val_loss improved from 0.13430 to 0.13425, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1573 - mae: 0.3123 - val_loss: 0.1342 - val_mae: 0.2886\n",
      "Epoch 797/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1582 - mae: 0.3133\n",
      "Epoch 797: val_loss improved from 0.13425 to 0.13418, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1582 - mae: 0.3133 - val_loss: 0.1342 - val_mae: 0.2885\n",
      "Epoch 798/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1570 - mae: 0.3121\n",
      "Epoch 798: val_loss improved from 0.13418 to 0.13410, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1570 - mae: 0.3121 - val_loss: 0.1341 - val_mae: 0.2885\n",
      "Epoch 799/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1578 - mae: 0.3128\n",
      "Epoch 799: val_loss improved from 0.13410 to 0.13402, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1578 - mae: 0.3128 - val_loss: 0.1340 - val_mae: 0.2884\n",
      "Epoch 800/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1574 - mae: 0.3125\n",
      "Epoch 800: val_loss improved from 0.13402 to 0.13394, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1574 - mae: 0.3125 - val_loss: 0.1339 - val_mae: 0.2883\n",
      "Epoch 801/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1568 - mae: 0.3121\n",
      "Epoch 801: val_loss improved from 0.13394 to 0.13389, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1568 - mae: 0.3121 - val_loss: 0.1339 - val_mae: 0.2883\n",
      "Epoch 802/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1575 - mae: 0.3123\n",
      "Epoch 802: val_loss improved from 0.13389 to 0.13386, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1575 - mae: 0.3123 - val_loss: 0.1339 - val_mae: 0.2882\n",
      "Epoch 803/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1572 - mae: 0.3119\n",
      "Epoch 803: val_loss improved from 0.13386 to 0.13382, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1572 - mae: 0.3119 - val_loss: 0.1338 - val_mae: 0.2882\n",
      "Epoch 804/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1568 - mae: 0.3118\n",
      "Epoch 804: val_loss improved from 0.13382 to 0.13380, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1568 - mae: 0.3118 - val_loss: 0.1338 - val_mae: 0.2881\n",
      "Epoch 805/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1568 - mae: 0.3116\n",
      "Epoch 805: val_loss improved from 0.13380 to 0.13376, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1568 - mae: 0.3116 - val_loss: 0.1338 - val_mae: 0.2881\n",
      "Epoch 806/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1572 - mae: 0.3122\n",
      "Epoch 806: val_loss improved from 0.13376 to 0.13369, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1572 - mae: 0.3122 - val_loss: 0.1337 - val_mae: 0.2880\n",
      "Epoch 807/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1565 - mae: 0.3118\n",
      "Epoch 807: val_loss improved from 0.13369 to 0.13361, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1565 - mae: 0.3118 - val_loss: 0.1336 - val_mae: 0.2879\n",
      "Epoch 808/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1567 - mae: 0.3115\n",
      "Epoch 808: val_loss improved from 0.13361 to 0.13353, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1567 - mae: 0.3115 - val_loss: 0.1335 - val_mae: 0.2879\n",
      "Epoch 809/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1566 - mae: 0.3115\n",
      "Epoch 809: val_loss improved from 0.13353 to 0.13347, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1566 - mae: 0.3115 - val_loss: 0.1335 - val_mae: 0.2878\n",
      "Epoch 810/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1567 - mae: 0.3116\n",
      "Epoch 810: val_loss improved from 0.13347 to 0.13340, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1567 - mae: 0.3116 - val_loss: 0.1334 - val_mae: 0.2877\n",
      "Epoch 811/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1568 - mae: 0.3119\n",
      "Epoch 811: val_loss improved from 0.13340 to 0.13333, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1568 - mae: 0.3119 - val_loss: 0.1333 - val_mae: 0.2876\n",
      "Epoch 812/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1566 - mae: 0.3115\n",
      "Epoch 812: val_loss improved from 0.13333 to 0.13327, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1566 - mae: 0.3115 - val_loss: 0.1333 - val_mae: 0.2876\n",
      "Epoch 813/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1565 - mae: 0.3116\n",
      "Epoch 813: val_loss improved from 0.13327 to 0.13321, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1565 - mae: 0.3116 - val_loss: 0.1332 - val_mae: 0.2875\n",
      "Epoch 814/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1557 - mae: 0.3107\n",
      "Epoch 814: val_loss improved from 0.13321 to 0.13315, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1557 - mae: 0.3107 - val_loss: 0.1331 - val_mae: 0.2874\n",
      "Epoch 815/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1560 - mae: 0.3111\n",
      "Epoch 815: val_loss improved from 0.13315 to 0.13308, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1560 - mae: 0.3111 - val_loss: 0.1331 - val_mae: 0.2873\n",
      "Epoch 816/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1566 - mae: 0.3117\n",
      "Epoch 816: val_loss improved from 0.13308 to 0.13300, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1566 - mae: 0.3117 - val_loss: 0.1330 - val_mae: 0.2872\n",
      "Epoch 817/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1560 - mae: 0.3112\n",
      "Epoch 817: val_loss improved from 0.13300 to 0.13292, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1560 - mae: 0.3112 - val_loss: 0.1329 - val_mae: 0.2872\n",
      "Epoch 818/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1561 - mae: 0.3112\n",
      "Epoch 818: val_loss improved from 0.13292 to 0.13288, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1561 - mae: 0.3112 - val_loss: 0.1329 - val_mae: 0.2871\n",
      "Epoch 819/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1562 - mae: 0.3112\n",
      "Epoch 819: val_loss improved from 0.13288 to 0.13282, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1562 - mae: 0.3112 - val_loss: 0.1328 - val_mae: 0.2870\n",
      "Epoch 820/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1558 - mae: 0.3110\n",
      "Epoch 820: val_loss improved from 0.13282 to 0.13275, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1558 - mae: 0.3110 - val_loss: 0.1327 - val_mae: 0.2870\n",
      "Epoch 821/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1554 - mae: 0.3103\n",
      "Epoch 821: val_loss improved from 0.13275 to 0.13268, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1554 - mae: 0.3103 - val_loss: 0.1327 - val_mae: 0.2869\n",
      "Epoch 822/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1558 - mae: 0.3108\n",
      "Epoch 822: val_loss improved from 0.13268 to 0.13261, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1558 - mae: 0.3108 - val_loss: 0.1326 - val_mae: 0.2868\n",
      "Epoch 823/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1554 - mae: 0.3105\n",
      "Epoch 823: val_loss improved from 0.13261 to 0.13255, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1554 - mae: 0.3105 - val_loss: 0.1325 - val_mae: 0.2867\n",
      "Epoch 824/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1556 - mae: 0.3106\n",
      "Epoch 824: val_loss improved from 0.13255 to 0.13249, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1556 - mae: 0.3106 - val_loss: 0.1325 - val_mae: 0.2867\n",
      "Epoch 825/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1555 - mae: 0.3107\n",
      "Epoch 825: val_loss improved from 0.13249 to 0.13246, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1555 - mae: 0.3107 - val_loss: 0.1325 - val_mae: 0.2866\n",
      "Epoch 826/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1554 - mae: 0.3106\n",
      "Epoch 826: val_loss improved from 0.13246 to 0.13243, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1554 - mae: 0.3106 - val_loss: 0.1324 - val_mae: 0.2866\n",
      "Epoch 827/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1555 - mae: 0.3105\n",
      "Epoch 827: val_loss improved from 0.13243 to 0.13239, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1555 - mae: 0.3105 - val_loss: 0.1324 - val_mae: 0.2865\n",
      "Epoch 828/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1554 - mae: 0.3104\n",
      "Epoch 828: val_loss improved from 0.13239 to 0.13232, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1554 - mae: 0.3104 - val_loss: 0.1323 - val_mae: 0.2865\n",
      "Epoch 829/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1545 - mae: 0.3094\n",
      "Epoch 829: val_loss improved from 0.13232 to 0.13223, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1545 - mae: 0.3094 - val_loss: 0.1322 - val_mae: 0.2864\n",
      "Epoch 830/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1546 - mae: 0.3097\n",
      "Epoch 830: val_loss improved from 0.13223 to 0.13214, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1546 - mae: 0.3097 - val_loss: 0.1321 - val_mae: 0.2863\n",
      "Epoch 831/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1551 - mae: 0.3099\n",
      "Epoch 831: val_loss improved from 0.13214 to 0.13205, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1551 - mae: 0.3099 - val_loss: 0.1321 - val_mae: 0.2862\n",
      "Epoch 832/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1548 - mae: 0.3099\n",
      "Epoch 832: val_loss improved from 0.13205 to 0.13199, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1548 - mae: 0.3099 - val_loss: 0.1320 - val_mae: 0.2861\n",
      "Epoch 833/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1549 - mae: 0.3100\n",
      "Epoch 833: val_loss improved from 0.13199 to 0.13194, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1549 - mae: 0.3100 - val_loss: 0.1319 - val_mae: 0.2861\n",
      "Epoch 834/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1549 - mae: 0.3098\n",
      "Epoch 834: val_loss improved from 0.13194 to 0.13187, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1549 - mae: 0.3098 - val_loss: 0.1319 - val_mae: 0.2860\n",
      "Epoch 835/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1547 - mae: 0.3101\n",
      "Epoch 835: val_loss improved from 0.13187 to 0.13183, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1547 - mae: 0.3101 - val_loss: 0.1318 - val_mae: 0.2859\n",
      "Epoch 836/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1544 - mae: 0.3095\n",
      "Epoch 836: val_loss improved from 0.13183 to 0.13180, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1544 - mae: 0.3095 - val_loss: 0.1318 - val_mae: 0.2859\n",
      "Epoch 837/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1550 - mae: 0.3101\n",
      "Epoch 837: val_loss improved from 0.13180 to 0.13173, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1550 - mae: 0.3101 - val_loss: 0.1317 - val_mae: 0.2858\n",
      "Epoch 838/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1541 - mae: 0.3093\n",
      "Epoch 838: val_loss improved from 0.13173 to 0.13167, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1541 - mae: 0.3093 - val_loss: 0.1317 - val_mae: 0.2858\n",
      "Epoch 839/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1540 - mae: 0.3092\n",
      "Epoch 839: val_loss improved from 0.13167 to 0.13161, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1540 - mae: 0.3092 - val_loss: 0.1316 - val_mae: 0.2857\n",
      "Epoch 840/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1541 - mae: 0.3091\n",
      "Epoch 840: val_loss improved from 0.13161 to 0.13154, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1541 - mae: 0.3091 - val_loss: 0.1315 - val_mae: 0.2856\n",
      "Epoch 841/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1543 - mae: 0.3094\n",
      "Epoch 841: val_loss improved from 0.13154 to 0.13147, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1543 - mae: 0.3094 - val_loss: 0.1315 - val_mae: 0.2856\n",
      "Epoch 842/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1544 - mae: 0.3096\n",
      "Epoch 842: val_loss improved from 0.13147 to 0.13140, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1544 - mae: 0.3096 - val_loss: 0.1314 - val_mae: 0.2855\n",
      "Epoch 843/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1545 - mae: 0.3094\n",
      "Epoch 843: val_loss improved from 0.13140 to 0.13135, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1545 - mae: 0.3094 - val_loss: 0.1314 - val_mae: 0.2854\n",
      "Epoch 844/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1540 - mae: 0.3089\n",
      "Epoch 844: val_loss improved from 0.13135 to 0.13132, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1540 - mae: 0.3089 - val_loss: 0.1313 - val_mae: 0.2854\n",
      "Epoch 845/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1543 - mae: 0.3095\n",
      "Epoch 845: val_loss improved from 0.13132 to 0.13127, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1543 - mae: 0.3095 - val_loss: 0.1313 - val_mae: 0.2853\n",
      "Epoch 846/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1547 - mae: 0.3099\n",
      "Epoch 846: val_loss improved from 0.13127 to 0.13122, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1547 - mae: 0.3099 - val_loss: 0.1312 - val_mae: 0.2852\n",
      "Epoch 847/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1539 - mae: 0.3092\n",
      "Epoch 847: val_loss improved from 0.13122 to 0.13114, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1539 - mae: 0.3092 - val_loss: 0.1311 - val_mae: 0.2851\n",
      "Epoch 848/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1542 - mae: 0.3093\n",
      "Epoch 848: val_loss improved from 0.13114 to 0.13104, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1542 - mae: 0.3093 - val_loss: 0.1310 - val_mae: 0.2850\n",
      "Epoch 849/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1544 - mae: 0.3093\n",
      "Epoch 849: val_loss improved from 0.13104 to 0.13096, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1544 - mae: 0.3093 - val_loss: 0.1310 - val_mae: 0.2849\n",
      "Epoch 850/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1539 - mae: 0.3089\n",
      "Epoch 850: val_loss improved from 0.13096 to 0.13088, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1539 - mae: 0.3089 - val_loss: 0.1309 - val_mae: 0.2848\n",
      "Epoch 851/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1534 - mae: 0.3085\n",
      "Epoch 851: val_loss improved from 0.13088 to 0.13081, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1534 - mae: 0.3085 - val_loss: 0.1308 - val_mae: 0.2848\n",
      "Epoch 852/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1534 - mae: 0.3084\n",
      "Epoch 852: val_loss improved from 0.13081 to 0.13079, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1534 - mae: 0.3084 - val_loss: 0.1308 - val_mae: 0.2847\n",
      "Epoch 853/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1533 - mae: 0.3083\n",
      "Epoch 853: val_loss improved from 0.13079 to 0.13076, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1533 - mae: 0.3083 - val_loss: 0.1308 - val_mae: 0.2847\n",
      "Epoch 854/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1532 - mae: 0.3085\n",
      "Epoch 854: val_loss improved from 0.13076 to 0.13072, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1532 - mae: 0.3085 - val_loss: 0.1307 - val_mae: 0.2846\n",
      "Epoch 855/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1534 - mae: 0.3083\n",
      "Epoch 855: val_loss improved from 0.13072 to 0.13067, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1534 - mae: 0.3083 - val_loss: 0.1307 - val_mae: 0.2845\n",
      "Epoch 856/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1530 - mae: 0.3081\n",
      "Epoch 856: val_loss improved from 0.13067 to 0.13061, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1530 - mae: 0.3081 - val_loss: 0.1306 - val_mae: 0.2845\n",
      "Epoch 857/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1532 - mae: 0.3082\n",
      "Epoch 857: val_loss improved from 0.13061 to 0.13054, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1532 - mae: 0.3082 - val_loss: 0.1305 - val_mae: 0.2844\n",
      "Epoch 858/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1533 - mae: 0.3085\n",
      "Epoch 858: val_loss improved from 0.13054 to 0.13047, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1533 - mae: 0.3085 - val_loss: 0.1305 - val_mae: 0.2844\n",
      "Epoch 859/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1536 - mae: 0.3088\n",
      "Epoch 859: val_loss improved from 0.13047 to 0.13043, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1536 - mae: 0.3088 - val_loss: 0.1304 - val_mae: 0.2843\n",
      "Epoch 860/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1525 - mae: 0.3076\n",
      "Epoch 860: val_loss improved from 0.13043 to 0.13039, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1525 - mae: 0.3076 - val_loss: 0.1304 - val_mae: 0.2842\n",
      "Epoch 861/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1526 - mae: 0.3080\n",
      "Epoch 861: val_loss improved from 0.13039 to 0.13036, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1526 - mae: 0.3080 - val_loss: 0.1304 - val_mae: 0.2842\n",
      "Epoch 862/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1526 - mae: 0.3075\n",
      "Epoch 862: val_loss improved from 0.13036 to 0.13031, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1526 - mae: 0.3075 - val_loss: 0.1303 - val_mae: 0.2841\n",
      "Epoch 863/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1529 - mae: 0.3083\n",
      "Epoch 863: val_loss improved from 0.13031 to 0.13026, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1529 - mae: 0.3083 - val_loss: 0.1303 - val_mae: 0.2840\n",
      "Epoch 864/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1527 - mae: 0.3078\n",
      "Epoch 864: val_loss improved from 0.13026 to 0.13019, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1527 - mae: 0.3078 - val_loss: 0.1302 - val_mae: 0.2840\n",
      "Epoch 865/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1519 - mae: 0.3072\n",
      "Epoch 865: val_loss improved from 0.13019 to 0.13011, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1519 - mae: 0.3072 - val_loss: 0.1301 - val_mae: 0.2839\n",
      "Epoch 866/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1523 - mae: 0.3072\n",
      "Epoch 866: val_loss improved from 0.13011 to 0.13003, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1523 - mae: 0.3072 - val_loss: 0.1300 - val_mae: 0.2838\n",
      "Epoch 867/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1520 - mae: 0.3071\n",
      "Epoch 867: val_loss improved from 0.13003 to 0.12996, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1520 - mae: 0.3071 - val_loss: 0.1300 - val_mae: 0.2837\n",
      "Epoch 868/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1523 - mae: 0.3074\n",
      "Epoch 868: val_loss improved from 0.12996 to 0.12989, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1523 - mae: 0.3074 - val_loss: 0.1299 - val_mae: 0.2837\n",
      "Epoch 869/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1528 - mae: 0.3080\n",
      "Epoch 869: val_loss improved from 0.12989 to 0.12982, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1528 - mae: 0.3080 - val_loss: 0.1298 - val_mae: 0.2836\n",
      "Epoch 870/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1524 - mae: 0.3071\n",
      "Epoch 870: val_loss improved from 0.12982 to 0.12976, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1524 - mae: 0.3071 - val_loss: 0.1298 - val_mae: 0.2835\n",
      "Epoch 871/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1516 - mae: 0.3068\n",
      "Epoch 871: val_loss improved from 0.12976 to 0.12968, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1516 - mae: 0.3068 - val_loss: 0.1297 - val_mae: 0.2835\n",
      "Epoch 872/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1528 - mae: 0.3076\n",
      "Epoch 872: val_loss improved from 0.12968 to 0.12962, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1528 - mae: 0.3076 - val_loss: 0.1296 - val_mae: 0.2834\n",
      "Epoch 873/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1529 - mae: 0.3080\n",
      "Epoch 873: val_loss improved from 0.12962 to 0.12957, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1529 - mae: 0.3080 - val_loss: 0.1296 - val_mae: 0.2833\n",
      "Epoch 874/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1516 - mae: 0.3067\n",
      "Epoch 874: val_loss improved from 0.12957 to 0.12952, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1516 - mae: 0.3067 - val_loss: 0.1295 - val_mae: 0.2833\n",
      "Epoch 875/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1514 - mae: 0.3067\n",
      "Epoch 875: val_loss improved from 0.12952 to 0.12946, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1514 - mae: 0.3067 - val_loss: 0.1295 - val_mae: 0.2832\n",
      "Epoch 876/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1513 - mae: 0.3064\n",
      "Epoch 876: val_loss improved from 0.12946 to 0.12940, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1513 - mae: 0.3064 - val_loss: 0.1294 - val_mae: 0.2831\n",
      "Epoch 877/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1515 - mae: 0.3068\n",
      "Epoch 877: val_loss improved from 0.12940 to 0.12933, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1515 - mae: 0.3068 - val_loss: 0.1293 - val_mae: 0.2830\n",
      "Epoch 878/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1516 - mae: 0.3067\n",
      "Epoch 878: val_loss improved from 0.12933 to 0.12926, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1516 - mae: 0.3067 - val_loss: 0.1293 - val_mae: 0.2830\n",
      "Epoch 879/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1511 - mae: 0.3063\n",
      "Epoch 879: val_loss improved from 0.12926 to 0.12920, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1511 - mae: 0.3063 - val_loss: 0.1292 - val_mae: 0.2829\n",
      "Epoch 880/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1512 - mae: 0.3064\n",
      "Epoch 880: val_loss improved from 0.12920 to 0.12909, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1512 - mae: 0.3064 - val_loss: 0.1291 - val_mae: 0.2828\n",
      "Epoch 881/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1511 - mae: 0.3063\n",
      "Epoch 881: val_loss improved from 0.12909 to 0.12899, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1511 - mae: 0.3063 - val_loss: 0.1290 - val_mae: 0.2827\n",
      "Epoch 882/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1509 - mae: 0.3060\n",
      "Epoch 882: val_loss improved from 0.12899 to 0.12891, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1509 - mae: 0.3060 - val_loss: 0.1289 - val_mae: 0.2826\n",
      "Epoch 883/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1515 - mae: 0.3064\n",
      "Epoch 883: val_loss improved from 0.12891 to 0.12885, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1515 - mae: 0.3064 - val_loss: 0.1288 - val_mae: 0.2825\n",
      "Epoch 884/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1507 - mae: 0.3058\n",
      "Epoch 884: val_loss improved from 0.12885 to 0.12881, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1507 - mae: 0.3058 - val_loss: 0.1288 - val_mae: 0.2825\n",
      "Epoch 885/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1504 - mae: 0.3053\n",
      "Epoch 885: val_loss improved from 0.12881 to 0.12879, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1504 - mae: 0.3053 - val_loss: 0.1288 - val_mae: 0.2824\n",
      "Epoch 886/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1512 - mae: 0.3063\n",
      "Epoch 886: val_loss improved from 0.12879 to 0.12876, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1512 - mae: 0.3063 - val_loss: 0.1288 - val_mae: 0.2824\n",
      "Epoch 887/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1515 - mae: 0.3066\n",
      "Epoch 887: val_loss improved from 0.12876 to 0.12871, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1515 - mae: 0.3066 - val_loss: 0.1287 - val_mae: 0.2823\n",
      "Epoch 888/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1509 - mae: 0.3060\n",
      "Epoch 888: val_loss improved from 0.12871 to 0.12865, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1509 - mae: 0.3060 - val_loss: 0.1286 - val_mae: 0.2823\n",
      "Epoch 889/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1501 - mae: 0.3052\n",
      "Epoch 889: val_loss improved from 0.12865 to 0.12860, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1501 - mae: 0.3052 - val_loss: 0.1286 - val_mae: 0.2822\n",
      "Epoch 890/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1509 - mae: 0.3057\n",
      "Epoch 890: val_loss improved from 0.12860 to 0.12852, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1509 - mae: 0.3057 - val_loss: 0.1285 - val_mae: 0.2821\n",
      "Epoch 891/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1503 - mae: 0.3053\n",
      "Epoch 891: val_loss improved from 0.12852 to 0.12845, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1503 - mae: 0.3053 - val_loss: 0.1284 - val_mae: 0.2820\n",
      "Epoch 892/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1504 - mae: 0.3057\n",
      "Epoch 892: val_loss improved from 0.12845 to 0.12836, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1504 - mae: 0.3057 - val_loss: 0.1284 - val_mae: 0.2819\n",
      "Epoch 893/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1503 - mae: 0.3054\n",
      "Epoch 893: val_loss improved from 0.12836 to 0.12827, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1503 - mae: 0.3054 - val_loss: 0.1283 - val_mae: 0.2818\n",
      "Epoch 894/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1502 - mae: 0.3054\n",
      "Epoch 894: val_loss improved from 0.12827 to 0.12818, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1502 - mae: 0.3054 - val_loss: 0.1282 - val_mae: 0.2817\n",
      "Epoch 895/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1497 - mae: 0.3048\n",
      "Epoch 895: val_loss improved from 0.12818 to 0.12813, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1497 - mae: 0.3048 - val_loss: 0.1281 - val_mae: 0.2817\n",
      "Epoch 896/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1497 - mae: 0.3051\n",
      "Epoch 896: val_loss improved from 0.12813 to 0.12810, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1497 - mae: 0.3051 - val_loss: 0.1281 - val_mae: 0.2816\n",
      "Epoch 897/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1504 - mae: 0.3054\n",
      "Epoch 897: val_loss did not improve from 0.12810\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1504 - mae: 0.3054 - val_loss: 0.1281 - val_mae: 0.2816\n",
      "Epoch 898/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1504 - mae: 0.3055\n",
      "Epoch 898: val_loss improved from 0.12810 to 0.12809, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1504 - mae: 0.3055 - val_loss: 0.1281 - val_mae: 0.2816\n",
      "Epoch 899/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1499 - mae: 0.3052\n",
      "Epoch 899: val_loss improved from 0.12809 to 0.12803, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1499 - mae: 0.3052 - val_loss: 0.1280 - val_mae: 0.2815\n",
      "Epoch 900/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1496 - mae: 0.3048\n",
      "Epoch 900: val_loss improved from 0.12803 to 0.12795, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1496 - mae: 0.3048 - val_loss: 0.1280 - val_mae: 0.2814\n",
      "Epoch 901/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1492 - mae: 0.3042\n",
      "Epoch 901: val_loss improved from 0.12795 to 0.12786, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1492 - mae: 0.3042 - val_loss: 0.1279 - val_mae: 0.2813\n",
      "Epoch 902/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1496 - mae: 0.3049\n",
      "Epoch 902: val_loss improved from 0.12786 to 0.12776, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1496 - mae: 0.3049 - val_loss: 0.1278 - val_mae: 0.2812\n",
      "Epoch 903/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1497 - mae: 0.3048\n",
      "Epoch 903: val_loss improved from 0.12776 to 0.12768, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1497 - mae: 0.3048 - val_loss: 0.1277 - val_mae: 0.2811\n",
      "Epoch 904/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1498 - mae: 0.3049\n",
      "Epoch 904: val_loss improved from 0.12768 to 0.12762, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1498 - mae: 0.3049 - val_loss: 0.1276 - val_mae: 0.2811\n",
      "Epoch 905/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1494 - mae: 0.3045\n",
      "Epoch 905: val_loss improved from 0.12762 to 0.12756, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1494 - mae: 0.3045 - val_loss: 0.1276 - val_mae: 0.2810\n",
      "Epoch 906/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1491 - mae: 0.3043\n",
      "Epoch 906: val_loss improved from 0.12756 to 0.12748, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1491 - mae: 0.3043 - val_loss: 0.1275 - val_mae: 0.2809\n",
      "Epoch 907/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1492 - mae: 0.3043\n",
      "Epoch 907: val_loss improved from 0.12748 to 0.12741, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1492 - mae: 0.3043 - val_loss: 0.1274 - val_mae: 0.2808\n",
      "Epoch 908/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1498 - mae: 0.3050\n",
      "Epoch 908: val_loss improved from 0.12741 to 0.12733, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1498 - mae: 0.3050 - val_loss: 0.1273 - val_mae: 0.2808\n",
      "Epoch 909/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1490 - mae: 0.3040\n",
      "Epoch 909: val_loss improved from 0.12733 to 0.12726, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1490 - mae: 0.3040 - val_loss: 0.1273 - val_mae: 0.2807\n",
      "Epoch 910/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1496 - mae: 0.3046\n",
      "Epoch 910: val_loss improved from 0.12726 to 0.12721, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1496 - mae: 0.3046 - val_loss: 0.1272 - val_mae: 0.2806\n",
      "Epoch 911/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1492 - mae: 0.3042\n",
      "Epoch 911: val_loss improved from 0.12721 to 0.12718, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1492 - mae: 0.3042 - val_loss: 0.1272 - val_mae: 0.2806\n",
      "Epoch 912/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1493 - mae: 0.3042\n",
      "Epoch 912: val_loss improved from 0.12718 to 0.12713, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1493 - mae: 0.3042 - val_loss: 0.1271 - val_mae: 0.2805\n",
      "Epoch 913/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1488 - mae: 0.3040\n",
      "Epoch 913: val_loss improved from 0.12713 to 0.12711, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1488 - mae: 0.3040 - val_loss: 0.1271 - val_mae: 0.2805\n",
      "Epoch 914/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1495 - mae: 0.3047\n",
      "Epoch 914: val_loss improved from 0.12711 to 0.12706, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1495 - mae: 0.3047 - val_loss: 0.1271 - val_mae: 0.2804\n",
      "Epoch 915/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1485 - mae: 0.3035\n",
      "Epoch 915: val_loss improved from 0.12706 to 0.12700, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1485 - mae: 0.3035 - val_loss: 0.1270 - val_mae: 0.2803\n",
      "Epoch 916/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1484 - mae: 0.3034\n",
      "Epoch 916: val_loss improved from 0.12700 to 0.12689, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1484 - mae: 0.3034 - val_loss: 0.1269 - val_mae: 0.2802\n",
      "Epoch 917/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1481 - mae: 0.3035\n",
      "Epoch 917: val_loss improved from 0.12689 to 0.12678, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1481 - mae: 0.3035 - val_loss: 0.1268 - val_mae: 0.2801\n",
      "Epoch 918/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1479 - mae: 0.3033\n",
      "Epoch 918: val_loss improved from 0.12678 to 0.12669, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1479 - mae: 0.3033 - val_loss: 0.1267 - val_mae: 0.2800\n",
      "Epoch 919/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1487 - mae: 0.3038\n",
      "Epoch 919: val_loss improved from 0.12669 to 0.12663, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1487 - mae: 0.3038 - val_loss: 0.1266 - val_mae: 0.2799\n",
      "Epoch 920/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1483 - mae: 0.3035\n",
      "Epoch 920: val_loss improved from 0.12663 to 0.12658, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1483 - mae: 0.3035 - val_loss: 0.1266 - val_mae: 0.2799\n",
      "Epoch 921/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1485 - mae: 0.3036\n",
      "Epoch 921: val_loss improved from 0.12658 to 0.12656, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1485 - mae: 0.3036 - val_loss: 0.1266 - val_mae: 0.2798\n",
      "Epoch 922/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1481 - mae: 0.3031\n",
      "Epoch 922: val_loss improved from 0.12656 to 0.12654, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1481 - mae: 0.3031 - val_loss: 0.1265 - val_mae: 0.2798\n",
      "Epoch 923/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1477 - mae: 0.3027\n",
      "Epoch 923: val_loss improved from 0.12654 to 0.12650, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1477 - mae: 0.3027 - val_loss: 0.1265 - val_mae: 0.2797\n",
      "Epoch 924/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1484 - mae: 0.3034\n",
      "Epoch 924: val_loss improved from 0.12650 to 0.12644, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1484 - mae: 0.3034 - val_loss: 0.1264 - val_mae: 0.2796\n",
      "Epoch 925/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1483 - mae: 0.3034\n",
      "Epoch 925: val_loss improved from 0.12644 to 0.12635, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1483 - mae: 0.3034 - val_loss: 0.1263 - val_mae: 0.2796\n",
      "Epoch 926/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1479 - mae: 0.3030\n",
      "Epoch 926: val_loss improved from 0.12635 to 0.12627, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1479 - mae: 0.3030 - val_loss: 0.1263 - val_mae: 0.2795\n",
      "Epoch 927/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1474 - mae: 0.3024\n",
      "Epoch 927: val_loss improved from 0.12627 to 0.12618, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1474 - mae: 0.3024 - val_loss: 0.1262 - val_mae: 0.2794\n",
      "Epoch 928/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1471 - mae: 0.3021\n",
      "Epoch 928: val_loss improved from 0.12618 to 0.12611, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1471 - mae: 0.3021 - val_loss: 0.1261 - val_mae: 0.2793\n",
      "Epoch 929/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1476 - mae: 0.3026\n",
      "Epoch 929: val_loss improved from 0.12611 to 0.12606, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1476 - mae: 0.3026 - val_loss: 0.1261 - val_mae: 0.2793\n",
      "Epoch 930/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1479 - mae: 0.3030\n",
      "Epoch 930: val_loss improved from 0.12606 to 0.12603, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1479 - mae: 0.3030 - val_loss: 0.1260 - val_mae: 0.2792\n",
      "Epoch 931/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1474 - mae: 0.3026\n",
      "Epoch 931: val_loss improved from 0.12603 to 0.12602, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1474 - mae: 0.3026 - val_loss: 0.1260 - val_mae: 0.2792\n",
      "Epoch 932/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1482 - mae: 0.3033\n",
      "Epoch 932: val_loss improved from 0.12602 to 0.12598, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1482 - mae: 0.3033 - val_loss: 0.1260 - val_mae: 0.2791\n",
      "Epoch 933/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1475 - mae: 0.3025\n",
      "Epoch 933: val_loss improved from 0.12598 to 0.12589, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1475 - mae: 0.3025 - val_loss: 0.1259 - val_mae: 0.2790\n",
      "Epoch 934/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1476 - mae: 0.3026\n",
      "Epoch 934: val_loss improved from 0.12589 to 0.12574, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1476 - mae: 0.3026 - val_loss: 0.1257 - val_mae: 0.2789\n",
      "Epoch 935/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1468 - mae: 0.3022\n",
      "Epoch 935: val_loss improved from 0.12574 to 0.12557, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1468 - mae: 0.3022 - val_loss: 0.1256 - val_mae: 0.2787\n",
      "Epoch 936/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1466 - mae: 0.3018\n",
      "Epoch 936: val_loss improved from 0.12557 to 0.12542, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1466 - mae: 0.3018 - val_loss: 0.1254 - val_mae: 0.2786\n",
      "Epoch 937/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1471 - mae: 0.3020\n",
      "Epoch 937: val_loss improved from 0.12542 to 0.12531, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1471 - mae: 0.3020 - val_loss: 0.1253 - val_mae: 0.2785\n",
      "Epoch 938/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1471 - mae: 0.3022\n",
      "Epoch 938: val_loss improved from 0.12531 to 0.12526, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1471 - mae: 0.3022 - val_loss: 0.1253 - val_mae: 0.2784\n",
      "Epoch 939/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1470 - mae: 0.3020\n",
      "Epoch 939: val_loss did not improve from 0.12526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1470 - mae: 0.3020 - val_loss: 0.1253 - val_mae: 0.2784\n",
      "Epoch 940/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1469 - mae: 0.3018\n",
      "Epoch 940: val_loss did not improve from 0.12526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1469 - mae: 0.3018 - val_loss: 0.1253 - val_mae: 0.2784\n",
      "Epoch 941/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1465 - mae: 0.3014\n",
      "Epoch 941: val_loss did not improve from 0.12526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1465 - mae: 0.3014 - val_loss: 0.1253 - val_mae: 0.2783\n",
      "Epoch 942/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1467 - mae: 0.3017\n",
      "Epoch 942: val_loss improved from 0.12526 to 0.12522, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1467 - mae: 0.3017 - val_loss: 0.1252 - val_mae: 0.2783\n",
      "Epoch 943/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1466 - mae: 0.3019\n",
      "Epoch 943: val_loss improved from 0.12522 to 0.12515, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1466 - mae: 0.3019 - val_loss: 0.1252 - val_mae: 0.2782\n",
      "Epoch 944/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1467 - mae: 0.3018\n",
      "Epoch 944: val_loss improved from 0.12515 to 0.12509, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1467 - mae: 0.3018 - val_loss: 0.1251 - val_mae: 0.2782\n",
      "Epoch 945/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1462 - mae: 0.3014\n",
      "Epoch 945: val_loss improved from 0.12509 to 0.12506, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1462 - mae: 0.3014 - val_loss: 0.1251 - val_mae: 0.2781\n",
      "Epoch 946/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1464 - mae: 0.3015\n",
      "Epoch 946: val_loss improved from 0.12506 to 0.12502, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1464 - mae: 0.3015 - val_loss: 0.1250 - val_mae: 0.2781\n",
      "Epoch 947/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1464 - mae: 0.3010\n",
      "Epoch 947: val_loss improved from 0.12502 to 0.12496, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1464 - mae: 0.3010 - val_loss: 0.1250 - val_mae: 0.2780\n",
      "Epoch 948/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1456 - mae: 0.3007\n",
      "Epoch 948: val_loss improved from 0.12496 to 0.12491, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1456 - mae: 0.3007 - val_loss: 0.1249 - val_mae: 0.2779\n",
      "Epoch 949/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1459 - mae: 0.3010\n",
      "Epoch 949: val_loss improved from 0.12491 to 0.12484, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1459 - mae: 0.3010 - val_loss: 0.1248 - val_mae: 0.2778\n",
      "Epoch 950/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1458 - mae: 0.3007\n",
      "Epoch 950: val_loss improved from 0.12484 to 0.12477, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1458 - mae: 0.3007 - val_loss: 0.1248 - val_mae: 0.2777\n",
      "Epoch 951/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1460 - mae: 0.3010\n",
      "Epoch 951: val_loss improved from 0.12477 to 0.12467, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1460 - mae: 0.3010 - val_loss: 0.1247 - val_mae: 0.2776\n",
      "Epoch 952/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1460 - mae: 0.3010\n",
      "Epoch 952: val_loss improved from 0.12467 to 0.12454, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1460 - mae: 0.3010 - val_loss: 0.1245 - val_mae: 0.2775\n",
      "Epoch 953/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1453 - mae: 0.3003\n",
      "Epoch 953: val_loss improved from 0.12454 to 0.12444, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1453 - mae: 0.3003 - val_loss: 0.1244 - val_mae: 0.2774\n",
      "Epoch 954/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1455 - mae: 0.3005\n",
      "Epoch 954: val_loss improved from 0.12444 to 0.12437, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1455 - mae: 0.3005 - val_loss: 0.1244 - val_mae: 0.2773\n",
      "Epoch 955/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1460 - mae: 0.3010\n",
      "Epoch 955: val_loss improved from 0.12437 to 0.12431, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1460 - mae: 0.3010 - val_loss: 0.1243 - val_mae: 0.2772\n",
      "Epoch 956/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1456 - mae: 0.3005\n",
      "Epoch 956: val_loss improved from 0.12431 to 0.12429, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1456 - mae: 0.3005 - val_loss: 0.1243 - val_mae: 0.2772\n",
      "Epoch 957/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1452 - mae: 0.3002\n",
      "Epoch 957: val_loss improved from 0.12429 to 0.12427, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1452 - mae: 0.3002 - val_loss: 0.1243 - val_mae: 0.2772\n",
      "Epoch 958/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1454 - mae: 0.3004\n",
      "Epoch 958: val_loss improved from 0.12427 to 0.12423, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1454 - mae: 0.3004 - val_loss: 0.1242 - val_mae: 0.2771\n",
      "Epoch 959/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1459 - mae: 0.3011\n",
      "Epoch 959: val_loss improved from 0.12423 to 0.12415, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1459 - mae: 0.3011 - val_loss: 0.1242 - val_mae: 0.2770\n",
      "Epoch 960/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1456 - mae: 0.3007\n",
      "Epoch 960: val_loss improved from 0.12415 to 0.12405, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1456 - mae: 0.3007 - val_loss: 0.1240 - val_mae: 0.2769\n",
      "Epoch 961/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1458 - mae: 0.3007\n",
      "Epoch 961: val_loss improved from 0.12405 to 0.12392, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1458 - mae: 0.3007 - val_loss: 0.1239 - val_mae: 0.2768\n",
      "Epoch 962/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1459 - mae: 0.3007\n",
      "Epoch 962: val_loss improved from 0.12392 to 0.12381, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1459 - mae: 0.3007 - val_loss: 0.1238 - val_mae: 0.2767\n",
      "Epoch 963/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1452 - mae: 0.3003\n",
      "Epoch 963: val_loss improved from 0.12381 to 0.12373, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1452 - mae: 0.3003 - val_loss: 0.1237 - val_mae: 0.2766\n",
      "Epoch 964/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1454 - mae: 0.3005\n",
      "Epoch 964: val_loss improved from 0.12373 to 0.12366, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1454 - mae: 0.3005 - val_loss: 0.1237 - val_mae: 0.2765\n",
      "Epoch 965/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1452 - mae: 0.3002\n",
      "Epoch 965: val_loss improved from 0.12366 to 0.12363, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1452 - mae: 0.3002 - val_loss: 0.1236 - val_mae: 0.2765\n",
      "Epoch 966/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1446 - mae: 0.2998\n",
      "Epoch 966: val_loss improved from 0.12363 to 0.12360, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1446 - mae: 0.2998 - val_loss: 0.1236 - val_mae: 0.2764\n",
      "Epoch 967/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1443 - mae: 0.2994\n",
      "Epoch 967: val_loss improved from 0.12360 to 0.12358, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1443 - mae: 0.2994 - val_loss: 0.1236 - val_mae: 0.2763\n",
      "Epoch 968/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1449 - mae: 0.2998\n",
      "Epoch 968: val_loss improved from 0.12358 to 0.12352, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1449 - mae: 0.2998 - val_loss: 0.1235 - val_mae: 0.2763\n",
      "Epoch 969/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1449 - mae: 0.2997\n",
      "Epoch 969: val_loss improved from 0.12352 to 0.12342, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1449 - mae: 0.2997 - val_loss: 0.1234 - val_mae: 0.2762\n",
      "Epoch 970/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1447 - mae: 0.2999\n",
      "Epoch 970: val_loss improved from 0.12342 to 0.12330, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1447 - mae: 0.2999 - val_loss: 0.1233 - val_mae: 0.2761\n",
      "Epoch 971/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1453 - mae: 0.3002\n",
      "Epoch 971: val_loss improved from 0.12330 to 0.12319, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1453 - mae: 0.3002 - val_loss: 0.1232 - val_mae: 0.2760\n",
      "Epoch 972/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1448 - mae: 0.2997\n",
      "Epoch 972: val_loss improved from 0.12319 to 0.12314, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1448 - mae: 0.2997 - val_loss: 0.1231 - val_mae: 0.2759\n",
      "Epoch 973/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1445 - mae: 0.2995\n",
      "Epoch 973: val_loss improved from 0.12314 to 0.12310, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1445 - mae: 0.2995 - val_loss: 0.1231 - val_mae: 0.2758\n",
      "Epoch 974/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1440 - mae: 0.2992\n",
      "Epoch 974: val_loss did not improve from 0.12310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1440 - mae: 0.2992 - val_loss: 0.1231 - val_mae: 0.2758\n",
      "Epoch 975/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1438 - mae: 0.2987\n",
      "Epoch 975: val_loss did not improve from 0.12310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1438 - mae: 0.2987 - val_loss: 0.1232 - val_mae: 0.2758\n",
      "Epoch 976/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1443 - mae: 0.2992\n",
      "Epoch 976: val_loss did not improve from 0.12310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1443 - mae: 0.2992 - val_loss: 0.1232 - val_mae: 0.2758\n",
      "Epoch 977/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1435 - mae: 0.2984\n",
      "Epoch 977: val_loss did not improve from 0.12310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.1435 - mae: 0.2984 - val_loss: 0.1231 - val_mae: 0.2757\n",
      "Epoch 978/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1437 - mae: 0.2985\n",
      "Epoch 978: val_loss improved from 0.12310 to 0.12300, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1437 - mae: 0.2985 - val_loss: 0.1230 - val_mae: 0.2756\n",
      "Epoch 979/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1441 - mae: 0.2990\n",
      "Epoch 979: val_loss improved from 0.12300 to 0.12287, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.1441 - mae: 0.2990 - val_loss: 0.1229 - val_mae: 0.2755\n",
      "Epoch 980/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1438 - mae: 0.2988\n",
      "Epoch 980: val_loss improved from 0.12287 to 0.12274, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1438 - mae: 0.2988 - val_loss: 0.1227 - val_mae: 0.2754\n",
      "Epoch 981/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1440 - mae: 0.2992\n",
      "Epoch 981: val_loss improved from 0.12274 to 0.12262, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1440 - mae: 0.2992 - val_loss: 0.1226 - val_mae: 0.2753\n",
      "Epoch 982/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1441 - mae: 0.2992\n",
      "Epoch 982: val_loss improved from 0.12262 to 0.12253, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1441 - mae: 0.2992 - val_loss: 0.1225 - val_mae: 0.2752\n",
      "Epoch 983/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1439 - mae: 0.2990\n",
      "Epoch 983: val_loss improved from 0.12253 to 0.12248, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1439 - mae: 0.2990 - val_loss: 0.1225 - val_mae: 0.2751\n",
      "Epoch 984/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1435 - mae: 0.2985\n",
      "Epoch 984: val_loss improved from 0.12248 to 0.12246, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1435 - mae: 0.2985 - val_loss: 0.1225 - val_mae: 0.2750\n",
      "Epoch 985/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1438 - mae: 0.2987\n",
      "Epoch 985: val_loss improved from 0.12246 to 0.12244, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1438 - mae: 0.2987 - val_loss: 0.1224 - val_mae: 0.2750\n",
      "Epoch 986/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1435 - mae: 0.2984\n",
      "Epoch 986: val_loss improved from 0.12244 to 0.12241, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1435 - mae: 0.2984 - val_loss: 0.1224 - val_mae: 0.2749\n",
      "Epoch 987/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1436 - mae: 0.2988\n",
      "Epoch 987: val_loss improved from 0.12241 to 0.12234, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1436 - mae: 0.2988 - val_loss: 0.1223 - val_mae: 0.2748\n",
      "Epoch 988/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1428 - mae: 0.2977\n",
      "Epoch 988: val_loss improved from 0.12234 to 0.12225, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1428 - mae: 0.2977 - val_loss: 0.1222 - val_mae: 0.2747\n",
      "Epoch 989/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1435 - mae: 0.2983\n",
      "Epoch 989: val_loss improved from 0.12225 to 0.12214, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1435 - mae: 0.2983 - val_loss: 0.1221 - val_mae: 0.2747\n",
      "Epoch 990/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1427 - mae: 0.2976\n",
      "Epoch 990: val_loss improved from 0.12214 to 0.12206, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1427 - mae: 0.2976 - val_loss: 0.1221 - val_mae: 0.2746\n",
      "Epoch 991/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1428 - mae: 0.2980\n",
      "Epoch 991: val_loss improved from 0.12206 to 0.12201, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.1428 - mae: 0.2980 - val_loss: 0.1220 - val_mae: 0.2745\n",
      "Epoch 992/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1433 - mae: 0.2982\n",
      "Epoch 992: val_loss improved from 0.12201 to 0.12196, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1433 - mae: 0.2982 - val_loss: 0.1220 - val_mae: 0.2745\n",
      "Epoch 993/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1432 - mae: 0.2980\n",
      "Epoch 993: val_loss improved from 0.12196 to 0.12195, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1432 - mae: 0.2980 - val_loss: 0.1220 - val_mae: 0.2744\n",
      "Epoch 994/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1431 - mae: 0.2982\n",
      "Epoch 994: val_loss improved from 0.12195 to 0.12190, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.1431 - mae: 0.2982 - val_loss: 0.1219 - val_mae: 0.2744\n",
      "Epoch 995/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1426 - mae: 0.2975\n",
      "Epoch 995: val_loss improved from 0.12190 to 0.12181, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1426 - mae: 0.2975 - val_loss: 0.1218 - val_mae: 0.2743\n",
      "Epoch 996/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1425 - mae: 0.2977\n",
      "Epoch 996: val_loss improved from 0.12181 to 0.12170, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1425 - mae: 0.2977 - val_loss: 0.1217 - val_mae: 0.2741\n",
      "Epoch 997/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1426 - mae: 0.2977\n",
      "Epoch 997: val_loss improved from 0.12170 to 0.12155, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.1426 - mae: 0.2977 - val_loss: 0.1216 - val_mae: 0.2740\n",
      "Epoch 998/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1427 - mae: 0.2974\n",
      "Epoch 998: val_loss improved from 0.12155 to 0.12143, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.1427 - mae: 0.2974 - val_loss: 0.1214 - val_mae: 0.2739\n",
      "Epoch 999/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1428 - mae: 0.2976\n",
      "Epoch 999: val_loss improved from 0.12143 to 0.12131, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1428 - mae: 0.2976 - val_loss: 0.1213 - val_mae: 0.2737\n",
      "Epoch 1000/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1420 - mae: 0.2973\n",
      "Epoch 1000: val_loss improved from 0.12131 to 0.12124, saving model to plpo.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.1420 - mae: 0.2973 - val_loss: 0.1212 - val_mae: 0.2737\n",
      "Restoring model weights from the end of the best epoch: 1000.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m1,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,985</span> (19.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,985\u001b[0m (19.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,661</span> (6.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,661\u001b[0m (6.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,324</span> (12.99 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,324\u001b[0m (12.99 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_file_name = 'plpo.keras'\n",
    "batch_size = len(X_train)\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(inputs,)),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, kernel_initializer= 'normal')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "epochs = 1000\n",
    "validation_batch_size = len(X_valid)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, restore_best_weights = True, verbose = 1)\n",
    "checkpoint = ModelCheckpoint(model_file_name,\n",
    "                                monitor = 'val_loss',\n",
    "                                save_best_only = True,\n",
    "                                mode = 'min',\n",
    "                                verbose = 1)\n",
    "\n",
    "callbacks = [early_stopping, checkpoint]\n",
    "\n",
    "model_history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_batch_size=validation_batch_size,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6f79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Loss:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1212 - mae: 0.2737\n",
      "Loss: [0.12124354392290115, 0.2736665606498718]\n",
      "\n",
      "Generating output predictions with model:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "\n",
      "Central Frequency Percentage 15cm: 34.27867878067419\n",
      "\n",
      "Central Frequency Percentage 5cm: 11.85573575613484\n",
      "\n",
      "Central Frequency Percentage 1cm: 2.362656024454445\n",
      "Mean Squared Error: 0.12124355433871768\n",
      "Root Mean Squared Error: 0.34820045137638417\n",
      "Mean Absolute Error: 0.2736665447765632\n",
      "Median Absolute Error: 0.2288742275238036\n",
      "R-squared: -4.290375770644729\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b74ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAHWCAYAAABJ4Xn8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaMZJREFUeJzt3Xd8FHX+x/H3bLLZ9ISaBAhVOogF9AcqRTqIYjsLKmA7FUREPUQFAQu2U05ULKegp1hPOAsKEUHFCiKIighKkyolpG82u/P7I8mSTQJkQ7IzIa/n45EH2e+0zy4fNO/MzHcM0zRNAQAAAAD8HFYXAAAAAAB2Q1ACAAAAgFIISgAAAABQCkEJAAAAAEohKAEAAABAKQQlAAAAACiFoAQAAAAApRCUAAAAAKAUghIAAAAAlEJQAgAbGDVqlJo3b16pbadOnSrDMKq2IJvZvHmzDMPQ3LlzQ35swzA0depU/+u5c+fKMAxt3rz5qNs2b95co0aNqtJ6jqVXAAAVR1ACgCMwDKNCX8uWLbO61Fpv3LhxMgxDGzduPOw6d999twzD0I8//hjCyoK3Y8cOTZ06VatXr7a6FL/isPrYY49ZXQoAhES41QUAgJ395z//CXj9yiuvKC0trcx4+/btj+k4L7zwgnw+X6W2veeee3TnnXce0/GPByNGjNCsWbM0b948TZkypdx1Xn/9dXXu3FknnnhipY9z5ZVX6tJLL5XL5ar0Po5mx44dmjZtmpo3b66TTjopYNmx9AoAoOIISgBwBFdccUXA62+++UZpaWllxkvLyclRdHR0hY/jdDorVZ8khYeHKzyc/5yffvrpOuGEE/T666+XG5S+/vprbdq0SQ899NAxHScsLExhYWHHtI9jcSy9AgCoOC69A4Bj1Lt3b3Xq1Enff/+9evbsqejoaN11112SpP/9738aOnSoGjVqJJfLpVatWum+++6T1+sN2Efp+05KXub0/PPPq1WrVnK5XOrWrZtWrFgRsG159ygZhqGxY8dqwYIF6tSpk1wulzp27KiPP/64TP3Lli1T165dFRkZqVatWum5556r8H1PX3zxhS6++GI1bdpULpdLqampuvXWW5Wbm1vm/cXGxmr79u0aPny4YmNj1aBBA91+++1lPov09HSNGjVKCQkJSkxM1MiRI5Wenn7UWqTCs0q//vqrVq1aVWbZvHnzZBiGLrvsMuXn52vKlCk69dRTlZCQoJiYGJ111llaunTpUY9R3j1Kpmnq/vvvV5MmTRQdHa0+ffro559/LrPt/v37dfvtt6tz586KjY1VfHy8Bg8erDVr1vjXWbZsmbp16yZJGj16tP/yzuL7s8q7Ryk7O1u33XabUlNT5XK51LZtWz322GMyTTNgvWD6orL27Nmja665RklJSYqMjFSXLl308ssvl1nvjTfe0Kmnnqq4uDjFx8erc+fO+te//uVf7vF4NG3aNLVu3VqRkZGqV6+ezjzzTKWlpVVZrQBwJPwKEgCqwL59+zR48GBdeumluuKKK5SUlCSp8Ifq2NhYTZgwQbGxsfr00081ZcoUZWRk6NFHHz3qfufNm6fMzEz9/e9/l2EYeuSRR3TBBRfojz/+OOqZheXLl+vdd9/VTTfdpLi4OD355JO68MILtXXrVtWrV0+S9MMPP2jQoEFKSUnRtGnT5PV6NX36dDVo0KBC7/vtt99WTk6ObrzxRtWrV0/fffedZs2apT///FNvv/12wLper1cDBw7U6aefrscee0yffPKJ/vnPf6pVq1a68cYbJRUGjvPOO0/Lly/XDTfcoPbt22v+/PkaOXJkheoZMWKEpk2bpnnz5umUU04JOPZbb72ls846S02bNtXevXv173//W5dddpmuu+46ZWZm6sUXX9TAgQP13Xfflbnc7WimTJmi+++/X0OGDNGQIUO0atUqDRgwQPn5+QHr/fHHH1qwYIEuvvhitWjRQrt379Zzzz2nXr166ZdfflGjRo3Uvn17TZ8+XVOmTNH111+vs846S5LUo0ePco9tmqbOPfdcLV26VNdcc41OOukkLVq0SHfccYe2b9+uJ554ImD9ivRFZeXm5qp3797auHGjxo4dqxYtWujtt9/WqFGjlJ6erltuuUWSlJaWpssuu0x9+/bVww8/LElat26dvvzyS/86U6dO1YwZM3TttdfqtNNOU0ZGhlauXKlVq1apf//+x1QnAFSICQCosDFjxpil/9PZq1cvU5L57LPPllk/JyenzNjf//53Mzo62szLy/OPjRw50mzWrJn/9aZNm0xJZr169cz9+/f7x//3v/+Zksz333/fP3bvvfeWqUmSGRERYW7cuNE/tmbNGlOSOWvWLP/YsGHDzOjoaHP79u3+sQ0bNpjh4eFl9lme8t7fjBkzTMMwzC1btgS8P0nm9OnTA9Y9+eSTzVNPPdX/esGCBaYk85FHHvGPFRQUmGeddZYpyZwzZ85Ra+rWrZvZpEkT0+v1+sc+/vhjU5L53HPP+ffpdrsDtjtw4ICZlJRkXn311QHjksx7773X/3rOnDmmJHPTpk2maZrmnj17zIiICHPo0KGmz+fzr3fXXXeZksyRI0f6x/Ly8gLqMs3Cv2uXyxXw2axYseKw77d0rxR/Zvfff3/AehdddJFpGEZAD1S0L8pT3JOPPvroYdeZOXOmKcl89dVX/WP5+flm9+7dzdjYWDMjI8M0TdO85ZZbzPj4eLOgoOCw++rSpYs5dOjQI9YEANWJS+8AoAq4XC6NHj26zHhUVJT/+8zMTO3du1dnnXWWcnJy9Ouvvx51v5dcconq1Knjf118duGPP/446rb9+vVTq1at/K9PPPFExcfH+7f1er365JNPNHz4cDVq1Mi/3gknnKDBgwcfdf9S4PvLzs7W3r171aNHD5mmqR9++KHM+jfccEPA67POOivgvSxcuFDh4eH+M0xS4T1BN998c4XqkQrvK/vzzz/1+eef+8fmzZuniIgIXXzxxf59RkRESJJ8Pp/279+vgoICde3atdzL9o7kk08+UX5+vm6++eaAyxXHjx9fZl2XyyWHo/B/vV6vV/v27VNsbKzatm0b9HGLLVy4UGFhYRo3blzA+G233SbTNPXRRx8FjB+tL47FwoULlZycrMsuu8w/5nQ6NW7cOGVlZemzzz6TJCUmJio7O/uIl9ElJibq559/1oYNG465LgCoDIISAFSBxo0b+3/wLunnn3/W+eefr4SEBMXHx6tBgwb+iSAOHjx41P02bdo04HVxaDpw4EDQ2xZvX7ztnj17lJubqxNOOKHMeuWNlWfr1q0aNWqU6tat67/vqFevXpLKvr/IyMgyl/SVrEeStmzZopSUFMXGxgas17Zt2wrVI0mXXnqpwsLCNG/ePElSXl6e5s+fr8GDBweEzpdfflknnnii//6XBg0a6MMPP6zQ30tJW7ZskSS1bt06YLxBgwYBx5MKQ9kTTzyh1q1by+VyqX79+mrQoIF+/PHHoI9b8viNGjVSXFxcwHjxTIzF9RU7Wl8ciy1btqh169b+MHi4Wm666Sa1adNGgwcPVpMmTXT11VeXuU9q+vTpSk9PV5s2bdS5c2fdcccdtp/WHcDxhaAEAFWg5JmVYunp6erVq5fWrFmj6dOn6/3331daWpr/noyKTPF8uNnVzFI36Vf1thXh9XrVv39/ffjhh5o4caIWLFigtLQ0/6QDpd9fqGaKa9iwofr376///ve/8ng8ev/995WZmakRI0b413n11Vc1atQotWrVSi+++KI+/vhjpaWl6eyzz67WqbcffPBBTZgwQT179tSrr76qRYsWKS0tTR07dgzZlN/V3RcV0bBhQ61evVrvvfee//6qwYMHB9yL1rNnT/3+++966aWX1KlTJ/373//WKaecon//+98hqxNA7cZkDgBQTZYtW6Z9+/bp3XffVc+ePf3jmzZtsrCqQxo2bKjIyMhyH9B6pIe2Flu7dq1+++03vfzyy7rqqqv848cyK1mzZs20ZMkSZWVlBZxVWr9+fVD7GTFihD7++GN99NFHmjdvnuLj4zVs2DD/8nfeeUctW7bUu+++G3C53L333lupmiVpw4YNatmypX/8r7/+KnOW5p133lGfPn304osvBoynp6erfv36/tcVmXGw5PE/+eQTZWZmBpxVKr60s7i+UGjWrJl+/PFH+Xy+gLNK5dUSERGhYcOGadiwYfL5fLrpppv03HPPafLkyf4zmnXr1tXo0aM1evRoZWVlqWfPnpo6daquvfbakL0nALUXZ5QAoJoU/+a+5G/q8/Pz9cwzz1hVUoCwsDD169dPCxYs0I4dO/zjGzduLHNfy+G2lwLfn2maAVM8B2vIkCEqKCjQ7Nmz/WNer1ezZs0Kaj/Dhw9XdHS0nnnmGX300Ue64IILFBkZecTav/32W3399ddB19yvXz85nU7NmjUrYH8zZ84ss25YWFiZMzdvv/22tm/fHjAWExMjSRWaFn3IkCHyer166qmnAsafeOIJGYZR4fvNqsKQIUO0a9cuvfnmm/6xgoICzZo1S7Gxsf7LMvft2xewncPh8D8E2O12l7tObGysTjjhBP9yAKhunFECgGrSo0cP1alTRyNHjtS4ceNkGIb+85//hPQSp6OZOnWqFi9erDPOOEM33nij/wfuTp06afXq1Ufctl27dmrVqpVuv/12bd++XfHx8frvf/97TPe6DBs2TGeccYbuvPNObd68WR06dNC7774b9P07sbGxGj58uP8+pZKX3UnSOeeco3fffVfnn3++hg4dqk2bNunZZ59Vhw4dlJWVFdSxip8HNWPGDJ1zzjkaMmSIfvjhB3300UcBZ4mKjzt9+nSNHj1aPXr00Nq1a/Xaa68FnImSpFatWikxMVHPPvus4uLiFBMTo9NPP10tWrQoc/xhw4apT58+uvvuu7V582Z16dJFixcv1v/+9z+NHz8+YOKGqrBkyRLl5eWVGR8+fLiuv/56Pffccxo1apS+//57NW/eXO+8846+/PJLzZw503/G69prr9X+/ft19tlnq0mTJtqyZYtmzZqlk046yX8/U4cOHdS7d2+deuqpqlu3rlauXKl33nlHY8eOrdL3AwCHQ1ACgGpSr149ffDBB7rtttt0zz33qE6dOrriiivUt29fDRw40OryJEmnnnqqPvroI91+++2aPHmyUlNTNX36dK1bt+6os/I5nU69//77GjdunGbMmKHIyEidf/75Gjt2rLp06VKpehwOh9577z2NHz9er776qgzD0Lnnnqt//vOfOvnkk4Pa14gRIzRv3jylpKTo7LPPDlg2atQo7dq1S88995wWLVqkDh066NVXX9Xbb7+tZcuWBV33/fffr8jISD377LNaunSpTj/9dC1evFhDhw4NWO+uu+5Sdna25s2bpzfffFOnnHKKPvzwQ915550B6zmdTr388suaNGmSbrjhBhUUFGjOnDnlBqXiz2zKlCl68803NWfOHDVv3lyPPvqobrvttqDfy9F8/PHH5T6gtnnz5urUqZOWLVumO++8Uy+//LIyMjLUtm1bzZkzR6NGjfKve8UVV+j555/XM888o/T0dCUnJ+uSSy7R1KlT/ZfsjRs3Tu+9954WL14st9utZs2a6f7779cdd9xR5e8JAMpjmHb61SYAwBaGDx/O1MwAgFqNe5QAoJbLzc0NeL1hwwYtXLhQvXv3tqYgAABsgDNKAFDLpaSkaNSoUWrZsqW2bNmi2bNny+1264cffijzbCAAAGoL7lECgFpu0KBBev3117Vr1y65XC51795dDz74ICEJAFCrcUYJAAAAAErhHiUAAAAAKIWgBAAAAAClHPf3KPl8Pu3YsUNxcXEyDMPqcgAAAABYxDRNZWZmqlGjRv7nth3OcR+UduzYodTUVKvLAAAAAGAT27ZtU5MmTY64znEflOLi4iQVfhjx8fGW1uLxeLR48WINGDBATqfT0lpQM9AzCBY9g2DRMwgWPYPKsEvfZGRkKDU11Z8RjuS4D0rFl9vFx8fbIihFR0crPj6e/7CgQugZBIueQbDoGQSLnkFl2K1vKnJLDpM5AAAAAEApBCUAAAAAKIWgBAAAAAClHPf3KAEAAMB+vF6vPB6P1WUgRDwej8LDw5WXlyev11ttxwkLC1N4eHiVPBaIoAQAAICQysrK0p9//inTNK0uBSFimqaSk5O1bdu2an+2aXR0tFJSUhQREXFM+yEoAQAAIGS8Xq/+/PNPRUdHq0GDBtX+QzPswefzKSsrS7GxsUd90Gtlmaap/Px8/fXXX9q0aZNat259TMciKAEAACBkPB6PTNNUgwYNFBUVZXU5CBGfz6f8/HxFRkZWW1CSpKioKDmdTm3ZssV/vMqydDKHzz//XMOGDVOjRo1kGIYWLFhQZp1169bp3HPPVUJCgmJiYtStWzdt3bo19MUCAACgynAmCdWlqoKYpUEpOztbXbp00dNPP13u8t9//11nnnmm2rVrp2XLlunHH3/U5MmTjykZAgAAAMDRWHrp3eDBgzV48ODDLr/77rs1ZMgQPfLII/6xVq1ahaI0AAAAALWYbe9R8vl8+vDDD/WPf/xDAwcO1A8//KAWLVpo0qRJGj58+GG3c7vdcrvd/tcZGRmSCq+HtXoKyuLjW10Hag56BsGiZxAsegbBOtaeKb5HyefzyefzVWVpNU7Lli11yy236JZbbqnQ+suWLVPfvn21b98+JSYmVm9xVax4hsPiv/vq5PP5ZJqmPB6PwsLCApYF07eGaZN5GQ3D0Pz58/0haNeuXUpJSVF0dLTuv/9+9enTRx9//LHuuusuLV26VL169Sp3P1OnTtW0adPKjM+bN0/R0dHV+RYAAABwFOHh4UpOTlZqauoxT98cKnXq1Dni8okTJ+rOO+8Mer979+5VdHR0hX9Gzc/P14EDB9SwYcNqvcdr+fLlGjZsmDZv3qyEhIRqO051yc/P17Zt27Rr1y4VFBQELMvJydHll1+ugwcPKj4+/oj7sW1Q2rFjhxo3bqzLLrtM8+bN86937rnnKiYmRq+//nq5+ynvjFJqaqr27t171A+junk8HqWlpal///5yOp2W1oKagZ5BsOgZBIueQbCOtWfy8vK0bds2NW/evMbcd75r1y7/92+99ZbuvfderVu3zj8WGxur2NhYSYVnTLxer8LDbXvh1lFVx5kr0zSVmZmpuLi4ap/IIy8vT5s3b1ZqamqZHsvIyFD9+vUrFJRs+zdYv359hYeHq0OHDgHj7du31/Llyw+7ncvlksvlKjPudDot/x/ArW/9qJUbw9SgQ5Z6tG5oaS2oWezQv6hZ6BkEi55BsCrbM16vV4ZhyOFwyOFwyDRN5Xq81VDh0UU5wyr0Q3ujRo383ycmJsowDP/YsmXL1KdPHy1cuFD33HOP1q5dq8WLFys1NVUTJkzQN998o+zsbLVv314zZsxQv379/Ptq3ry5xo8fr/Hjx0sqPHHwwgsv6MMPP9SiRYvUuHFj/fOf/9S5554bcKwDBw4oMTFRc+fO1fjx4/Xmm29q/Pjx2rZtm84880zNmTNHKSkpkqSCggJNmDBBr7zyisLCwnTttddq165dOnjwYLkzTkuHZo0r/jsq7cCBA7rlllv0/vvvy+12q1evXnryySfVunVrSdKWLVs0duxYLV++XPn5+WrevLkefvhhnXnmmUpPT9e4ceO0ePFiZWVlqUmTJrrrrrs0evToo/49VJTD4ZBhGOX2aDA9a9ugFBERoW7dumn9+vUB47/99puaNWtmUVXHZtuBXO3KNZSRy3XgAAAAkpTr8arDlEWWHPuX6QMVHVE1Pw7feeedeuyxx9SyZUvVqVNH27Zt05AhQ/TAAw/I5XLplVde0bBhw7R+/Xo1bdr0sPuZNm2aHnnkET366KOaNWuWRowYoS1btqhu3brlrp+Tk6PHHntM//nPf+RwOHTFFVfo9ttv12uvvSZJevjhh/Xaa69pzpw5at++vf71r39pwYIF6tOnT6Xf66hRo7Rhwwa99957io+P18SJEzVkyBD98ssvcjqdGjNmjPLz8/X5558rJiZGv/zyi/+M25QpU/TLL7/oo48+Uv369bVx40bl5uZWupbqZGlQysrK0saNG/2vN23apNWrV6tu3bpq2rSp7rjjDl1yySXq2bOn/x6l999/X8uWLbOu6GMQ6SxM5HkFtfvGRQAAgOPN9OnT1b9/f//runXrqkuXLv7X9913n+bPn6/33ntPY8eOPex+Ro0apcsuu0yS9OCDD+rJJ5/Ud999p0GDBpW7vsfj0bPPPuufGXrs2LGaPn26f/msWbM0adIknX/++ZKkp556SgsXLqz0+ywOSF9++aV69OghSXrttdeUmpqqBQsW6OKLL9bWrVt14YUXqnPnzpIKJ63w+XzKyMjQ1q1bdfLJJ6tr166SCs+q2ZWlQWnlypUBaXbChAmSpJEjR2ru3Lk6//zz9eyzz2rGjBkaN26c2rZtq//+978688wzrSr5mESGF866kWfR6WUAAAC7iXKG6ZfpAy07dlUp/sG/WFZWlqZOnaoPP/xQO3fuVEFBgXJzc7V169Yj7ufEE0/0fx8TE6P4+Hjt2bPnsOtHR0cHPD4nJSXFv/7Bgwe1e/dunXbaaf7lYWFhOvXUUys989y6desUHh6u008/3T9Wr149tW3b1n/f1rhx43TjjTdq8eLF6tevny688EJ16tRJknTDDTfo4osv1qpVqzRgwAANHz7cH7jsxtKg1Lt3bx1tLomrr75aV199dYgqql4R4YVnlNycUQIAAJBUeF9OVV3+ZqWYmJiA17fffrvS0tL02GOP6YQTTlBUVJQuuugi5efnH3E/pe+hMQzjiKGmvPWtnqvt2muv1cCBA/Xhhx9q8eLFmjFjhh577DFdddVVGjx4sLZs2aKFCxcqLS1Nffv21ZgxY/TYY49ZWnN5yt6dhWpTfOkdQQkAAOD49uWXX2rUqFE6//zz1blzZyUnJ2vz5s0hrSEhIUFJSUlasWKFf8zr9WrVqlWV3mf79u1VUFCgb7/91j+2b98+rV+/PmASttTUVN1www169913ddttt+nf//63f1mDBg00cuRIvfrqq5o5c6aef/75StdTnWp+fK9BXFx6BwAAUCu0bt1a7777roYNGybDMDR58mRLHrB78803a8aMGTrhhBPUrl07zZo1SwcOHKjQbH9r165VXFyc/7VhGOrSpYvOO+88XXfddXruuecUFxenO++8U40bN9Z5550nSRo/frwGDx6sNm3a6MCBA1q6dKnatWsnSbr33nvVtWtXdezYUW63Wx988IHat29fPW/+GBGUQogzSgAAALXD448/rquvvlo9evRQ/fr1NXHiRGVkZIS8jokTJ2rXrl266qqrFBYWpuuvv14DBw5UWNjR78/q2bNnwOuwsDAVFBRozpw5uuWWW3TOOecoPz9fPXv21MKFC/2XAXq9Xo0ZM0Z//vmn4uPjNWjQIP3zn/+UVDiz9aRJk7R582ZFRUXprLPO0htvvFH1b7wK2OaBs9UlIyNDCQkJFXqoVHW77/2f9OKXW3TNGc00eVgnS2tBzeDxeLRw4UINGTKE55ugQugZBIueQbCOtWfy8vK0adMmtWjRosY8cPZ44vP51L59e/3tb3/TfffdF9LjZmRkKD4+vtxnM1WlI/VYMNmAM0ohVHzpXT5nlAAAABACW7Zs0eLFi9WrVy+53W499dRT2rRpky6//HKrS7M9JnMIIZ6jBAAAgFByOByaO3euunXrpjPOOENr167VJ598Ytv7guyEM0oh5CqaHpzJHAAAABAKqamp+vLLL60uo0bijFIIuYoeasZkDgAAAIC9EZRCKLL4gbMeghIAAABgZwSlEPJfelfApXcAAACAnRGUQiiSS+8AAACAGoGgFEKu4lnvuPQOAAAAsDWCUggVX3qXz6V3AAAAgK0RlEIosuiBs5xRAgAAqH169+6t8ePH+183b95cM2fOPOI2hmFowYIFx3zsqtpPbUJQCiEmcwAAAKh5hg0bpkGDBpW77IsvvpBhGPrxxx+D3u+KFSt0/fXXH2t5AaZOnaqTTjqpzPjOnTs1ePDgKj1WaXPnzlViYmK1HiOUCEohxGQOAAAANc8111yjtLQ0/fnnn2WWzZkzR127dtWJJ54Y9H4bNGig6OjoqijxqJKTk+VyuUJyrOMFQSmEiidz4DlKAAAARUxTys+25ss0K1TiOeecowYNGmju3LkB41lZWXr77bd1zTXXaN++fbrsssvUuHFjRUdHq3Pnznr99dePuN/Sl95t2LBBPXv2VGRkpDp06KC0tLQy20ycOFFt2rRRdHS0WrZsqcmTJ8vj8UgqPKMzbdo0rVmzRoZhyDAMf82lL71bu3atzj77bEVFRalevXq6/vrrlZWV5V8+atQoDR8+XI899phSUlJUr149jRkzxn+syti2bZuGDx+u2NhYxcfH629/+5t2797tX75mzRr16dNHcXFxio+P16mnnqqVK1dKkrZs2aJhw4apTp06iomJUceOHbVw4cJK11IR4dW6dwQovvSuwGeqwOtTeBg5FQAA1HKeHOnBRtYc+64dUkTMUVcLDw/XVVddpblz5+ruu++WYRiSpLffflter1eXXXaZsrKydOqpp2rixImKj4/Xhx9+qCuvvFKtWrXSaaeddtRj+Hw+XXDBBUpKStK3336rgwcPBtzPVCwuLk5z585Vo0aNtHbtWl133XWKi4vTP/7xD11yySX66aef9PHHH+uTTz6RJCUkJJTZR3Z2tgYOHKju3btrxYoV2rNnj6699lqNHTs2IAwuXbpUKSkpWrp0qTZu3KhLLrlEJ510kq677rqjvp/y3t+IESOUkJCgzz77TAUFBRozZowuueQSLVu2TJI0YsQInXzyyZo9e7bCwsK0evVqOZ1OSdKYMWOUn5+vzz//XDExMfrll18UGxsbdB3BICiFUPFkDlLh5XcEJQAAgJrh6quv1qOPPqrPPvtMvXv3llR42d2FF16ohIQEJSQk6Pbbb/evf/PNN2vRokV66623KhSUPvnkE/36669atGiRGjUqDI4PPvhgmfuK7rnnHv/3zZs31+2336433nhD//jHPxQVFaXY2FiFh4crOTn5sMeaN2+e8vLy9MorrygmpjAoPvXUUxo2bJgefvhhJSUlSZLq1Kmjp556SmFhYWrXrp2GDh2qJUuWVCooLVmyRL/88ot+//13NWvWTJL0yiuvqGPHjlqxYoW6deumrVu36o477lC7du0kSa1bt/Zvv3XrVl144YXq3LmzJKlly5ZB1xAsglIIFZ9RkqQ8j1cxLj5+AABQyzmjC8/sWHXsCmrXrp169Oihl156Sb1799bGjRv1xRdfaPr06ZIkr9erBx98UG+99Za2b9+u/Px8ud3uCt+DtG7dOqWmpvpDkiR17969zHpvvvmmnnzySf3+++/KyspSQUGB4uPjK/w+io/VpUsXf0iSpDPOOEM+n0/r16/3B6WOHTsqLOzQL/pTUlK0du3aoI5V7Ndff1Xjxo2VmprqH+vQoYMSExO1bt06devWTRMmTNC1116r//znP+rXr58uvvhitWrVSpI0btw43XjjjVq8eLH69eunCy+8sFL3hQWDUxoh5HAYCjMKr4VlQgcAAABJhlF4+ZsVX0WX0FXUNddco//+97/KzMzUnDlz1KpVK/Xq1UuS9Oijj+pf//qXJk6cqKVLl2r16tUaOHCg8vPzq+yj+vrrrzVixAgNGTJEH3zwgX744QfdfffdVXqMkooveytmGIZ8vur7GXbq1Kn6+eefNXToUH366afq0KGD5s+fL0m69tpr9ccff+jKK6/U2rVr1bVrV82aNavaapEISiEXUfSJ53mYIhwAAKAm+dvf/iaHw6F58+bplVde0dVXX+2/X+nLL7/UeeedpyuuuEJdunRRy5Yt9dtvv1V43+3bt9e2bdu0c+dO/9g333wTsM5XX32lZs2a6e6771bXrl3VunVrbdmyJWCdiIgIeb1H/jmzffv2WrNmjbKzs/1jX375pRwOh9q2bVvhmoPRrl07bd++Xdu2bfOP/fLLL0pPT1eHDh38Y23atNGtt96qxYsX64ILLtCcOXP8y1JTU3XDDTfo3Xff1W233aYXXnihWmotRlAKsXB/UOKMEgAAQE0SGxurSy65RJMmTdLOnTs1atQo/7LWrVsrLS1NX331ldatW6e///3vATO6HU2/fv3Upk0bjRw5UmvWrNEXX3yhu+++O2Cd1q1ba+vWrXrjjTf0+++/68knn/SfcSnWvHlzbdq0SatXr9bevXvldrvLHGvEiBGKjIzUyJEj9dNPP2np0qW6+eabdeWVV/ovu6ssr9er1atXB3ytW7dO/fr1U4cOHXTllVdq1apV+u6773TVVVepV69e6tq1q3JzczV27FgtW7ZMW7Zs0ZdffqkVK1aoffv2kqTx48dr0aJF2rRpk1atWqWlS5f6l1UXglKIFc0QLjcPnQUAAKhxrrnmGh04cEADBw4MuJ/onnvu0SmnnKKBAweqd+/eSk5O1vDhwyu8X4fDofnz5ys3N1ennXaarr32Wj3wwAMB65x77rm69dZbNXbsWJ100kn66quvNHny5IB1LrzwQg0aNEh9+vRRgwYNyp2iPDo6WosWLdL+/fvVrVs3XXTRRerbt6+eeuqp4D6McmRlZenkk08O+Bo2bJgMw9Brr72mxMRE9ezZU/369VPLli315ptvSpLCwsK0b98+XXXVVWrTpo3+9re/afDgwZo2bZqkwgA2ZswYtW/fXoMGDVKbNm30zDPPHHO9R2KYZgUnkK+hMjIylJCQoIMHDwZ9o1tV83g8OvPBRdqda+j16/5P3VvVs7Qe2J/H49HChQs1ZMiQMtcJA+WhZxAsegbBOtaeycvL06ZNm9SiRQtFRkZWQ4WwI5/Pp4yMDMXHx8vhqN5zNUfqsWCyAWeUQil9izoamxSnHOVxRgkAAACwLYJSCIW/M0ov+e7RKY4NcnOPEgAAAGBbBKUQMovm6o+Um3uUAAAAABsjKIWSM0qSFC03Z5QAAAAAGyMohVLRGaUoI597lAAAQK12nM8nBgtVVW8RlEKp6IxSlNw8cBYAANRKYWFhkqT8/HyLK8HxKicnR5KOeSbP8KooBhVUFJQilc+ldwAAoFYKDw9XdHS0/vrrLzmdzmqfKhr24PP5lJ+fr7y8vGr7OzdNUzk5OdqzZ48SExP9obyyCEohZDpjJEnRRp5yuPQOAADUQoZhKCUlRZs2bdKWLVusLgchYpqmcnNzFRUVJcMwqvVYiYmJSk5OPub9EJRCyX/pXb72c0YJAADUUhEREWrdujWX39UiHo9Hn3/+uXr27FmtD7d2Op3HfCapmKVB6fPPP9ejjz6q77//Xjt37tT8+fM1fPjwcte94YYb9Nxzz+mJJ57Q+PHjQ1pnlfFfesf04AAAoHZzOByKjIy0ugyESFhYmAoKChQZGVmtQakqWXpRaHZ2trp06aKnn376iOvNnz9f33zzjRo1ahSiyqpJ8fTghlt5nFECAAAAbMvSM0qDBw/W4MGDj7jO9u3bdfPNN2vRokUaOnRoiCqrJsXTgytf7gKCEgAAAGBXtr5Hyefz6corr9Qdd9yhjh07Vmgbt9stt9vtf52RkSGp8LpIj8dTLXVWlM8RoTAVTg+e67a+HthfcY/QK6goegbBomcQLHoGlWGXvgnm+LYOSg8//LDCw8M1bty4Cm8zY8YMTZs2rcz44sWLFR0dXZXlBS0lfYNOkxRp5Gvbzt1auHChpfWg5khLS7O6BNQw9AyCRc8gWPQMKsPqvil+xlJF2DYoff/99/rXv/6lVatWBTWF4KRJkzRhwgT/64yMDKWmpmrAgAGKj4+vjlIrzLveIW2apWi5FZdYV0OGnGZpPbA/j8ejtLQ09e/fv8bc+Ahr0TMIFj2DYNEzqAy79E3x1WYVYdug9MUXX2jPnj1q2rSpf8zr9eq2227TzJkztXnz5nK3c7lccrlcZcadTqfl/5iNqMKgFiW33AWm5fWg5rBD/6JmoWcQLHoGwaJnUBlW900wx7ZtULryyivVr1+/gLGBAwfqyiuv1OjRoy2q6hiFF00PbuQr18P04AAAAIBdWRqUsrKytHHjRv/rTZs2afXq1apbt66aNm2qevXqBazvdDqVnJystm3bhrrUKmEWzXoXLbdy8wlKAAAAgF1ZGpRWrlypPn36+F8X31s0cuRIzZ0716KqqlFE8fTgPHAWAAAAsDNLg1Lv3r1lmmaF1z/cfUk1hv/SO4/y8plSEwAAALArh9UF1CrOKP+3pic3qJAIAAAAIHQISqFUIihFmm7le30WFgMAAADgcAhKoWQ4VGBESJKijHzleQhKAAAAgB0RlELM6ygMSpFyK48pwgEAAABbIiiFWHFQilI+U4QDAAAANkVQCjGvwyWp8FlKeUwRDgAAANgSQSnEioNSlMFDZwEAAAC7IiiF2KF7lPKVyz1KAAAAgC0RlEKsOChFK09uZr0DAAAAbImgFGIF/kvvOKMEAAAA2BVBKcSK71GKFPcoAQAAAHZFUAqxgOnBOaMEAAAA2BJBKcT89ygZPHAWAAAAsCuCUoj5pwcXQQkAAACwK4JSiB26R4lL7wAAAAC7IiiFWEHApXdMDw4AAADYEUEpxIrPKEXLzRklAAAAwKYISiFWPJlDpNzKY3pwAAAAwJYISiFW4IiUVHTpXQFBCQAAALAjglKIHbr0Lo8HzgIAAAA2RVAKsYIw7lECAAAA7I6gFGL+M0pGnnKZ9Q4AAACwJYJSiPnvUZJbbs4oAQAAALZEUAqx4qAUY7iVl++xuBoAAAAA5SEohVjxpXeSZObnWFgJAAAAgMMhKIWY1+GUKUOSZBQQlAAAAAA7IiiFmuGQ6YySJIV5CEoAAACAHRGULGA6YyRJTl+evD7T4moAAAAAlEZQsoDhjJZU+NDZPGa+AwAAAGyHoGQFV6wkKdrgobMAAACAHRGUrFB0RilGecrNJygBAAAAdkNQskJE4T1KUeKMEgAAAGBHBCUrFJ9RMvKUwxklAAAAwHYsDUqff/65hg0bpkaNGskwDC1YsMC/zOPxaOLEiercubNiYmLUqFEjXXXVVdqxY4d1BVeVEmeUcvILLC4GAAAAQGmWBqXs7Gx16dJFTz/9dJllOTk5WrVqlSZPnqxVq1bp3Xff1fr163XuuedaUGnVMrlHCQAAALC1cCsPPnjwYA0ePLjcZQkJCUpLSwsYe+qpp3Taaadp69atatq0aShKrB7FZ5QMN5feAQAAADZkaVAK1sGDB2UYhhITEw+7jtvtltvt9r/OyMiQVHgpn8fjqe4Sj6j4+L6wSIWp8IxSZm6+5XXBvop7gx5BRdEzCBY9g2DRM6gMu/RNMMevMUEpLy9PEydO1GWXXab4+PjDrjdjxgxNmzatzPjixYsVHR1dnSVW2G+bt6ujpGgjT1+t/lExu9dYXRJsrvTZVeBo6BkEi55BsOgZVIbVfZOTk1PhdWtEUPJ4PPrb3/4m0zQ1e/bsI647adIkTZgwwf86IyNDqampGjBgwBEDVih4PB6lpaWpdceTpB1vKlputWjdTkN6trC0LthXcc/0799fTqfT6nJQA9AzCBY9g2DRM6gMu/RN8dVmFWH7oFQckrZs2aJPP/30qGHH5XLJ5XKVGXc6nbb5x+yILHwPMcqT22vapi7Yl536FzUDPYNg0TMIFj2DyrC6b4I5tq2DUnFI2rBhg5YuXap69epZXVLVYDIHAAAAwNYsDUpZWVnauHGj//WmTZu0evVq1a1bVykpKbrooou0atUqffDBB/J6vdq1a5ckqW7duoqIiLCq7GPnnx6coAQAAADYkaVBaeXKlerTp4//dfG9RSNHjtTUqVP13nvvSZJOOumkgO2WLl2q3r17h6rMqud/4GyecnngLAAAAGA7lgal3r17yzTNwy4/0rKazP/AWS69AwAAAGzJYXUBtZL/jJJbuR6CEgAAAGA3BCUr+O9RylOOm0vvAAAAALshKFmh6IySwzBVkJ9rcTEAAAAASiMoWaHojJIkKT/LujoAAAAAlIugZAVHmHxhkZIkIz/b4mIAAAAAlEZQsogvIlaSFOYhKAEAAAB2Q1CySnFQKsg+bqdBBwAAAGoqgpJFjMg4SVK0maN8r8/iagAAAACURFCyiOEqDEoxylMuD50FAAAAbIWgZBFHcVAycpVDUAIAAABshaBkFVfhPUqxyiMoAQAAADZDULKK/9K7XC69AwAAAGyGoGSVolnvYo1c5eQXWFwMAAAAgJIISlYpOqMUqzzleDijBAAAANgJQckqRWeUYgwuvQMAAADshqBklRLTgzOZAwAAAGAvBCWrFM16F2fkKpd7lAAAAABbIShZJeLQrHecUQIAAADshaBklaIzSlx6BwAAANgPQckq/unB85TLrHcAAACArRCUrFLigbPZbu5RAgAAAOyEoGSV4qBkuJXrzre4GAAAAAAlEZSsUnTpnSQV5GVaWAgAAACA0ghKVgl3yWeES5JMghIAAABgKwQlqxiGCpyFZ5VMd5bFxQAAAAAoiaBkIZ8zRpJkuDmjBAAAANgJQclCZtF9Sg5PtsWVAAAAACiJoGSl4qBUwKV3AAAAgJ0QlCxkRMZLksI5owQAAADYCkHJQmGRhc9SijRzlV/gs7gaAAAAAMUIShYqDkpxylVOfoHF1QAAAAAoRlCykKMoKMUYucpyE5QAAAAAuyAoWclVFJSUp5x8r8XFAAAAAChGULJS0ax3sUaesjmjBAAAANiGpUHp888/17Bhw9SoUSMZhqEFCxYELDdNU1OmTFFKSoqioqLUr18/bdiwwZpiq4OrKCgpV9luzigBAAAAdmFpUMrOzlaXLl309NNPl7v8kUce0ZNPPqlnn31W3377rWJiYjRw4EDl5eWFuNJqElF86V2uspnMAQAAALCNcCsPPnjwYA0ePLjcZaZpaubMmbrnnnt03nnnSZJeeeUVJSUlacGCBbr00ktDWWr1KLpHKdbI016CEgAAAGAblgalI9m0aZN27dqlfv36+ccSEhJ0+umn6+uvvz5sUHK73XK73f7XGRkZkiSPxyOPx1O9RR9F8fGL/zTCIhWuwjNKB3PyLa8P9lO6Z4CjoWcQLHoGwaJnUBl26Ztgjm/boLRr1y5JUlJSUsB4UlKSf1l5ZsyYoWnTppUZX7x4saKjo6u2yEpKS0uTJCXm/KFekmKMPK1a85Pq7F1rbWGwreKeASqKnkGw6BkEi55BZVjdNzk5ORVe17ZBqbImTZqkCRMm+F9nZGQoNTVVAwYMUHx8vIWVFSbYtLQ09e/fX06nU9q3QVo/VXHKVdOWrTWk7wmW1gf7KdMzwFHQMwgWPYNg0TOoDLv0TfHVZhVh26CUnJwsSdq9e7dSUlL847t379ZJJ5102O1cLpdcLleZcafTaZt/zP5aoutIKrz0Ltfjs019sB879S9qBnoGwaJnECx6BpVhdd8Ec2zbPkepRYsWSk5O1pIlS/xjGRkZ+vbbb9W9e3cLK6tCRZM5hBmmPO5si4sBAAAAUMzSM0pZWVnauHGj//WmTZu0evVq1a1bV02bNtX48eN1//33q3Xr1mrRooUmT56sRo0aafjw4dYVXZUiYmTKkCFT3txMq6sBAAAAUMTSoLRy5Ur16dPH/7r43qKRI0dq7ty5+sc//qHs7Gxdf/31Sk9P15lnnqmPP/5YkZGRVpVctQxDBeHRchZky5dX8eslAQAAAFQvS4NS7969ZZrmYZcbhqHp06dr+vTpIawqtArCY+QsyJbp5owSAAAAYBe2vUeptvBFxEqSHPlZFlcCAAAAoBhByWK+iARJkiOfM0oAAACAXRCUrBZZ+Gwnp4egBAAAANgFQcliRlFQiiggKAEAAAB2QVCyWFh0oiTJ5c2Sz3f4iS0AAAAAhA5ByWLhRUEpTjnK8XitLQYAAACAJIKS5UoGpay8AmuLAQAAACCJoGQ5I7Jw1rt4I0dZbo/F1QAAAACQCErWKwpKccpRJmeUAAAAAFsgKFnNVTjrXbxBUAIAAADsgqBktRJnlLLcBCUAAADADghKVit6jlKcwWQOAAAAgF0QlKxWfOmdcpTJGSUAAADAFghKViu69M5lFCgnJ8viYgAAAABIBCXrRcTKlCFJKshOt7YWAAAAAJIIStZzOOQOj5UkeXMPWlwMAAAAAImgZAsFRUHJzE23thAAAAAAkghKtlAQUTihg/IyrC0EAAAAgCSCki34ioOSm6AEAAAA2AFByQ6KpggP9xCUAAAAADsgKNmAEVU4RXh4fqbFlQAAAACQCEq24CgKSi4vz1ECAAAA7ICgZAPhMYmSCoOSz2daWwwAAAAAgpIdREQnSpLilKMcj9faYgAAAAAQlOwgPKaOpMKglJVXYHE1AAAAAAhKNmBEFs56F2/kKMvtsbgaAAAAAJUKStu2bdOff/7pf/3dd99p/Pjxev7556ussFqlaHrweOUokzNKAAAAgOUqFZQuv/xyLV26VJK0a9cu9e/fX999953uvvtuTZ8+vUoLrBUiEyVJcQZBCQAAALCDSgWln376Saeddpok6a233lKnTp301Vdf6bXXXtPcuXOrsr7aIfLQGaUsN0EJAAAAsFqlgpLH45HL5ZIkffLJJzr33HMlSe3atdPOnTurrrraIrLwOUqxylVWbr7FxQAAAACoVFDq2LGjnn32WX3xxRdKS0vToEGDJEk7duxQvXr1qrTAWqHoHiWHYSov56DFxQAAAACoVFB6+OGH9dxzz6l379667LLL1KVLF0nSe++9578kD0FwRspjREiSPFnp1tYCAAAAQOGV2ah3797au3evMjIyVKdOHf/49ddfr+jo6CorrjbJD4uRsyBf3px0q0sBAAAAar1KnVHKzc2V2+32h6QtW7Zo5syZWr9+vRo2bFhlxXm9Xk2ePFktWrRQVFSUWrVqpfvuu0+maVbZMezC44yTJPnyuPQOAAAAsFqlziidd955uuCCC3TDDTcoPT1dp59+upxOp/bu3avHH39cN954Y5UU9/DDD2v27Nl6+eWX1bFjR61cuVKjR49WQkKCxo0bVyXHsAuPM07KlUyCEgAAAGC5Sp1RWrVqlc466yxJ0jvvvKOkpCRt2bJFr7zyip588skqK+6rr77Seeedp6FDh6p58+a66KKLNGDAAH333XdVdgy78EUUznzncBOUAAAAAKtV6oxSTk6O4uIKLxVbvHixLrjgAjkcDv3f//2ftmzZUmXF9ejRQ88//7x+++03tWnTRmvWrNHy5cv1+OOPH3Ybt9stt9vtf52RkSGpcEpzj8dTZbVVRvHxy6vDWzTzXVjeQcvrhH0cqWeA8tAzCBY9g2DRM6gMu/RNMMevVFA64YQTtGDBAp1//vlatGiRbr31VknSnj17FB8fX5ldluvOO+9URkaG2rVrp7CwMHm9Xj3wwAMaMWLEYbeZMWOGpk2bVmZ88eLFtploIi0trcxYs4x8NZKk7L+0cOHCkNcEeyuvZ4AjoWcQLHoGwaJnUBlW901OTk6F161UUJoyZYouv/xy3XrrrTr77LPVvXt3SYVh5OSTT67MLsv11ltv6bXXXtO8efPUsWNHrV69WuPHj1ejRo00cuTIcreZNGmSJkyY4H+dkZGh1NRUDRgwoEpDXGV4PB6lpaWpf//+cjqdAcv2vfe1tPZTxYW5NWTIEIsqhN0cqWeA8tAzCBY9g2DRM6gMu/RN8dVmFVGpoHTRRRfpzDPP1M6dO/3PUJKkvn376vzzz6/MLst1xx136M4779Sll14qSercubO2bNmiGTNmHDYouVwuuVyuMuNOp9M2/5jLq8UZV1+SFO3NsE2dsA879S9qBnoGwaJnECx6BpVhdd8Ec+xKBSVJSk5OVnJysv78809JUpMmTar8YbM5OTlyOALnmwgLC5PP56vS49hBRGw9SVK0N0s+nymHw7C4IgAAAKD2qtSsdz6fT9OnT1dCQoKaNWumZs2aKTExUffdd1+Vhphhw4bpgQce0IcffqjNmzdr/vz5evzxx6v0rJVduOLqSpISjSxl5xdYXA0AAABQu1XqjNLdd9+tF198UQ899JDOOOMMSdLy5cs1depU5eXl6YEHHqiS4mbNmqXJkyfrpptu0p49e9SoUSP9/e9/15QpU6pk/3bijC0KSspSZl6B4iI5lQ0AAABYpVJB6eWXX9a///1vnXvuuf6xE088UY0bN9ZNN91UZUEpLi5OM2fO1MyZM6tkf3ZmRBcGpXgjW3tzPWqUGGVxRQAAAEDtValL7/bv36927dqVGW/Xrp32799/zEXVSpGJkqREZSsjJ9/aWgAAAIBarlJBqUuXLnrqqafKjD/11FM68cQTj7moWimqjiTJaXiVlZlubS0AAABALVepS+8eeeQRDR06VJ988on/GUpff/21tm3bxsNSK8sZJY+ccsqjvEzOygEAAABWqtQZpV69eum3337T+eefr/T0dKWnp+uCCy7Qzz//rP/85z9VXWPtYBjKCYuTJOVn7bO4GAAAAKB2q/RzlBo1alRm0oY1a9boxRdf1PPPP3/MhdVGec54JXj3y5vFGSUAAADASpU6o4Tq4XHGS5K8OQcsrgQAAACo3QhKNlIQkShJMvIISgAAAICVCEo2YhZNEe7IO2htIQAAAEAtF9Q9ShdccMERl6enpx9LLSiaIjw8n6AEAAAAWCmooJSQkHDU5VddddUxFVSbOaILg1IEQQkAAACwVFBBac6cOdVVByQ5Y+pKkiK9GRZXAgAAANRu3KNkI864wqAUTVACAAAALEVQspHI+AaSpHgzSx6vz+JqAAAAgNqLoGQjUYkNJUl1jExl5HosrgYAAACovQhKNhIeW1+SVFcEJQAAAMBKBCU7iSq8R8lleJSZycx3AAAAgFUISnYSEaN8OSVJuQf/srgYAAAAoPYiKNmJYSjTUfisKjdBCQAAALAMQclmcsILg1JBFkEJAAAAsApByWZynYmSJG/WXmsLAQAAAGoxgpLNeCISC7/J2W9pHQAAAEBtRlCyGV/RzHdGzj6LKwEAAABqL4KSzZjRhUEpzJ1ubSEAAABALUZQspmwmMKHzrryD1hcCQAAAFB7EZRsxhlXGJSiCtKtLQQAAACoxQhKNhOZ0FCSFOs9aHElAAAAQO1FULKZ6KKgFG9myDRNi6sBAAAAaieCks3E1k2SJNVRpjLzPBZXAwAAANROBCWbiYxvIEmKMLw6mM6EDgAAAIAVCEp2ExGtPEVIkrL277a4GAAAAKB2IijZUIaRIEnKSd9lcSUAAABA7URQsqHM8DqSJE/GHosrAQAAAGongpIN5TjrSpK8GVx6BwAAAFiBoGRD+a7CoGRm77W4EgAAAKB2sn1Q2r59u6644grVq1dPUVFR6ty5s1auXGl1WdWqIKpw5rvw3L8srgQAAAConcKtLuBIDhw4oDPOOEN9+vTRRx99pAYNGmjDhg2qU6eO1aVVK19MYVBy5nFGCQAAALCCrYPSww8/rNTUVM2ZM8c/1qJFiyNu43a75Xa7/a8zMjIkSR6PRx6PtQ9wLT7+0eowoutJkiLz91teM6xV0Z4BitEzCBY9g2DRM6gMu/RNMMc3TNM0q7GWY9KhQwcNHDhQf/75pz777DM1btxYN910k6677rrDbjN16lRNmzatzPi8efMUHR1dneVWmewdv+jy3Q9pkxrrx5NnWF0OAAAAcFzIycnR5ZdfroMHDyo+Pv6I69o6KEVGRkqSJkyYoIsvvlgrVqzQLbfcomeffVYjR44sd5vyziilpqZq7969R/0wqpvH41FaWpr69+8vp9N52PV+/fFbdX5/qA4oXrF3/xHCCmE3Fe0ZoBg9g2DRMwgWPYPKsEvfZGRkqH79+hUKSra+9M7n86lr16568MEHJUknn3yyfvrppyMGJZfLJZfLVWbc6XTa5h/z0WpJbJgqSUowM+VwGFKYrf+aEAJ26l/UDPQMgkXPIFj0DCrD6r4J5ti2nvUuJSVFHTp0CBhr3769tm7dalFFoVGnfrK8piGHYSrnIM9SAgAAAELN1kHpjDPO0Pr16wPGfvvtNzVr1syiikIjJjJCB1R4KjDjrx0WVwMAAADUPrYOSrfeequ++eYbPfjgg9q4caPmzZun559/XmPGjLG6tGplGIbSHYmSpKwDu6wtBgAAAKiFbB2UunXrpvnz5+v1119Xp06ddN9992nmzJkaMWKE1aVVu6zwwmdFudN3WlwJAAAAUPvYfpaAc845R+ecc47VZYRcrrOu5JE8GXusLgUAAACodWx9Rqk2y4+sL0kyMwlKAAAAQKgRlGzKG9NAkhSWQ1ACAAAAQo2gZFdxyZIkVx5BCQAAAAg1gpJNhSc0kiTFuv+yuBIAAACg9iEo2VRk3caSpATvPosrAQAAAGofgpJNxdRLlSTFmtlSfrbF1QAAAAC1C0HJpurUqats0yVJMjN56CwAAAAQSgQlm6ob69Jus/Chs9n7/rS4GgAAAKB2ISjZVKQzTPuMupKk7L0EJQAAACCUCEo2djC8niTJfWC7xZUAAAAAtQtBycZyXIUPnS1I32FxJQAAAEDtQlCysfzopMJvMndaWwgAAABQyxCUbMyMTZEkhWfvtrgSAAAAoHYhKNmYM7GRJCnKvcfiSgAAAIDahaBkY5F1G0uS4j17JdO0uBoAAACg9iAo2Vh8g6aSJJfplnIPWFwNAAAAUHsQlGysQd0E/WXGF744uM3aYgAAAIBahKBkYw3jIrXdrC9Jyt+31eJqAAAAgNqDoGRj8VHh2qXCoJS9Z5PF1QAAAAC1B0HJxgzD0MGIZEmSe98Wi6sBAAAAag+Cks1lRxU+S8lM5x4lAAAAIFQISjbniW0iSQrP/NPiSgAAAIDag6Bkd4mFQSkqd6fFhQAAAAC1B0HJ5px1m0mSYj37JU+exdUAAAAAtQNByebq1GuoHNNV+CJju7XFAAAAALUEQcnmGiVG+5+lpHSepQQAAACEAkHJ5hrXifIHJR9BCQAAAAgJgpLNJcdHapsaSpJyd2+0uBoAAACgdiAo2Vx4mEMHXIUz37n3/G5xNQAAAEDtQFCqAXJim0qSwg78YXElAAAAQO1AUKoBzDotJUlRWVsl07S4GgAAAOD4R1CqAVwNW8hnGorwZkvZe60uBwAAADjuEZRqgOS6dbRD9Qpf7OfyOwAAAKC61aig9NBDD8kwDI0fP97qUkKqUWKktviSCl8QlAAAAIBqV2OC0ooVK/Tcc8/pxBNPtLqUkGtSJ0pbTIISAAAAECo1IihlZWVpxIgReuGFF1SnTh2rywm5RolR2lwUlPL/4llKAAAAQHULt7qAihgzZoyGDh2qfv366f777z/ium63W2632/86IyNDkuTxeOTxeKq1zqMpPn6wdTgNab+rieSTPHs2yLD4fSB0KtszqL3oGQSLnkGw6BlUhl36Jpjj2z4ovfHGG1q1apVWrFhRofVnzJihadOmlRlfvHixoqOjq7q8SklLSwt6m72OBpJPCt//mxZ++KFkGNVQGeyqMj2D2o2eQbDoGQSLnkFlWN03OTk5FV7X1kFp27ZtuuWWW5SWlqbIyMgKbTNp0iRNmDDB/zojI0OpqakaMGCA4uPjq6vUCvF4PEpLS1P//v3ldDqD2vbLvDXK/yVMLrk15MwTpYTUaqoSdnIsPYPaiZ5BsOgZBIueQWXYpW+KrzarCFsHpe+//1579uzRKaec4h/zer36/PPP9dRTT8ntdissLCxgG5fLJZfLVWZfTqfTNv+YK1NLi6Q6+uPnRmpnbJNz/0apfstqqg52ZKf+Rc1AzyBY9AyCRc+gMqzum2CObeug1LdvX61duzZgbPTo0WrXrp0mTpxYJiQdz1rUj9ZGs7HaaZv01zqpzQCrSwIAAACOW7YOSnFxcerUqVPAWExMjOrVq1dm/HjXon6sPvQ1kcIkc886cYcSAAAAUH1qxPTgkJrVi9YGNZEkeXets7gaAAAA4Phm6zNK5Vm2bJnVJVgi0hmm9JhWUr5k7Fsv+XySg5wLAAAAVAd+0q5BnA1OkNsMV1hBrnRwq9XlAAAAAMctglIN0qxBvH43Gxe+2PWTtcUAAAAAxzGCUg3SJjlOP/maF77YudrKUgAAAIDjGkGpBumQEqe1ZovCFztWW1oLAAAAcDwjKNUgbZPj9ZOvMCj5dqyWTNPaggAAAIDjFEGpBol1hSszsZ0KTIccOX9JmTutLgkAAAA4LhGUaphWjeprQ/GEDlx+BwAAAFQLglIN0z7l0OV3TOgAAAAAVA+CUg3TPiVeP5otC1/8ucLaYgAAAIDjFEGphumQEq+VvraSJHPbd5LPa3FFAAAAwPGHoFTDNKkTpf0xrZRhRsnIz5J2/2x1SQAAAMBxh6BUwxiGoZOb1dcqX5vCga3fWFsQAAAAcBwiKNVAXZvX0Yqiy++09WtriwEAAACOQwSlGuiUZnUO3ae09WsePAsAAABUMYJSDdSxUbzWhZ0gtxkuI3OntP8Pq0sCAAAAjisEpRrIFR6mtk0aHrpP6fdPrS0IAAAAOM4QlGqoM06ory98nQpf/LHM0loAAACA4w1BqYbq2aaBlvs6S5LMTZ9J3gKLKwIAAACOHwSlGqpLk0Rtc7VWuhkjw50pbf/e6pIAAACA4wZBqYYKcxjq0SZJy4svv9uYZm1BAAAAwHGEoFSD9WrTQJ96Ty588etCa4sBAAAAjiMEpRqsb7uG+kynqMB0SHt+lvZvsrokAAAA4LhAUKrB6sW61K5lM33na1c4sJ6zSgAAAEBVICjVcEM6pyjNd2rhCy6/AwAAAKoEQamGG9gxWZ/4ukqSzK1fSdn7LK4IAAAAqPkISjVc/ViXWrbuoJ99zWSYPum3j60uCQAAAKjxCErHgUu7pWqxt/Csku/n+RZXAwAAANR8BKXjQN/2Sfrc1avwxe+fSll/WVsQAAAAUMMRlI4DEeEOdT21m9b4WspheiXOKgEAAADHhKB0nLikW6r+5z1DkuRZ/YbF1QAAAAA1G0HpOHFCwzhtTRkkr2nIufN7af8fVpcEAAAA1FgEpePI8J6n6EtfJ0mSZ/VbFlcDAAAA1FwEpePIoI7JWhZ5tiTJveIVyee1uCIAAACgZiIoHUfCwxxq3OMSpZsxis3dLnPjJ1aXBAAAANRItg9KM2bMULdu3RQXF6eGDRtq+PDhWr9+vdVl2dZF/9dGC9RbkrR/2WxriwEAAABqKNsHpc8++0xjxozRN998o7S0NHk8Hg0YMEDZ2dlWl2ZLCVFOZXa8UpJUZ8cyKX2rtQUBAAAANVC41QUczccffxzweu7cuWrYsKG+//579ezZs8z6brdbbrfb/zojI0OS5PF45PF4qrfYoyg+fnXXMbhXDy3/qaPOdPysPUufVZ1zplXr8VB9QtUzOH7QMwgWPYNg0TOoDLv0TTDHN0zTNKuxliq3ceNGtW7dWmvXrlWnTp3KLJ86daqmTSsbDObNm6fo6OhQlGgLv/6yUhPdTypDsfr8xMflDYu0uiQAAADAUjk5Obr88st18OBBxcfHH3HdGhWUfD6fzj33XKWnp2v58uXlrlPeGaXU1FTt3bv3qB9GdfN4PEpLS1P//v3ldDqr9Vjrtqcr7qUz1MKxW3v+7x7V6Tu+Wo+H6hHKnsHxgZ5BsOgZBIueQWXYpW8yMjJUv379CgUl2196V9KYMWP0008/HTYkSZLL5ZLL5Soz7nQ6bfOPORS1nNi8gV5uOEIt9j4u18rZcva9WXJyVqmmslP/omagZxAsegbBomdQGVb3TTDHtv1kDsXGjh2rDz74QEuXLlWTJk2sLqdGOPmcG7XdrKeEgn3a/+m/rC4HAAAAqDFsH5RM09TYsWM1f/58ffrpp2rRooXVJdUYJzZvqI8aXCNJivpmppS529qCAAAAgBrC9kFpzJgxevXVVzVv3jzFxcVp165d2rVrl3Jzc60urUboffHNWuNrpSgzR7vfud3qcgAAAIAawfZBafbs2Tp48KB69+6tlJQU/9ebb75pdWk1wglJ8VrZYZIKTIeStryngjVvWV0SAAAAYHu2n8yhBk3KZ1sXnTdcL65fpL+bb8v73niFJ7WXkjtbXRYAAABgW7Y/o4RjlxDlVMqwyfrG114ub7Y8L58v/fWb1WUBAAAAtkVQqiXOPaWZ3m3ziNb5msqZ+5d8L5wt/bxA4owdAAAAUAZBqRa5+8LuujP2Pn3raydHfqb09kjp1QulnWusLg0AAACwFYJSLZIQ5dQ/R/XTDY4perrgXHkULv2+RHqup/TGCGn3z1aXCAAAANgCQamWOaFhrF68uof+HXGl+rkf0Yc6S6YM6dcPpNk9pLdHSX+tt7pMAAAAwFIEpVrolKZ19N8beygmuY3G5N2o/u6H9ZnzzMKFP88vDExf/kvy+awtFAAAALAIQamWatkgVgvGnKE7BrbV7ojmGpl5kwa5H9I34d0kX4GUNkV6Z7TkybO6VAAAACDkCEq1WES4Q2P6nKDlE8/WuL6ttT2ipS7NGq+7PNcU3r/0y4LCyR5y060uFQAAAAgpghKUEO3UhP5ttHzi2bqlbxu9Hz5QI/P/oSwzStqyXObcoVLWX1aXCQAAAIQMQQl+CdFO3dq/jdIm9FJYq976W/5k/WUmyNj9k7wvDpTSt1ldIgAAABASBCWUkZwQqZdHn6YLhgzW5QVT9adZX2EHflf+CwOkvRutLg8AAACodgQllMvhMHTtWS01c8yFujV6hn73pSgie4dyn+8vc9daq8sDAAAAqhVBCUfUsVGC/j3ufD3dYpZ+9jVTVP5+5T4/SHmbvrG6NAAAAKDaEJRwVAlRTv1zVD992/MVrfS1UbQvS+bL52nHt+9aXRoAAABQLQhKqBDDMHR1v5PkvfxdfW10UZTy1Oij0Vrz6iSZPq/V5QEAAABViqCEoJzeLlUtx32oxbHnSZK6bHxGPzx2rv7at9fiygAAAICqQ1BC0JLqxKnfhJf1eft7lW+G65Sc5cqadZaWLvtEpmlaXR4AAABwzAhKqBSHw1DPSyZo+/D/aq9RVy20Qz2WXqK3Zt2p3QdzrC4PAAAAOCYEJRyTFif3VsKE7/RHvZ5yGQW6ZP+z2vDEYH345SrOLgEAAKDGIijhmDnjGqjl2Pe0+6wH5VaEztRqnbV4iOY8ea9+23XQ6vIAAACAoBGUUDUMQ0l9xyjs78u0O66j4o1cXX3gX9r/zED9840PtSczz+oKAQAAgAojKKFKhad0VNKtX2j/mdPkNiL1f451unndlXrvsev01EerlOUusLpEAAAA4KgISqh6jjDV7TdernHfKb1xL0UYXl1rvKdLvhmufz50j57/bIOyCUwAAACwMYISqk+dZkq87j2Zl7+lrNjmamAc1L3mbHVfcpEmPvRPPf3pBmXmeayuEgAAACiDoIRqZ7QZqNjxK+Ttd5/yw2PV2bFZT5kPqvuySzX+wSf0j7dXa+Xm/cySBwAAANsgKCE0wiMUduY4RYxfLV/3sSoIi9Qpjo160fGARqwdrdeef0SDHvtEs5f9rl0HmfgBAAAA1gq3ugDUMrEN5Bj4gBw9xslc/oTMlS+pi/7QExGz9VfWa3rjk7M1YtEZim3SUQM7Jql3m4Zqlxwnh8OwunIAAADUIgQlWCMuScbgh2T0vENaNVe+7/6tBpk7dHP4At0cvkA/7W6u93Z0142Luik9MlWnt6irbs3r6oSkWJ3QIFaNEqMURngCAABANSEowVox9aSzbpOjxy3Srx9Iq+fJ/H2JOmmzOjk26y69ri3ehvrit8764tcT9ZyvjfYqQRHhDrWsH6NWDWLVqkGMWjaIVasGsWpcJ0p1op0yDEIUAAAAKo+gBHsIC5c6Dpc6DpeRvU9a9z/p5/kyt3ytZtqjZo4lukJLJEnbzIZa5TtBv/zVTBv3NNL/zMbaZjaUr+iWO1e4Q/ViIpQYHaE6MU4lRkUoMdqpOtGFfyZGRyjS6ZArPEwR4Q5FhDnkcjoUExGuGFeYYiLCC8fDHQp3GIQuAACAWoigBPuJqSd1vVrqerUMd5a0+Qtp4xJpy5fSnnVKNfYoNWyPzgv7yr9Jvpzarob601tHu8y62pldV7uy6mmfGa+9Zqx+V4zSzRgdVIxy5ZJUsfBjGFJEWGFocoYVBidnmEPhYUa5r51hhsIdhX8WjjvkdBgKcxgKL7G+M9xQRJhDYQ5DYYYhh8OQwzAU5pAcRvH3hkzTp3W7DWWt/FNOZ3jRuvIvDzMKg1yYo3BbwygcCyvan8NQ4ffF+y/a/tDyQ/sJDzP8wdBhHKrD8NdU+KdRYpnDEEESAAAclwhKsDdXrNR2cOGXJOUdlLavkv5cKf21TvrrN2nfBkUU5KmFtqtF2Paj7tIjp3Ic0XLLpTxFyK0I5SpCuT6nsnwRyjWdcitcXjNMXjnklUMFBWHyFThUoENjXoWpwDz0vVcO+eSQT0bRl0Nmie8LZMgjQ9mm4V/PlFFim8Nvu23zusLX5qF1zFLbmEXfe8vdd9Fys+T+JRWtZ0ql/jQOszzwexV9r6LAZhiGZDhkGJJhOOQo+j4waJUMYira7vBhzP+9QzJU+NqQCo9Z+EfhekXf+9cp9b2jKNAZRev6j128XTn7cBQdzCjazlHie/+f/rHC7RyOws+teNwRsI4RWFeJ+hVw3NLrlN2ueIIT/3GL3qfP59OvOwzt/mqLwsPCAvbhMCryuZX8TMr73Eq955KfRZn3U/JzM2TKVMmnABRn7IC/18KBMmMlP+tD25azTjn7LXm8kj1R3rFL7reolIrt9zDrlPeejrRflfjcAz7PEuv51y0zVrKmwN4GAASPoISaJTJBatWn8KuYzyulby38ytghZWyXMncWfp+9V8o9IOWlS7npks8jpzxK8B0sf/9MmF8tfL7yw5aOEMIOBbWjLT9KkDOPvLw4NAbut3DfkgLGDoXDkt8fGlM5Y+WtV3osYLlZ3nqH1im5Temx4vXayJC5o+y2OsL+AscCa1KJMe8RP5fDfH7m4der8s+vgtuq3PWO/LmU+fzMw31+gWM6zP4q/PkdYb2jfRYl1zNKrlccvorXM019tfKrwqXGofda+DrwP4yBvyQpuaRwPdM4tF8jYJ3ytjtUz6FtisaNwOUlCpL876c4CBbtu2g/hxJkifVUMiEf5s9y6gnYj1FyfyU/GyPgD8khGSXfz6HajaJ+KFG2/7Mrk8BV+v0Uvt+Sf5+G/zMp9TmW8/kVBuiiviix3Ch+H8V/d/7P0XHo3RuHPkefaWrnznStzlomh+Hw/yKg5HENQzKLty/xIRR+Xzwe+L789fonazq0fcn3ZfjrKqw14HMxDv0iQUXv91A9Zf/ejHI+h8BfaJSsT6WWG4dfv9Sykvvwd5VR8vuy+1K565ezXqllCqin7D4Ot6/D1Vjy/ZRU8pdmxZ9l8TGLx/3HMSRXDfwZq0YEpaefflqPPvqodu3apS5dumjWrFk67bTTrC4LduEIk+q2KPw6EtOU8rMLQ1PeQcmTJxXklviz6KsgTypwS6ZX8hVIPl/RnwVFY8Xj3hLjvkN/lvkyDzNeYrnPW+4y0+dVRsZBxcfFyjjsforHy9/HkY9f+GNitf8VGYd+XAu5sv9tB1CdSv8z51nix68NVhdQfXwlfmlV+KdR6s/AcR1m/Oi/YKvYtocfD2bd8n+5Iukwv6Sr4LYVWN+UtN+ZIp04VjWJ7YPSm2++qQkTJujZZ5/V6aefrpkzZ2rgwIFav369GjZsaHV5qEkMo/BSPleslNDE6moqpMDj0bKFCzVkyBA5nc7qPZhpHgpOJf80fWXHDvunKrheKPaj4LczfcUfRmANJYYCaix3PbOK1lMF1wvcpsBboLU/rlHnzicqPMwRsuMe23rl1VCdx1UF16uq4ypgPbPolxRmqfVMM3Bbs2h/hS99pfZb4vtSxzUDxkpc7miaAfsxi4/rM7Vv317Vq1uv8Lfupes/0utS48XvofBCy1L1SiX+janE5+KTKcko8TmZJT+3ktscrqbD9dBhlptFNVZoff9hy/8MjrifgPV05OOW3l95/z5KjBuHOZ7hf3+B6x9+/4fqO9z+AseLvjcPndcquV+j9OdWQx36BV+x4+N9HVYIfqm402Fob/UfpkrZPig9/vjjuu666zR69GhJ0rPPPqsPP/xQL730ku68806LqwOOIyVv3ECNZXo82ro9UZ1OGiJVd7hGpRil/rSax+PR96H6hQyOCx6PRwsr2jOHCdVV97poLKj1q6ueytRexbUEva2CXL/ytTQwIqQf/1JNYuuglJ+fr++//16TJk3yjzkcDvXr109ff/11udu43W653W7/64yMDEmF/6g9Hk/1FnwUxce3ug7UHPQMgkXPIFj0DIJVPT1zlF8hlB62y28aUGGF/ZJm+X9rgjm+rYPS3r175fV6lZSUFDCelJSkX3/9tdxtZsyYoWnTppUZX7x4saKjo6ulzmClpaVZXQJqGHoGwaJnECx6BsGiZ1AZVvdNTk5Ohde1dVCqjEmTJmnChAn+1xkZGUpNTdWAAQMUHx9vYWWFCTYtLU39+/fn8gZUCD2DYNEzCBY9g2DRM6gMu/RN8dVmFWHroFS/fn2FhYVp9+7dAeO7d+9WcnJyudu4XC65XK4y406n0zb/mO1UC2oGegbBomcQLHoGwaJnUBlW900wx7b1jOYRERE69dRTtWTJEv+Yz+fTkiVL1L17dwsrAwAAAHA8s/UZJUmaMGGCRo4cqa5du+q0007TzJkzlZ2d7Z8FDwAAAACqmu2D0iWXXKK//vpLU6ZM0a5du3TSSSfp448/LjPBAwAAAABUFdsHJUkaO3asxo6tWU/yBQAAAFBz2foeJQAAAACwAkEJAAAAAEohKAEAAABAKQQlAAAAACiFoAQAAAAApRCUAAAAAKAUghIAAAAAlFIjnqN0LEzTlCRlZGRYXInk8XiUk5OjjIwMOZ1Oq8tBDUDPIFj0DIJFzyBY9Awqwy59U5wJijPCkRz3QSkzM1OSlJqaanElAAAAAOwgMzNTCQkJR1zHMCsSp2own8+nHTt2KC4uToZhWFpLRkaGUlNTtW3bNsXHx1taC2oGegbBomcQLHoGwaJnUBl26RvTNJWZmalGjRrJ4TjyXUjH/Rklh8OhJk2aWF1GgPj4eP7DgqDQMwgWPYNg0TMIFj2DyrBD3xztTFIxJnMAAAAAgFIISgAAAABQCkEphFwul+699165XC6rS0ENQc8gWPQMgkXPIFj0DCqjJvbNcT+ZAwAAAAAEizNKAAAAAFAKQQkAAAAASiEoAQAAAEApBCUAAAAAKIWgFEJPP/20mjdvrsjISJ1++un67rvvrC4JFpgxY4a6deumuLg4NWzYUMOHD9f69esD1snLy9OYMWNUr149xcbG6sILL9Tu3bsD1tm6dauGDh2q6OhoNWzYUHfccYcKCgpC+VZgkYceekiGYWj8+PH+MXoGpW3fvl1XXHGF6tWrp6ioKHXu3FkrV670LzdNU1OmTFFKSoqioqLUr18/bdiwIWAf+/fv14gRIxQfH6/ExERdc801ysrKCvVbQQh4vV5NnjxZLVq0UFRUlFq1aqX77rtPJef8omfw+eefa9iwYWrUqJEMw9CCBQsClldVj/z4448666yzFBkZqdTUVD3yyCPV/dbKZyIk3njjDTMiIsJ86aWXzJ9//tm87rrrzMTERHP37t1Wl4YQGzhwoDlnzhzzp59+MlevXm0OGTLEbNq0qZmVleVf54YbbjBTU1PNJUuWmCtXrjT/7//+z+zRo4d/eUFBgdmpUyezX79+5g8//GAuXLjQrF+/vjlp0iQr3hJC6LvvvjObN29unnjiieYtt9ziH6dnUNL+/fvNZs2amaNGjTK//fZb848//jAXLVpkbty40b/OQw89ZCYkJJgLFiww16xZY5577rlmixYtzNzcXP86gwYNMrt06WJ+88035hdffGGecMIJ5mWXXWbFW0I1e+CBB8x69eqZH3zwgblp0ybz7bffNmNjY81//etf/nXoGSxcuNC8++67zXfffdeUZM6fPz9geVX0yMGDB82kpCRzxIgR5k8//WS+/vrrZlRUlPncc8+F6m36EZRC5LTTTjPHjBnjf+31es1GjRqZM2bMsLAq2MGePXtMSeZnn31mmqZppqenm06n03z77bf966xbt86UZH799demaRb+h8rhcJi7du3yrzN79mwzPj7edLvdoX0DCJnMzEyzdevWZlpamtmrVy9/UKJnUNrEiRPNM88887DLfT6fmZycbD766KP+sfT0dNPlcpmvv/66aZqm+csvv5iSzBUrVvjX+eijj0zDMMzt27dXX/GwxNChQ82rr746YOyCCy4wR4wYYZomPYOySgelquqRZ555xqxTp07A/5smTpxotm3btprfUVlcehcC+fn5+v7779WvXz//mMPhUL9+/fT1119bWBns4ODBg5KkunXrSpK+//57eTyegH5p166dmjZt6u+Xr7/+Wp07d1ZSUpJ/nYEDByojI0M///xzCKtHKI0ZM0ZDhw4N6A2JnkFZ7733nrp27aqLL75YDRs21Mknn6wXXnjBv3zTpk3atWtXQM8kJCTo9NNPD+iZxMREde3a1b9Ov3795HA49O2334buzSAkevTooSVLlui3336TJK1Zs0bLly/X4MGDJdEzOLqq6pGvv/5aPXv2VEREhH+dgQMHav369Tpw4ECI3k2h8JAerZbau3evvF5vwA8okpSUlKRff/3VoqpgBz6fT+PHj9cZZ5yhTp06SZJ27dqliIgIJSYmBqyblJSkXbt2+dcpr5+Kl+H488Ybb2jVqlVasWJFmWX0DEr7448/NHv2bE2YMEF33XWXVqxYoXHjxikiIkIjR470/52X1xMle6Zhw4YBy8PDw1W3bl165jh05513KiMjQ+3atVNYWJi8Xq8eeOABjRgxQpLoGRxVVfXIrl271KJFizL7KF5Wp06daqm/PAQlwEJjxozRTz/9pOXLl1tdCmxs27ZtuuWWW5SWlqbIyEiry0EN4PP51LVrVz344IOSpJNPPlk//fSTnn32WY0cOdLi6mBHb731ll577TXNmzdPHTt21OrVqzV+/Hg1atSInkGtxaV3IVC/fn2FhYWVmYFq9+7dSk5OtqgqWG3s2LH64IMPtHTpUjVp0sQ/npycrPz8fKWnpwesX7JfkpOTy+2n4mU4vnz//ffas2ePTjnlFIWHhys8PFyfffaZnnzySYWHhyspKYmeQYCUlBR16NAhYKx9+/baunWrpEN/50f6/1JycrL27NkTsLygoED79++nZ45Dd9xxh+68805deuml6ty5s6688krdeuutmjFjhiR6BkdXVT1ip/9fEZRCICIiQqeeeqqWLFniH/P5fFqyZIm6d+9uYWWwgmmaGjt2rObPn69PP/20zOnlU089VU6nM6Bf1q9fr61bt/r7pXv37lq7dm3Af2zS0tIUHx9f5ocj1Hx9+/bV2rVrtXr1av9X165dNWLECP/39AxKOuOMM8o8duC3335Ts2bNJEktWrRQcnJyQM9kZGTo22+/DeiZ9PR0ff/99/51Pv30U/l8Pp1++ukheBcIpZycHDkcgT8WhoWFyefzSaJncHRV1SPdu3fX559/Lo/H418nLS1Nbdu2Delld5KYHjxU3njjDdPlcplz5841f/nlF/P66683ExMTA2agQu1w4403mgkJCeayZcvMnTt3+r9ycnL869xwww1m06ZNzU8//dRcuXKl2b17d7N79+7+5cVTPQ8YMMBcvXq1+fHHH5sNGjRgqudapOSsd6ZJzyDQd999Z4aHh5sPPPCAuWHDBvO1114zo6OjzVdffdW/zkMPPWQmJiaa//vf/8wff/zRPO+888qdxvfkk082v/32W3P58uVm69atmer5ODVy5EizcePG/unB3333XbN+/frmP/7xD/869AwyMzPNH374wfzhhx9MSebjjz9u/vDDD+aWLVtM06yaHklPTzeTkpLMK6+80vzpp5/MN954w4yOjmZ68OPdrFmzzKZNm5oRERHmaaedZn7zzTdWlwQLSCr3a86cOf51cnNzzZtuusmsU6eOGR0dbZ5//vnmzp07A/azefNmc/DgwWZUVJRZv35987bbbjM9Hk+I3w2sUjoo0TMo7f333zc7depkulwus127dubzzz8fsNzn85mTJ082k5KSTJfLZfbt29dcv359wDr79u0zL7vsMjM2NtaMj483R48ebWZmZobybSBEMjIyzFtuucVs2rSpGRkZabZs2dK8++67A6ZopmewdOnScn+GGTlypGmaVdcja9asMc8880zT5XKZjRs3Nh966KFQvcUAhmmWeOQyAAAAAIB7lAAAAACgNIISAAAAAJRCUAIAAACAUghKAAAAAFAKQQkAAAAASiEoAQAAAEApBCUAAAAAKIWgBAAAAAClEJQAACjBMAwtWLDA6jIAABYjKAEAbGPUqFEyDKPM16BBg6wuDQBQy4RbXQAAACUNGjRIc+bMCRhzuVwWVQMAqK04owQAsBWXy6Xk5OSArzp16kgqvCxu9uzZGjx4sKKiotSyZUu98847AduvXbtWZ599tqKiolSvXj1df/31ysrKCljnpZdeUseOHeVyuZSSkqKxY8cGLN+7d6/OP/98RUdHq3Xr1nrvvff8yw4cOKARI0aoQYMGioqKUuvWrcsEOwBAzUdQAgDUKJMnT9aFF16oNWvWaMSIEbr00ku1bt06SVJ2drYGDhyoOnXqaMWKFXr77bf1ySefBASh2bNna8yYMbr++uu1du1avffeezrhhBMCjjFt2jT97W9/048//qghQ4ZoxIgR2r9/v//4v/zyiz766COtW7dOs2fPVv369UP3AQAAQsIwTdO0uggAAKTCe5ReffVVRUZGBozfdddduuuuu2QYhm644QbNnj3bv+z//u//dMopp+iZZ57RCy+8oIkTJ2rbtm2KiYmRJC1cuFDDhg3Tjh07lJSUpMaNG2v06NG6//77y63BMAzdc889uu+++yQVhq/Y2Fh99NFHGjRokM4991zVr19fL730UjV9CgAAO+AeJQCArfTp0ycgCElS3bp1/d937949YFn37t21evVqSdK6devUpUsXf0iSpDPOOEM+n0/r16+XYRjasWOH+vbte8QaTjzxRP/3MTExio+P1549eyRJN954oy688EKtWrVKAwYM0PDhw9WjR49KvVcAgH0RlAAAthITE1PmUriqEhUVVaH1nE5nwGvDMOTz+SRJgwcP1pYtW7Rw4UKlpaWpb9++GjNmjB577LEqrxcAYB3uUQIA1CjffPNNmdft27eXJLVv315r1qxRdna2f/mXX34ph8Ohtm3bKi4uTs2bN9eSJUuOqYYGDRpo5MiRevXVVzVz5kw9//zzx7Q/AID9cEYJAGArbrdbu3btChgLDw/3T5jw9ttvq2vXrjrzzDP12muv6bvvvtOLL74oSRoxYoTuvfdejRw5UlOnTtVff/2lm2++WVdeeaWSkpIkSVOnTtUNN9yghg0bavDgwcrMzNSXX36pm2++uUL1TZkyRaeeeqo6duwot9utDz74wB/UAADHD4ISAMBWPv74Y6WkpASMtW3bVr/++qukwhnp3njjDd10001KSUnR66+/rg4dOkiSoqOjtWjRIt1yyy3q1q2boqOjdeGFF+rxxx/372vkyJHKy8vTE088odtvv13169fXRRddVOH6IiIiNGnSJG3evFlRUVE666yz9MYbb1TBOwcA2Amz3gEAagzDMDR//nwNHz7c6lIAAMc57lECAAAAgFIISgAAAABQCvcoAQBqDK4WBwCECmeUAAAAAKAUghIAAAAAlEJQAgAAAIBSCEoAAAAAUApBCQAAAABKISgBAAAAQCkEJQAAAAAohaAEAAAAAKX8P4VV8k1WqzJwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting validation and training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model_history.history['loss'], label='Training Loss')\n",
    "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c866f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting predictions vs true values\n",
    "predictions = model.predict(X_valid, batch_size = len(X_valid))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(indices_valid[0:1000], y_valid[0:1000], label='True Values', color='blue', alpha=0.5)\n",
    "plt.plot(indices_valid[0:1000], predictions[0:1000], label='Predictions', color='red', alpha=0.5)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Water Level')\n",
    "plt.title('Water Level Predictions vs True Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
