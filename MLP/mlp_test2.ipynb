{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12acdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 09:19:25.639509: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78411be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM limited to 1500 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 09:19:27.360531: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-30 09:19:29.762234: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-30 09:19:29.762306: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=10000)]  # MB\n",
    "        )\n",
    "        print(\"VRAM limited to 1500 MB.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory configuration must be set at program start:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16394be",
   "metadata": {},
   "source": [
    "INPUT PL OUTPUT PO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea7daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Drop last 6 rows\n",
    "    df = df.iloc[:-6]\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df['#date+time'] = pd.to_datetime(df['#date+time'], errors='coerce')\n",
    "    df = df.rename(columns={'#date+time': 'date_time'})\n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != 'date_time':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "def wind_to_uv(speed, direction_deg):\n",
    "    # Convert to radians\n",
    "    direction_rad = np.deg2rad(direction_deg)\n",
    "\n",
    "    # U = -speed * sin(direction), V = -speed * cos(direction)\n",
    "    # This converts FROM meteorological TO Cartesian\n",
    "    u = -speed * np.sin(direction_rad)\n",
    "    v = -speed * np.cos(direction_rad)\n",
    "    return u, v\n",
    "\n",
    "def prepare_dataframe(df):\n",
    "    # Rename columns to a consistent format\n",
    "    df = df.rename(columns={\n",
    "        df.columns[0]: 'date_time',\n",
    "        df.columns[1]: 'pwl',\n",
    "        df.columns[2]: 'wsd',\n",
    "        df.columns[3]: 'wdr'\n",
    "    })\n",
    "    \n",
    "    # Convert wind to U/V components\n",
    "    u, v = wind_to_uv(df['wsd'], df['wdr'])\n",
    "    df[['u', 'v']] = np.column_stack((u, v))\n",
    "    \n",
    "    # Drop raw wind columns\n",
    "    df.drop(columns=['wsd', 'wdr'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_central_frequency_percentage(y_true, y_pred, threshold_cm):\n",
    "    \"\"\"Percentage of predictions within Â±threshold_cm of truth\"\"\"\n",
    "    y_true = np.asarray(y_true).flatten()\n",
    "    y_pred = np.asarray(y_pred).flatten()\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    within_threshold = diff <= (threshold_cm / 100.0)  # convert cm to meters\n",
    "    return np.mean(within_threshold) * 100  # percent\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, y_mean=None, y_std=None, config=None, results_df=None):\n",
    "\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    loss, mae = model.evaluate(X_test, y_test, batch_size=len(X_test), verbose=0)\n",
    "    print(f\"Raw Loss: {loss:.6f}, Raw MAE: {mae:.6f}\")\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test, batch_size=len(X_test), verbose=0)\n",
    "\n",
    "    y_test_unscaled = y_test\n",
    "    y_pred_unscaled = y_pred\n",
    "\n",
    "    # Metrics\n",
    "    cf_15cm = calculate_central_frequency_percentage(y_test_unscaled, y_pred_unscaled, 15)\n",
    "    cf_5cm = calculate_central_frequency_percentage(y_test_unscaled, y_pred_unscaled, 5)\n",
    "    cf_1cm = calculate_central_frequency_percentage(y_test_unscaled, y_pred_unscaled, 1)\n",
    "    mse = mean_squared_error(y_test_unscaled.flatten(), y_pred_unscaled.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_unscaled.flatten(), y_pred_unscaled.flatten())\n",
    "    medae = median_absolute_error(y_test_unscaled.flatten(), y_pred_unscaled.flatten())\n",
    "    r2 = r2_score(y_test_unscaled.flatten(), y_pred_unscaled.flatten())\n",
    "\n",
    "    # Report\n",
    "    print(f\"\\nðŸ“Š Evaluation Metrics (unscaled to meters):\")\n",
    "    print(f\"Central Frequency (Â±15cm): {cf_15cm:.2f}%\")\n",
    "    print(f\"Central Frequency (Â±5cm):  {cf_5cm:.2f}%\")\n",
    "    print(f\"Central Frequency (Â±1cm):  {cf_1cm:.2f}%\")\n",
    "    print(f\"MAE:   {mae:.4f} m\")\n",
    "    print(f\"RMSE:  {rmse:.4f} m\")\n",
    "    print(f\"MSE:   {mse:.6f}\")\n",
    "    print(f\"MedAE: {medae:.4f} m\")\n",
    "    print(f\"RÂ²:    {r2:.4f}\")\n",
    "    if config is not None:\n",
    "        row = {\n",
    "            'Inputs': config.get('Inputs', X_test.shape[1]),\n",
    "            'WL HR': config.get('WL_HR', '-'),\n",
    "            'WIND HR': config.get('WIND_HR', '-'),\n",
    "            'Layer 1': config.get('Layer_1', '-'),\n",
    "            'Layer 2': config.get('Layer_2', '-'),\n",
    "            'Dropout': config.get('Dropout', '-'),\n",
    "            'Epochs': config.get('Epochs', '-'),\n",
    "            'Batch Size': config.get('Batch_Size', '-'),\n",
    "            'R2': r2,\n",
    "            'CF 1cm': cf_1cm,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MSE': mse\n",
    "        }\n",
    "\n",
    "        if results_df is None:\n",
    "            results_df = pd.DataFrame(columns=row.keys())\n",
    "\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    return None\n",
    "\n",
    "file_paths = {\n",
    "    'po11': '/home/ryan/Downloads/po_2011-2012.csv',\n",
    "    'pl11': '/home/ryan/Downloads/pl_2011-2012.csv',\n",
    "    'po13': '/home/ryan/Downloads/po_2013-2014.csv',\n",
    "    'pl13': '/home/ryan/Downloads/pl_2013-2014.csv',\n",
    "    'po15': '/home/ryan/Downloads/po_2015-2016.csv',\n",
    "    'pl15': '/home/ryan/Downloads/pl_2015-2016.csv',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730399ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412063/1169060759.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "/tmp/ipykernel_412063/1169060759.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "datasets = {key: load_and_clean_csv(path) for key, path in file_paths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c9ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor df in datasets.values():\\n    for col in df.columns:\\n        if col != 'date_time':\\n            df[col] = df[col].interpolate(limit_direction='both')\\n\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for name, df in datasets.items():\n",
    "    datasets[name] = prepare_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b37be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['pl11', 'pl13', 'pl15']:\n",
    "    df = datasets[key]\n",
    "    hourly_df = df[df['date_time'].dt.minute == 0].copy()\n",
    "    datasets[f'{key}_uv_hourly'] = hourly_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88153ec5",
   "metadata": {},
   "source": [
    "only pwl, 1hr before and after point of PWL, and only past winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ae44d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def   input_output_arrays(pl_df, po_df, wl_half_window=5, wind_window=30, wind_step=10):\\n    \"\"\"\\n    Predict po.pwl[t] using:\\n    - pl.pwl[t-5:t+6] â†’ 11 values (past, current, future)\\n    - pl.u, pl.v from t-30 to t (3 hourly steps)\\n    \"\"\"\\n    pl_df = pl_df.reset_index(drop=True)\\n    po_df = po_df.reset_index(drop=True)\\n\\n    pl_pwl = pl_df[\\'pwl\\'].to_numpy()\\n    pl_u   = pl_df[\\'u\\'].to_numpy()\\n    pl_v   = pl_df[\\'v\\'].to_numpy()\\n    po_pwl = po_df[\\'pwl\\'].to_numpy()\\n\\n    X, y, indices = [], [], []\\n\\n    for t in range(wind_window, len(pl_df) - wl_half_window):\\n        # Water level: centered window [t-5 : t+5]\\n        wl_slice = slice(t - wl_half_window, t + wl_half_window + 1)\\n        # Wind: hourly up to t\\n        wind_slice = slice(t - wind_window + 1, t + 1, wind_step)\\n\\n        pwl_input = pl_pwl[wl_slice]     # 11\\n        u_input = pl_u[wind_slice]       # 3\\n        v_input = pl_v[wind_slice]       # 3\\n        target = po_pwl[t]\\n\\n        if (\\n            np.isnan(pwl_input).any() or\\n            np.isnan(u_input).any() or\\n            np.isnan(v_input).any() or\\n            np.isnan(target)\\n        ):\\n            continue\\n\\n        features = np.concatenate([pwl_input, u_input, v_input])\\n        X.append(features)\\n        y.append(target)\\n        indices.append(t)\\n\\n    return np.array(X), np.array(y), np.array(indices)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def   input_output_arrays(pl_df, po_df, wl_half_window=5, wind_window=30, wind_step=10):\n",
    "    \"\"\"\n",
    "    Predict po.pwl[t] using:\n",
    "    - pl.pwl[t-5:t+6] â†’ 11 values (past, current, future)\n",
    "    - pl.u, pl.v from t-30 to t (3 hourly steps)\n",
    "    \"\"\"\n",
    "    pl_df = pl_df.reset_index(drop=True)\n",
    "    po_df = po_df.reset_index(drop=True)\n",
    "\n",
    "    pl_pwl = pl_df['pwl'].to_numpy()\n",
    "    pl_u   = pl_df['u'].to_numpy()\n",
    "    pl_v   = pl_df['v'].to_numpy()\n",
    "    po_pwl = po_df['pwl'].to_numpy()\n",
    "\n",
    "    X, y, indices = [], [], []\n",
    "\n",
    "    for t in range(wind_window, len(pl_df) - wl_half_window):\n",
    "        # Water level: centered window [t-5 : t+5]\n",
    "        wl_slice = slice(t - wl_half_window, t + wl_half_window + 1)\n",
    "        # Wind: hourly up to t\n",
    "        wind_slice = slice(t - wind_window + 1, t + 1, wind_step)\n",
    "\n",
    "        pwl_input = pl_pwl[wl_slice]     # 11\n",
    "        u_input = pl_u[wind_slice]       # 3\n",
    "        v_input = pl_v[wind_slice]       # 3\n",
    "        target = po_pwl[t]\n",
    "\n",
    "        if (\n",
    "            np.isnan(pwl_input).any() or\n",
    "            np.isnan(u_input).any() or\n",
    "            np.isnan(v_input).any() or\n",
    "            np.isnan(target)\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        features = np.concatenate([pwl_input, u_input, v_input])\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "        indices.append(t)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(indices)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5517a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def input_output_arrays(pl_df, po_df, po_hourly_wind_df, wl_half_window=5, wind_window=3):\n",
    "    \"\"\"\n",
    "    Predict po.pwl[t] using:\n",
    "    - pl.pwl[t-5:t+6] â†’ 11 values (past, current, future)\n",
    "    - hourly u/v from po station (po_hourly_wind_df): 3 hourly values ending at time t\n",
    "    \"\"\"\n",
    "    pl_df = pl_df.reset_index(drop=True)\n",
    "    po_df = po_df.reset_index(drop=True)\n",
    "    po_hourly_wind_df = po_hourly_wind_df.reset_index(drop=True)\n",
    "\n",
    "    pl_pwl = pl_df['pwl'].to_numpy()\n",
    "    po_pwl = po_df['pwl'].to_numpy()\n",
    "\n",
    "    # Create lookup map from datetime to [u, v] for PO station\n",
    "    hourly_wind_map = po_hourly_wind_df.set_index('date_time')[['u', 'v']]\n",
    "    available_wind_times = set(hourly_wind_map.index)\n",
    "\n",
    "    X, y, indices = [], [], []\n",
    "    datetimes = pl_df['date_time'].values\n",
    "\n",
    "    for t in range(wl_half_window, len(pl_df) - wl_half_window):\n",
    "        curr_time = datetimes[t]\n",
    "\n",
    "        # Water level input from PL station (centered at t)\n",
    "        pwl_input = pl_pwl[t - wl_half_window: t + wl_half_window + 1]\n",
    "\n",
    "        # Wind input from PO station, past 3 hours\n",
    "        hourly_times = [(curr_time - pd.Timedelta(hours=i)).replace(minute=0, second=0, microsecond=0)\n",
    "                        for i in reversed(range(wind_window))]\n",
    "\n",
    "        if not all(ts in available_wind_times for ts in hourly_times):\n",
    "            continue  # skip if any hourly wind value is missing\n",
    "\n",
    "        uv_values = np.concatenate([hourly_wind_map.loc[ts].values for ts in hourly_times])\n",
    "        target = po_pwl[t]\n",
    "\n",
    "        if np.isnan(pwl_input).any() or np.isnan(uv_values).any() or np.isnan(target):\n",
    "            continue\n",
    "\n",
    "        features = np.concatenate([pwl_input, uv_values])\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "        indices.append(t)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5c27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_last_wl_hr = None\n",
    "_last_wind_hr = None\n",
    "\n",
    "_cached_data = {}\n",
    "\n",
    "def model_config(l1, dropout, epochs, wl_hr, wind_hr, results_df=None):\n",
    "    global _last_wl_hr, _last_wind_hr, _cached_data\n",
    "\n",
    "    wl_half_window = wl_hr * 10  # 6-min steps â†’ 10 per hour\n",
    "    wind_window = wind_hr        # hourly values, already filtered\n",
    "\n",
    "    # Only recompute if window sizes changed\n",
    "    if wl_hr != _last_wl_hr or wind_hr != _last_wind_hr:\n",
    "        print(\"âš™ï¸ Recomputing input/output arrays for new WL_HR and WIND_HR settings...\")\n",
    "\n",
    "        _cached_data['X_train'], _cached_data['y_train'], _cached_data['idx_train'] = input_output_arrays(\n",
    "            datasets['pl11'], datasets['po11'], datasets['pl11_uv_hourly'],\n",
    "            wl_half_window=wl_half_window, wind_window=wind_window\n",
    "        )\n",
    "        _cached_data['X_valid'], _cached_data['y_valid'], _cached_data['idx_valid'] = input_output_arrays(\n",
    "            datasets['pl13'], datasets['po13'], datasets['pl13_uv_hourly'],\n",
    "            wl_half_window=wl_half_window, wind_window=wind_window\n",
    "        )\n",
    "        _cached_data['X_test'], _cached_data['y_test'], _cached_data['idx_test'] = input_output_arrays(\n",
    "            datasets['pl15'], datasets['po15'], datasets['pl15_uv_hourly'],\n",
    "            wl_half_window=wl_half_window, wind_window=wind_window\n",
    "        )\n",
    "\n",
    "        _last_wl_hr = wl_hr\n",
    "        _last_wind_hr = wind_hr\n",
    "\n",
    "    # Unpack cached data\n",
    "    X_train = _cached_data['X_train']\n",
    "    y_train = _cached_data['y_train']\n",
    "    X_valid = _cached_data['X_valid']\n",
    "    y_valid = _cached_data['y_valid']\n",
    "    X_test  = _cached_data['X_test']\n",
    "    y_test  = _cached_data['y_test']\n",
    "\n",
    "    model_file_name = 'model2.keras'\n",
    "    BATCH_SIZE = len(X_train)\n",
    "    inputs = X_train.shape[1]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(inputs,)),\n",
    "        tf.keras.layers.Dense(l1, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose=1)\n",
    "    checkpoint = ModelCheckpoint(model_file_name, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_batch_size=len(X_valid),\n",
    "        callbacks=[early_stop, checkpoint]\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        'Inputs': inputs,\n",
    "        'WL_HR': wl_hr,\n",
    "        'WIND_HR': wind_hr,\n",
    "        'Layer_1': l1,\n",
    "        'Layer_2': 0,\n",
    "        'Dropout': dropout,\n",
    "        'Epochs': epochs,\n",
    "        'Batch_Size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "    y_mean = np.mean(y_train)\n",
    "    y_std = np.std(y_train)\n",
    "\n",
    "    results_df = evaluate_model(\n",
    "        model, X_test, y_test,\n",
    "        y_mean=y_mean, y_std=y_std,\n",
    "        config=config,\n",
    "        results_df=results_df\n",
    "    )\n",
    "\n",
    "    return model, results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef1a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "328cdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by='R2', ascending=False, inplace=True)\n",
    "results_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4c375c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, idx_train = input_output_arrays(\n",
    "    datasets['pl11'], datasets['po11'], datasets['pl11_uv_hourly'],\n",
    "    wl_half_window=30, wind_window=12\n",
    ")\n",
    "\n",
    "X_valid, y_valid, idx_valid = input_output_arrays(\n",
    "    datasets['pl13'], datasets['po13'], datasets['pl13_uv_hourly'],\n",
    "    wl_half_window=30, wind_window=12\n",
    ")\n",
    "\n",
    "X_test, y_test, idx_test = input_output_arrays(\n",
    "    datasets['pl15'], datasets['po15'], datasets['pl15_uv_hourly'],\n",
    "    wl_half_window=30, wind_window=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1750f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - loss: 12.3347 - mae: 3.5038\n",
      "Epoch 1: val_loss improved from inf to 10.30732, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 12.3347 - mae: 3.5038 - val_loss: 10.3073 - val_mae: 3.2000\n",
      "Epoch 2/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 10.3473 - mae: 3.2072\n",
      "Epoch 2: val_loss improved from 10.30732 to 8.56034, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 10.3473 - mae: 3.2072 - val_loss: 8.5603 - val_mae: 2.9122\n",
      "Epoch 3/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 8.5585 - mae: 2.9123\n",
      "Epoch 3: val_loss improved from 8.56034 to 6.99557, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 8.5585 - mae: 2.9123 - val_loss: 6.9956 - val_mae: 2.6256\n",
      "Epoch 4/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 6.9544 - mae: 2.6174\n",
      "Epoch 4: val_loss improved from 6.99557 to 5.59337, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 6.9544 - mae: 2.6174 - val_loss: 5.5934 - val_mae: 2.3365\n",
      "Epoch 5/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 5.5196 - mae: 2.3194\n",
      "Epoch 5: val_loss improved from 5.59337 to 4.34522, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 5.5196 - mae: 2.3194 - val_loss: 4.3452 - val_mae: 2.0416\n",
      "Epoch 6/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 4.2483 - mae: 2.0150\n",
      "Epoch 6: val_loss improved from 4.34522 to 3.25604, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 4.2483 - mae: 2.0150 - val_loss: 3.2560 - val_mae: 1.7390\n",
      "Epoch 7/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 3.1468 - mae: 1.7034\n",
      "Epoch 7: val_loss improved from 3.25604 to 2.33872, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 3.1468 - mae: 1.7034 - val_loss: 2.3387 - val_mae: 1.4301\n",
      "Epoch 8/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 2.2297 - mae: 1.3912\n",
      "Epoch 8: val_loss improved from 2.33872 to 1.60820, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2.2297 - mae: 1.3912 - val_loss: 1.6082 - val_mae: 1.1400\n",
      "Epoch 9/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.5119 - mae: 1.0982\n",
      "Epoch 9: val_loss improved from 1.60820 to 1.07975, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1.5119 - mae: 1.0982 - val_loss: 1.0797 - val_mae: 0.9049\n",
      "Epoch 10/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.0120 - mae: 0.8606\n",
      "Epoch 10: val_loss improved from 1.07975 to 0.76377, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1.0120 - mae: 0.8606 - val_loss: 0.7638 - val_mae: 0.7375\n",
      "Epoch 11/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.7413 - mae: 0.7112\n",
      "Epoch 11: val_loss improved from 0.76377 to 0.65521, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.7413 - mae: 0.7112 - val_loss: 0.6552 - val_mae: 0.6667\n",
      "Epoch 12/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.6910 - mae: 0.6678\n",
      "Epoch 12: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.6910 - mae: 0.6678 - val_loss: 0.7183 - val_mae: 0.6825\n",
      "Epoch 13/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.8154 - mae: 0.7081\n",
      "Epoch 13: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.8154 - mae: 0.7081 - val_loss: 0.8798 - val_mae: 0.7421\n",
      "Epoch 14/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.0260 - mae: 0.7878\n",
      "Epoch 14: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.0260 - mae: 0.7878 - val_loss: 1.0511 - val_mae: 0.8095\n",
      "Epoch 15/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.2208 - mae: 0.8641\n",
      "Epoch 15: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.2208 - mae: 0.8641 - val_loss: 1.1647 - val_mae: 0.8557\n",
      "Epoch 16/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.3282 - mae: 0.9093\n",
      "Epoch 16: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.3282 - mae: 0.9093 - val_loss: 1.1927 - val_mae: 0.8680\n",
      "Epoch 17/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.3265 - mae: 0.9132\n",
      "Epoch 17: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.3265 - mae: 0.9132 - val_loss: 1.1415 - val_mae: 0.8478\n",
      "Epoch 18/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.2304 - mae: 0.8780\n",
      "Epoch 18: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.2304 - mae: 0.8780 - val_loss: 1.0365 - val_mae: 0.8039\n",
      "Epoch 19/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0773 - mae: 0.8149\n",
      "Epoch 19: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 1.0773 - mae: 0.8149 - val_loss: 0.9070 - val_mae: 0.7483\n",
      "Epoch 20/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.9049 - mae: 0.7382\n",
      "Epoch 20: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.9049 - mae: 0.7382 - val_loss: 0.7788 - val_mae: 0.6919\n",
      "Epoch 21/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.7432 - mae: 0.6620\n",
      "Epoch 21: val_loss did not improve from 0.65521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.7432 - mae: 0.6620 - val_loss: 0.6695 - val_mae: 0.6440\n",
      "Epoch 22/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.6119 - mae: 0.5988\n",
      "Epoch 22: val_loss improved from 0.65521 to 0.58817, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.6119 - mae: 0.5988 - val_loss: 0.5882 - val_mae: 0.6094\n",
      "Epoch 23/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5196 - mae: 0.5567\n",
      "Epoch 23: val_loss improved from 0.58817 to 0.53632, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.5196 - mae: 0.5567 - val_loss: 0.5363 - val_mae: 0.5902\n",
      "Epoch 24/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4660 - mae: 0.5374\n",
      "Epoch 24: val_loss improved from 0.53632 to 0.51016, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.4660 - mae: 0.5374 - val_loss: 0.5102 - val_mae: 0.5853\n",
      "Epoch 25/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.4438 - mae: 0.5366\n",
      "Epoch 25: val_loss improved from 0.51016 to 0.50262, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.4438 - mae: 0.5366 - val_loss: 0.5026 - val_mae: 0.5899\n",
      "Epoch 26/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4441 - mae: 0.5481\n",
      "Epoch 26: val_loss did not improve from 0.50262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.4441 - mae: 0.5481 - val_loss: 0.5056 - val_mae: 0.5992\n",
      "Epoch 27/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.4574 - mae: 0.5653\n",
      "Epoch 27: val_loss did not improve from 0.50262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.4574 - mae: 0.5653 - val_loss: 0.5115 - val_mae: 0.6079\n",
      "Epoch 28/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4733 - mae: 0.5813\n",
      "Epoch 28: val_loss did not improve from 0.50262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.4733 - mae: 0.5813 - val_loss: 0.5139 - val_mae: 0.6126\n",
      "Epoch 29/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4843 - mae: 0.5923\n",
      "Epoch 29: val_loss did not improve from 0.50262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.4843 - mae: 0.5923 - val_loss: 0.5085 - val_mae: 0.6109\n",
      "Epoch 30/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.4856 - mae: 0.5953\n",
      "Epoch 30: val_loss improved from 0.50262 to 0.49313, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.4856 - mae: 0.5953 - val_loss: 0.4931 - val_mae: 0.6016\n",
      "Epoch 31/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4754 - mae: 0.5897\n",
      "Epoch 31: val_loss improved from 0.49313 to 0.46731, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.4754 - mae: 0.5897 - val_loss: 0.4673 - val_mae: 0.5844\n",
      "Epoch 32/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4530 - mae: 0.5745\n",
      "Epoch 32: val_loss improved from 0.46731 to 0.43254, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.4530 - mae: 0.5745 - val_loss: 0.4325 - val_mae: 0.5602\n",
      "Epoch 33/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4198 - mae: 0.5506\n",
      "Epoch 33: val_loss improved from 0.43254 to 0.39167, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.4198 - mae: 0.5506 - val_loss: 0.3917 - val_mae: 0.5306\n",
      "Epoch 34/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3797 - mae: 0.5200\n",
      "Epoch 34: val_loss improved from 0.39167 to 0.34861, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.3797 - mae: 0.5200 - val_loss: 0.3486 - val_mae: 0.4979\n",
      "Epoch 35/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3365 - mae: 0.4843\n",
      "Epoch 35: val_loss improved from 0.34861 to 0.30768, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.3365 - mae: 0.4843 - val_loss: 0.3077 - val_mae: 0.4652\n",
      "Epoch 36/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2959 - mae: 0.4481\n",
      "Epoch 36: val_loss improved from 0.30768 to 0.27319, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.2959 - mae: 0.4481 - val_loss: 0.2732 - val_mae: 0.4356\n",
      "Epoch 37/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2617 - mae: 0.4142\n",
      "Epoch 37: val_loss improved from 0.27319 to 0.24854, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.2617 - mae: 0.4142 - val_loss: 0.2485 - val_mae: 0.4128\n",
      "Epoch 38/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2385 - mae: 0.3883\n",
      "Epoch 38: val_loss improved from 0.24854 to 0.23532, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.2385 - mae: 0.3883 - val_loss: 0.2353 - val_mae: 0.3987\n",
      "Epoch 39/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2278 - mae: 0.3733\n",
      "Epoch 39: val_loss improved from 0.23532 to 0.23276, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.2278 - mae: 0.3733 - val_loss: 0.2328 - val_mae: 0.3933\n",
      "Epoch 40/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2286 - mae: 0.3701\n",
      "Epoch 40: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.2286 - mae: 0.3701 - val_loss: 0.2377 - val_mae: 0.3937\n",
      "Epoch 41/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2371 - mae: 0.3750\n",
      "Epoch 41: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2371 - mae: 0.3750 - val_loss: 0.2452 - val_mae: 0.3964\n",
      "Epoch 42/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2478 - mae: 0.3833\n",
      "Epoch 42: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2478 - mae: 0.3833 - val_loss: 0.2506 - val_mae: 0.3980\n",
      "Epoch 43/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2542 - mae: 0.3884\n",
      "Epoch 43: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.2542 - mae: 0.3884 - val_loss: 0.2507 - val_mae: 0.3961\n",
      "Epoch 44/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2536 - mae: 0.3884\n",
      "Epoch 44: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.2536 - mae: 0.3884 - val_loss: 0.2444 - val_mae: 0.3900\n",
      "Epoch 45/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2443 - mae: 0.3814\n",
      "Epoch 45: val_loss did not improve from 0.23276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.2443 - mae: 0.3814 - val_loss: 0.2334 - val_mae: 0.3804\n",
      "Epoch 46/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2287 - mae: 0.3690\n",
      "Epoch 46: val_loss improved from 0.23276 to 0.22040, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.2287 - mae: 0.3690 - val_loss: 0.2204 - val_mae: 0.3692\n",
      "Epoch 47/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2105 - mae: 0.3539\n",
      "Epoch 47: val_loss improved from 0.22040 to 0.20848, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.2105 - mae: 0.3539 - val_loss: 0.2085 - val_mae: 0.3586\n",
      "Epoch 48/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1934 - mae: 0.3394\n",
      "Epoch 48: val_loss improved from 0.20848 to 0.19989, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1934 - mae: 0.3394 - val_loss: 0.1999 - val_mae: 0.3509\n",
      "Epoch 49/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1802 - mae: 0.3282\n",
      "Epoch 49: val_loss improved from 0.19989 to 0.19559, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1802 - mae: 0.3282 - val_loss: 0.1956 - val_mae: 0.3471\n",
      "Epoch 50/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1723 - mae: 0.3224\n",
      "Epoch 50: val_loss improved from 0.19559 to 0.19526, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1723 - mae: 0.3224 - val_loss: 0.1953 - val_mae: 0.3471\n",
      "Epoch 51/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1694 - mae: 0.3220\n",
      "Epoch 51: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1694 - mae: 0.3220 - val_loss: 0.1977 - val_mae: 0.3497\n",
      "Epoch 52/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1700 - mae: 0.3250\n",
      "Epoch 52: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1700 - mae: 0.3250 - val_loss: 0.2012 - val_mae: 0.3533\n",
      "Epoch 53/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1722 - mae: 0.3293\n",
      "Epoch 53: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1722 - mae: 0.3293 - val_loss: 0.2043 - val_mae: 0.3560\n",
      "Epoch 54/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1743 - mae: 0.3327\n",
      "Epoch 54: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1743 - mae: 0.3327 - val_loss: 0.2057 - val_mae: 0.3567\n",
      "Epoch 55/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1750 - mae: 0.3342\n",
      "Epoch 55: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1750 - mae: 0.3342 - val_loss: 0.2050 - val_mae: 0.3550\n",
      "Epoch 56/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1730 - mae: 0.3325\n",
      "Epoch 56: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1730 - mae: 0.3325 - val_loss: 0.2021 - val_mae: 0.3508\n",
      "Epoch 57/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1695 - mae: 0.3284\n",
      "Epoch 57: val_loss did not improve from 0.19526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1695 - mae: 0.3284 - val_loss: 0.1978 - val_mae: 0.3448\n",
      "Epoch 58/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1642 - mae: 0.3222\n",
      "Epoch 58: val_loss improved from 0.19526 to 0.19271, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1642 - mae: 0.3222 - val_loss: 0.1927 - val_mae: 0.3379\n",
      "Epoch 59/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1585 - mae: 0.3152\n",
      "Epoch 59: val_loss improved from 0.19271 to 0.18789, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1585 - mae: 0.3152 - val_loss: 0.1879 - val_mae: 0.3312\n",
      "Epoch 60/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1531 - mae: 0.3084\n",
      "Epoch 60: val_loss improved from 0.18789 to 0.18402, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1531 - mae: 0.3084 - val_loss: 0.1840 - val_mae: 0.3257\n",
      "Epoch 61/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1492 - mae: 0.3031\n",
      "Epoch 61: val_loss improved from 0.18402 to 0.18148, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1492 - mae: 0.3031 - val_loss: 0.1815 - val_mae: 0.3217\n",
      "Epoch 62/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1466 - mae: 0.2996\n",
      "Epoch 62: val_loss improved from 0.18148 to 0.18018, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1466 - mae: 0.2996 - val_loss: 0.1802 - val_mae: 0.3193\n",
      "Epoch 63/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1458 - mae: 0.2981\n",
      "Epoch 63: val_loss improved from 0.18018 to 0.17966, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1458 - mae: 0.2981 - val_loss: 0.1797 - val_mae: 0.3183\n",
      "Epoch 64/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1461 - mae: 0.2982\n",
      "Epoch 64: val_loss improved from 0.17966 to 0.17923, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1461 - mae: 0.2982 - val_loss: 0.1792 - val_mae: 0.3178\n",
      "Epoch 65/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1464 - mae: 0.2986\n",
      "Epoch 65: val_loss improved from 0.17923 to 0.17830, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1464 - mae: 0.2986 - val_loss: 0.1783 - val_mae: 0.3173\n",
      "Epoch 66/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1463 - mae: 0.2986\n",
      "Epoch 66: val_loss improved from 0.17830 to 0.17655, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1463 - mae: 0.2986 - val_loss: 0.1765 - val_mae: 0.3164\n",
      "Epoch 67/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1453 - mae: 0.2976\n",
      "Epoch 67: val_loss improved from 0.17655 to 0.17408, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1453 - mae: 0.2976 - val_loss: 0.1741 - val_mae: 0.3150\n",
      "Epoch 68/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1435 - mae: 0.2957\n",
      "Epoch 68: val_loss improved from 0.17408 to 0.17131, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1435 - mae: 0.2957 - val_loss: 0.1713 - val_mae: 0.3134\n",
      "Epoch 69/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1410 - mae: 0.2932\n",
      "Epoch 69: val_loss improved from 0.17131 to 0.16873, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1410 - mae: 0.2932 - val_loss: 0.1687 - val_mae: 0.3121\n",
      "Epoch 70/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1386 - mae: 0.2907\n",
      "Epoch 70: val_loss improved from 0.16873 to 0.16678, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1386 - mae: 0.2907 - val_loss: 0.1668 - val_mae: 0.3113\n",
      "Epoch 71/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1367 - mae: 0.2887\n",
      "Epoch 71: val_loss improved from 0.16678 to 0.16562, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1367 - mae: 0.2887 - val_loss: 0.1656 - val_mae: 0.3112\n",
      "Epoch 72/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1356 - mae: 0.2876\n",
      "Epoch 72: val_loss improved from 0.16562 to 0.16514, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1356 - mae: 0.2876 - val_loss: 0.1651 - val_mae: 0.3116\n",
      "Epoch 73/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1353 - mae: 0.2874\n",
      "Epoch 73: val_loss improved from 0.16514 to 0.16507, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1353 - mae: 0.2874 - val_loss: 0.1651 - val_mae: 0.3121\n",
      "Epoch 74/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1350 - mae: 0.2874\n",
      "Epoch 74: val_loss did not improve from 0.16507\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1350 - mae: 0.2874 - val_loss: 0.1651 - val_mae: 0.3123\n",
      "Epoch 75/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1346 - mae: 0.2874\n",
      "Epoch 75: val_loss improved from 0.16507 to 0.16488, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1346 - mae: 0.2874 - val_loss: 0.1649 - val_mae: 0.3120\n",
      "Epoch 76/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1343 - mae: 0.2872\n",
      "Epoch 76: val_loss improved from 0.16488 to 0.16436, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1343 - mae: 0.2872 - val_loss: 0.1644 - val_mae: 0.3111\n",
      "Epoch 77/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1339 - mae: 0.2866\n",
      "Epoch 77: val_loss improved from 0.16436 to 0.16355, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1339 - mae: 0.2866 - val_loss: 0.1636 - val_mae: 0.3096\n",
      "Epoch 78/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1327 - mae: 0.2853\n",
      "Epoch 78: val_loss improved from 0.16355 to 0.16263, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1327 - mae: 0.2853 - val_loss: 0.1626 - val_mae: 0.3078\n",
      "Epoch 79/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1314 - mae: 0.2839\n",
      "Epoch 79: val_loss improved from 0.16263 to 0.16178, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1314 - mae: 0.2839 - val_loss: 0.1618 - val_mae: 0.3059\n",
      "Epoch 80/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1303 - mae: 0.2825\n",
      "Epoch 80: val_loss improved from 0.16178 to 0.16118, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1303 - mae: 0.2825 - val_loss: 0.1612 - val_mae: 0.3043\n",
      "Epoch 81/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1295 - mae: 0.2816\n",
      "Epoch 81: val_loss improved from 0.16118 to 0.16089, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1295 - mae: 0.2816 - val_loss: 0.1609 - val_mae: 0.3032\n",
      "Epoch 82/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1290 - mae: 0.2810\n",
      "Epoch 82: val_loss improved from 0.16089 to 0.16082, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1290 - mae: 0.2810 - val_loss: 0.1608 - val_mae: 0.3024\n",
      "Epoch 83/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1288 - mae: 0.2810\n",
      "Epoch 83: val_loss did not improve from 0.16082\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1288 - mae: 0.2810 - val_loss: 0.1608 - val_mae: 0.3019\n",
      "Epoch 84/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1286 - mae: 0.2809\n",
      "Epoch 84: val_loss improved from 0.16082 to 0.16073, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1286 - mae: 0.2809 - val_loss: 0.1607 - val_mae: 0.3014\n",
      "Epoch 85/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1285 - mae: 0.2808\n",
      "Epoch 85: val_loss improved from 0.16073 to 0.16043, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1285 - mae: 0.2808 - val_loss: 0.1604 - val_mae: 0.3010\n",
      "Epoch 86/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1282 - mae: 0.2805\n",
      "Epoch 86: val_loss improved from 0.16043 to 0.15993, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1282 - mae: 0.2805 - val_loss: 0.1599 - val_mae: 0.3006\n",
      "Epoch 87/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1275 - mae: 0.2798\n",
      "Epoch 87: val_loss improved from 0.15993 to 0.15934, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1275 - mae: 0.2798 - val_loss: 0.1593 - val_mae: 0.3002\n",
      "Epoch 88/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1269 - mae: 0.2790\n",
      "Epoch 88: val_loss improved from 0.15934 to 0.15876, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1269 - mae: 0.2790 - val_loss: 0.1588 - val_mae: 0.2999\n",
      "Epoch 89/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1265 - mae: 0.2786\n",
      "Epoch 89: val_loss improved from 0.15876 to 0.15831, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1265 - mae: 0.2786 - val_loss: 0.1583 - val_mae: 0.2999\n",
      "Epoch 90/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1258 - mae: 0.2777\n",
      "Epoch 90: val_loss improved from 0.15831 to 0.15798, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1258 - mae: 0.2777 - val_loss: 0.1580 - val_mae: 0.3000\n",
      "Epoch 91/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1253 - mae: 0.2773\n",
      "Epoch 91: val_loss improved from 0.15798 to 0.15772, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1253 - mae: 0.2773 - val_loss: 0.1577 - val_mae: 0.3001\n",
      "Epoch 92/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1253 - mae: 0.2771\n",
      "Epoch 92: val_loss improved from 0.15772 to 0.15743, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1253 - mae: 0.2771 - val_loss: 0.1574 - val_mae: 0.3002\n",
      "Epoch 93/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1249 - mae: 0.2768\n",
      "Epoch 93: val_loss improved from 0.15743 to 0.15707, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1249 - mae: 0.2768 - val_loss: 0.1571 - val_mae: 0.3000\n",
      "Epoch 94/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1249 - mae: 0.2767\n",
      "Epoch 94: val_loss improved from 0.15707 to 0.15660, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1249 - mae: 0.2767 - val_loss: 0.1566 - val_mae: 0.2997\n",
      "Epoch 95/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1242 - mae: 0.2759\n",
      "Epoch 95: val_loss improved from 0.15660 to 0.15609, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1242 - mae: 0.2759 - val_loss: 0.1561 - val_mae: 0.2992\n",
      "Epoch 96/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1240 - mae: 0.2756\n",
      "Epoch 96: val_loss improved from 0.15609 to 0.15559, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1240 - mae: 0.2756 - val_loss: 0.1556 - val_mae: 0.2987\n",
      "Epoch 97/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1237 - mae: 0.2752\n",
      "Epoch 97: val_loss improved from 0.15559 to 0.15517, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1237 - mae: 0.2752 - val_loss: 0.1552 - val_mae: 0.2981\n",
      "Epoch 98/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1234 - mae: 0.2748\n",
      "Epoch 98: val_loss improved from 0.15517 to 0.15483, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1234 - mae: 0.2748 - val_loss: 0.1548 - val_mae: 0.2976\n",
      "Epoch 99/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1232 - mae: 0.2745\n",
      "Epoch 99: val_loss improved from 0.15483 to 0.15457, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1232 - mae: 0.2745 - val_loss: 0.1546 - val_mae: 0.2971\n",
      "Epoch 100/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1226 - mae: 0.2740\n",
      "Epoch 100: val_loss improved from 0.15457 to 0.15434, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1226 - mae: 0.2740 - val_loss: 0.1543 - val_mae: 0.2967\n",
      "Epoch 101/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1226 - mae: 0.2738\n",
      "Epoch 101: val_loss improved from 0.15434 to 0.15411, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1226 - mae: 0.2738 - val_loss: 0.1541 - val_mae: 0.2962\n",
      "Epoch 102/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1226 - mae: 0.2738\n",
      "Epoch 102: val_loss improved from 0.15411 to 0.15388, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1226 - mae: 0.2738 - val_loss: 0.1539 - val_mae: 0.2958\n",
      "Epoch 103/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1222 - mae: 0.2733\n",
      "Epoch 103: val_loss improved from 0.15388 to 0.15364, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1222 - mae: 0.2733 - val_loss: 0.1536 - val_mae: 0.2955\n",
      "Epoch 104/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1220 - mae: 0.2732\n",
      "Epoch 104: val_loss improved from 0.15364 to 0.15342, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1220 - mae: 0.2732 - val_loss: 0.1534 - val_mae: 0.2951\n",
      "Epoch 105/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1219 - mae: 0.2729\n",
      "Epoch 105: val_loss improved from 0.15342 to 0.15323, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1219 - mae: 0.2729 - val_loss: 0.1532 - val_mae: 0.2949\n",
      "Epoch 106/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1214 - mae: 0.2725\n",
      "Epoch 106: val_loss improved from 0.15323 to 0.15303, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1214 - mae: 0.2725 - val_loss: 0.1530 - val_mae: 0.2947\n",
      "Epoch 107/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1212 - mae: 0.2724\n",
      "Epoch 107: val_loss improved from 0.15303 to 0.15280, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1212 - mae: 0.2724 - val_loss: 0.1528 - val_mae: 0.2944\n",
      "Epoch 108/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1211 - mae: 0.2722\n",
      "Epoch 108: val_loss improved from 0.15280 to 0.15250, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1211 - mae: 0.2722 - val_loss: 0.1525 - val_mae: 0.2942\n",
      "Epoch 109/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1208 - mae: 0.2718\n",
      "Epoch 109: val_loss improved from 0.15250 to 0.15211, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1208 - mae: 0.2718 - val_loss: 0.1521 - val_mae: 0.2938\n",
      "Epoch 110/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1206 - mae: 0.2717\n",
      "Epoch 110: val_loss improved from 0.15211 to 0.15164, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1206 - mae: 0.2717 - val_loss: 0.1516 - val_mae: 0.2934\n",
      "Epoch 111/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1205 - mae: 0.2716\n",
      "Epoch 111: val_loss improved from 0.15164 to 0.15110, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1205 - mae: 0.2716 - val_loss: 0.1511 - val_mae: 0.2929\n",
      "Epoch 112/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1203 - mae: 0.2713\n",
      "Epoch 112: val_loss improved from 0.15110 to 0.15055, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1203 - mae: 0.2713 - val_loss: 0.1506 - val_mae: 0.2925\n",
      "Epoch 113/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1199 - mae: 0.2709\n",
      "Epoch 113: val_loss improved from 0.15055 to 0.15000, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1199 - mae: 0.2709 - val_loss: 0.1500 - val_mae: 0.2920\n",
      "Epoch 114/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1198 - mae: 0.2707\n",
      "Epoch 114: val_loss improved from 0.15000 to 0.14947, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1198 - mae: 0.2707 - val_loss: 0.1495 - val_mae: 0.2916\n",
      "Epoch 115/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1194 - mae: 0.2702\n",
      "Epoch 115: val_loss improved from 0.14947 to 0.14896, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1194 - mae: 0.2702 - val_loss: 0.1490 - val_mae: 0.2913\n",
      "Epoch 116/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1193 - mae: 0.2702\n",
      "Epoch 116: val_loss improved from 0.14896 to 0.14848, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1193 - mae: 0.2702 - val_loss: 0.1485 - val_mae: 0.2910\n",
      "Epoch 117/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1193 - mae: 0.2701\n",
      "Epoch 117: val_loss improved from 0.14848 to 0.14804, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1193 - mae: 0.2701 - val_loss: 0.1480 - val_mae: 0.2907\n",
      "Epoch 118/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1191 - mae: 0.2698\n",
      "Epoch 118: val_loss improved from 0.14804 to 0.14762, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1191 - mae: 0.2698 - val_loss: 0.1476 - val_mae: 0.2904\n",
      "Epoch 119/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1188 - mae: 0.2695\n",
      "Epoch 119: val_loss improved from 0.14762 to 0.14725, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1188 - mae: 0.2695 - val_loss: 0.1472 - val_mae: 0.2902\n",
      "Epoch 120/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1186 - mae: 0.2694\n",
      "Epoch 120: val_loss improved from 0.14725 to 0.14692, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1186 - mae: 0.2694 - val_loss: 0.1469 - val_mae: 0.2900\n",
      "Epoch 121/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1183 - mae: 0.2689\n",
      "Epoch 121: val_loss improved from 0.14692 to 0.14661, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1183 - mae: 0.2689 - val_loss: 0.1466 - val_mae: 0.2898\n",
      "Epoch 122/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1181 - mae: 0.2688\n",
      "Epoch 122: val_loss improved from 0.14661 to 0.14632, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1181 - mae: 0.2688 - val_loss: 0.1463 - val_mae: 0.2896\n",
      "Epoch 123/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1180 - mae: 0.2687\n",
      "Epoch 123: val_loss improved from 0.14632 to 0.14603, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1180 - mae: 0.2687 - val_loss: 0.1460 - val_mae: 0.2893\n",
      "Epoch 124/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1177 - mae: 0.2684\n",
      "Epoch 124: val_loss improved from 0.14603 to 0.14573, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.1177 - mae: 0.2684 - val_loss: 0.1457 - val_mae: 0.2890\n",
      "Epoch 125/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1176 - mae: 0.2682\n",
      "Epoch 125: val_loss improved from 0.14573 to 0.14542, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1176 - mae: 0.2682 - val_loss: 0.1454 - val_mae: 0.2887\n",
      "Epoch 126/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1176 - mae: 0.2683\n",
      "Epoch 126: val_loss improved from 0.14542 to 0.14512, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1176 - mae: 0.2683 - val_loss: 0.1451 - val_mae: 0.2883\n",
      "Epoch 127/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1174 - mae: 0.2679\n",
      "Epoch 127: val_loss improved from 0.14512 to 0.14482, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1174 - mae: 0.2679 - val_loss: 0.1448 - val_mae: 0.2880\n",
      "Epoch 128/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1171 - mae: 0.2676\n",
      "Epoch 128: val_loss improved from 0.14482 to 0.14454, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1171 - mae: 0.2676 - val_loss: 0.1445 - val_mae: 0.2877\n",
      "Epoch 129/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1171 - mae: 0.2676\n",
      "Epoch 129: val_loss improved from 0.14454 to 0.14427, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1171 - mae: 0.2676 - val_loss: 0.1443 - val_mae: 0.2874\n",
      "Epoch 130/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1169 - mae: 0.2675\n",
      "Epoch 130: val_loss improved from 0.14427 to 0.14402, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1169 - mae: 0.2675 - val_loss: 0.1440 - val_mae: 0.2871\n",
      "Epoch 131/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1167 - mae: 0.2673\n",
      "Epoch 131: val_loss improved from 0.14402 to 0.14378, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1167 - mae: 0.2673 - val_loss: 0.1438 - val_mae: 0.2869\n",
      "Epoch 132/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1166 - mae: 0.2671\n",
      "Epoch 132: val_loss improved from 0.14378 to 0.14356, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1166 - mae: 0.2671 - val_loss: 0.1436 - val_mae: 0.2866\n",
      "Epoch 133/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1165 - mae: 0.2670\n",
      "Epoch 133: val_loss improved from 0.14356 to 0.14333, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1165 - mae: 0.2670 - val_loss: 0.1433 - val_mae: 0.2864\n",
      "Epoch 134/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1160 - mae: 0.2665\n",
      "Epoch 134: val_loss improved from 0.14333 to 0.14311, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1160 - mae: 0.2665 - val_loss: 0.1431 - val_mae: 0.2863\n",
      "Epoch 135/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1161 - mae: 0.2665\n",
      "Epoch 135: val_loss improved from 0.14311 to 0.14289, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.1161 - mae: 0.2665 - val_loss: 0.1429 - val_mae: 0.2861\n",
      "Epoch 136/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1161 - mae: 0.2665\n",
      "Epoch 136: val_loss improved from 0.14289 to 0.14266, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1161 - mae: 0.2665 - val_loss: 0.1427 - val_mae: 0.2859\n",
      "Epoch 137/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1158 - mae: 0.2662\n",
      "Epoch 137: val_loss improved from 0.14266 to 0.14241, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1158 - mae: 0.2662 - val_loss: 0.1424 - val_mae: 0.2857\n",
      "Epoch 138/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1155 - mae: 0.2658\n",
      "Epoch 138: val_loss improved from 0.14241 to 0.14215, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1155 - mae: 0.2658 - val_loss: 0.1422 - val_mae: 0.2855\n",
      "Epoch 139/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1153 - mae: 0.2658\n",
      "Epoch 139: val_loss improved from 0.14215 to 0.14189, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1153 - mae: 0.2658 - val_loss: 0.1419 - val_mae: 0.2852\n",
      "Epoch 140/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1155 - mae: 0.2660\n",
      "Epoch 140: val_loss improved from 0.14189 to 0.14162, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1155 - mae: 0.2660 - val_loss: 0.1416 - val_mae: 0.2850\n",
      "Epoch 141/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1151 - mae: 0.2654\n",
      "Epoch 141: val_loss improved from 0.14162 to 0.14135, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1151 - mae: 0.2654 - val_loss: 0.1414 - val_mae: 0.2848\n",
      "Epoch 142/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1152 - mae: 0.2656\n",
      "Epoch 142: val_loss improved from 0.14135 to 0.14110, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1152 - mae: 0.2656 - val_loss: 0.1411 - val_mae: 0.2845\n",
      "Epoch 143/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1151 - mae: 0.2653\n",
      "Epoch 143: val_loss improved from 0.14110 to 0.14086, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1151 - mae: 0.2653 - val_loss: 0.1409 - val_mae: 0.2843\n",
      "Epoch 144/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1148 - mae: 0.2651\n",
      "Epoch 144: val_loss improved from 0.14086 to 0.14064, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1148 - mae: 0.2651 - val_loss: 0.1406 - val_mae: 0.2841\n",
      "Epoch 145/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1147 - mae: 0.2649\n",
      "Epoch 145: val_loss improved from 0.14064 to 0.14043, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1147 - mae: 0.2649 - val_loss: 0.1404 - val_mae: 0.2839\n",
      "Epoch 146/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1144 - mae: 0.2647\n",
      "Epoch 146: val_loss improved from 0.14043 to 0.14024, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1144 - mae: 0.2647 - val_loss: 0.1402 - val_mae: 0.2837\n",
      "Epoch 147/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1145 - mae: 0.2647\n",
      "Epoch 147: val_loss improved from 0.14024 to 0.14006, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1145 - mae: 0.2647 - val_loss: 0.1401 - val_mae: 0.2836\n",
      "Epoch 148/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1141 - mae: 0.2642\n",
      "Epoch 148: val_loss improved from 0.14006 to 0.13988, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1141 - mae: 0.2642 - val_loss: 0.1399 - val_mae: 0.2834\n",
      "Epoch 149/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1140 - mae: 0.2643\n",
      "Epoch 149: val_loss improved from 0.13988 to 0.13970, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1140 - mae: 0.2643 - val_loss: 0.1397 - val_mae: 0.2832\n",
      "Epoch 150/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1139 - mae: 0.2642\n",
      "Epoch 150: val_loss improved from 0.13970 to 0.13951, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1139 - mae: 0.2642 - val_loss: 0.1395 - val_mae: 0.2830\n",
      "Epoch 151/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1139 - mae: 0.2641\n",
      "Epoch 151: val_loss improved from 0.13951 to 0.13932, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1139 - mae: 0.2641 - val_loss: 0.1393 - val_mae: 0.2828\n",
      "Epoch 152/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1139 - mae: 0.2641\n",
      "Epoch 152: val_loss improved from 0.13932 to 0.13913, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1139 - mae: 0.2641 - val_loss: 0.1391 - val_mae: 0.2826\n",
      "Epoch 153/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1136 - mae: 0.2637\n",
      "Epoch 153: val_loss improved from 0.13913 to 0.13893, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1136 - mae: 0.2637 - val_loss: 0.1389 - val_mae: 0.2824\n",
      "Epoch 154/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1136 - mae: 0.2637\n",
      "Epoch 154: val_loss improved from 0.13893 to 0.13874, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1136 - mae: 0.2637 - val_loss: 0.1387 - val_mae: 0.2822\n",
      "Epoch 155/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1134 - mae: 0.2634\n",
      "Epoch 155: val_loss improved from 0.13874 to 0.13855, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1134 - mae: 0.2634 - val_loss: 0.1386 - val_mae: 0.2821\n",
      "Epoch 156/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1133 - mae: 0.2634\n",
      "Epoch 156: val_loss improved from 0.13855 to 0.13837, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1133 - mae: 0.2634 - val_loss: 0.1384 - val_mae: 0.2819\n",
      "Epoch 157/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1131 - mae: 0.2631\n",
      "Epoch 157: val_loss improved from 0.13837 to 0.13819, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1131 - mae: 0.2631 - val_loss: 0.1382 - val_mae: 0.2817\n",
      "Epoch 158/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1130 - mae: 0.2630\n",
      "Epoch 158: val_loss improved from 0.13819 to 0.13802, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1130 - mae: 0.2630 - val_loss: 0.1380 - val_mae: 0.2816\n",
      "Epoch 159/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1130 - mae: 0.2630\n",
      "Epoch 159: val_loss improved from 0.13802 to 0.13785, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1130 - mae: 0.2630 - val_loss: 0.1378 - val_mae: 0.2814\n",
      "Epoch 160/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1129 - mae: 0.2629\n",
      "Epoch 160: val_loss improved from 0.13785 to 0.13768, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1129 - mae: 0.2629 - val_loss: 0.1377 - val_mae: 0.2813\n",
      "Epoch 161/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1125 - mae: 0.2625\n",
      "Epoch 161: val_loss improved from 0.13768 to 0.13752, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1125 - mae: 0.2625 - val_loss: 0.1375 - val_mae: 0.2811\n",
      "Epoch 162/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1125 - mae: 0.2623\n",
      "Epoch 162: val_loss improved from 0.13752 to 0.13735, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1125 - mae: 0.2623 - val_loss: 0.1374 - val_mae: 0.2809\n",
      "Epoch 163/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1123 - mae: 0.2622\n",
      "Epoch 163: val_loss improved from 0.13735 to 0.13719, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1123 - mae: 0.2622 - val_loss: 0.1372 - val_mae: 0.2808\n",
      "Epoch 164/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1121 - mae: 0.2620\n",
      "Epoch 164: val_loss improved from 0.13719 to 0.13702, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1121 - mae: 0.2620 - val_loss: 0.1370 - val_mae: 0.2806\n",
      "Epoch 165/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1120 - mae: 0.2618\n",
      "Epoch 165: val_loss improved from 0.13702 to 0.13686, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1120 - mae: 0.2618 - val_loss: 0.1369 - val_mae: 0.2805\n",
      "Epoch 166/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1120 - mae: 0.2618\n",
      "Epoch 166: val_loss improved from 0.13686 to 0.13670, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1120 - mae: 0.2618 - val_loss: 0.1367 - val_mae: 0.2803\n",
      "Epoch 167/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1119 - mae: 0.2617\n",
      "Epoch 167: val_loss improved from 0.13670 to 0.13654, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1119 - mae: 0.2617 - val_loss: 0.1365 - val_mae: 0.2801\n",
      "Epoch 168/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1117 - mae: 0.2615\n",
      "Epoch 168: val_loss improved from 0.13654 to 0.13639, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1117 - mae: 0.2615 - val_loss: 0.1364 - val_mae: 0.2800\n",
      "Epoch 169/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1118 - mae: 0.2616\n",
      "Epoch 169: val_loss improved from 0.13639 to 0.13625, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1118 - mae: 0.2616 - val_loss: 0.1363 - val_mae: 0.2798\n",
      "Epoch 170/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1116 - mae: 0.2614\n",
      "Epoch 170: val_loss improved from 0.13625 to 0.13612, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1116 - mae: 0.2614 - val_loss: 0.1361 - val_mae: 0.2797\n",
      "Epoch 171/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1116 - mae: 0.2614\n",
      "Epoch 171: val_loss improved from 0.13612 to 0.13599, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1116 - mae: 0.2614 - val_loss: 0.1360 - val_mae: 0.2796\n",
      "Epoch 172/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1114 - mae: 0.2611\n",
      "Epoch 172: val_loss improved from 0.13599 to 0.13586, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1114 - mae: 0.2611 - val_loss: 0.1359 - val_mae: 0.2794\n",
      "Epoch 173/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1114 - mae: 0.2612\n",
      "Epoch 173: val_loss improved from 0.13586 to 0.13574, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1114 - mae: 0.2612 - val_loss: 0.1357 - val_mae: 0.2793\n",
      "Epoch 174/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1112 - mae: 0.2609\n",
      "Epoch 174: val_loss improved from 0.13574 to 0.13562, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1112 - mae: 0.2609 - val_loss: 0.1356 - val_mae: 0.2791\n",
      "Epoch 175/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1109 - mae: 0.2607\n",
      "Epoch 175: val_loss improved from 0.13562 to 0.13549, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1109 - mae: 0.2607 - val_loss: 0.1355 - val_mae: 0.2790\n",
      "Epoch 176/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1109 - mae: 0.2605\n",
      "Epoch 176: val_loss improved from 0.13549 to 0.13537, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.1109 - mae: 0.2605 - val_loss: 0.1354 - val_mae: 0.2789\n",
      "Epoch 177/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1108 - mae: 0.2604\n",
      "Epoch 177: val_loss improved from 0.13537 to 0.13524, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.1108 - mae: 0.2604 - val_loss: 0.1352 - val_mae: 0.2787\n",
      "Epoch 178/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1107 - mae: 0.2604\n",
      "Epoch 178: val_loss improved from 0.13524 to 0.13511, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1107 - mae: 0.2604 - val_loss: 0.1351 - val_mae: 0.2786\n",
      "Epoch 179/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1104 - mae: 0.2600\n",
      "Epoch 179: val_loss improved from 0.13511 to 0.13498, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1104 - mae: 0.2600 - val_loss: 0.1350 - val_mae: 0.2784\n",
      "Epoch 180/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1104 - mae: 0.2600\n",
      "Epoch 180: val_loss improved from 0.13498 to 0.13486, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.1104 - mae: 0.2600 - val_loss: 0.1349 - val_mae: 0.2783\n",
      "Epoch 181/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1104 - mae: 0.2600\n",
      "Epoch 181: val_loss improved from 0.13486 to 0.13473, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1104 - mae: 0.2600 - val_loss: 0.1347 - val_mae: 0.2782\n",
      "Epoch 182/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1102 - mae: 0.2598\n",
      "Epoch 182: val_loss improved from 0.13473 to 0.13460, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1102 - mae: 0.2598 - val_loss: 0.1346 - val_mae: 0.2780\n",
      "Epoch 183/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1104 - mae: 0.2599\n",
      "Epoch 183: val_loss improved from 0.13460 to 0.13448, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1104 - mae: 0.2599 - val_loss: 0.1345 - val_mae: 0.2779\n",
      "Epoch 184/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1099 - mae: 0.2595\n",
      "Epoch 184: val_loss improved from 0.13448 to 0.13436, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.1099 - mae: 0.2595 - val_loss: 0.1344 - val_mae: 0.2778\n",
      "Epoch 185/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1099 - mae: 0.2594\n",
      "Epoch 185: val_loss improved from 0.13436 to 0.13424, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1099 - mae: 0.2594 - val_loss: 0.1342 - val_mae: 0.2776\n",
      "Epoch 186/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1097 - mae: 0.2592\n",
      "Epoch 186: val_loss improved from 0.13424 to 0.13413, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1097 - mae: 0.2592 - val_loss: 0.1341 - val_mae: 0.2775\n",
      "Epoch 187/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1098 - mae: 0.2592\n",
      "Epoch 187: val_loss improved from 0.13413 to 0.13401, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1098 - mae: 0.2592 - val_loss: 0.1340 - val_mae: 0.2774\n",
      "Epoch 188/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1095 - mae: 0.2589\n",
      "Epoch 188: val_loss improved from 0.13401 to 0.13389, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1095 - mae: 0.2589 - val_loss: 0.1339 - val_mae: 0.2773\n",
      "Epoch 189/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1094 - mae: 0.2588\n",
      "Epoch 189: val_loss improved from 0.13389 to 0.13376, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1094 - mae: 0.2588 - val_loss: 0.1338 - val_mae: 0.2771\n",
      "Epoch 190/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1092 - mae: 0.2586\n",
      "Epoch 190: val_loss improved from 0.13376 to 0.13364, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1092 - mae: 0.2586 - val_loss: 0.1336 - val_mae: 0.2770\n",
      "Epoch 191/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1094 - mae: 0.2588\n",
      "Epoch 191: val_loss improved from 0.13364 to 0.13352, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1094 - mae: 0.2588 - val_loss: 0.1335 - val_mae: 0.2769\n",
      "Epoch 192/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1094 - mae: 0.2587\n",
      "Epoch 192: val_loss improved from 0.13352 to 0.13340, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1094 - mae: 0.2587 - val_loss: 0.1334 - val_mae: 0.2767\n",
      "Epoch 193/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1092 - mae: 0.2585\n",
      "Epoch 193: val_loss improved from 0.13340 to 0.13328, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1092 - mae: 0.2585 - val_loss: 0.1333 - val_mae: 0.2766\n",
      "Epoch 194/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1089 - mae: 0.2582\n",
      "Epoch 194: val_loss improved from 0.13328 to 0.13316, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1089 - mae: 0.2582 - val_loss: 0.1332 - val_mae: 0.2765\n",
      "Epoch 195/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1087 - mae: 0.2579\n",
      "Epoch 195: val_loss improved from 0.13316 to 0.13305, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1087 - mae: 0.2579 - val_loss: 0.1331 - val_mae: 0.2764\n",
      "Epoch 196/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1088 - mae: 0.2581\n",
      "Epoch 196: val_loss improved from 0.13305 to 0.13294, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1088 - mae: 0.2581 - val_loss: 0.1329 - val_mae: 0.2762\n",
      "Epoch 197/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1084 - mae: 0.2577\n",
      "Epoch 197: val_loss improved from 0.13294 to 0.13283, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.1084 - mae: 0.2577 - val_loss: 0.1328 - val_mae: 0.2761\n",
      "Epoch 198/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1088 - mae: 0.2580\n",
      "Epoch 198: val_loss improved from 0.13283 to 0.13272, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1088 - mae: 0.2580 - val_loss: 0.1327 - val_mae: 0.2760\n",
      "Epoch 199/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1088 - mae: 0.2580\n",
      "Epoch 199: val_loss improved from 0.13272 to 0.13262, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1088 - mae: 0.2580 - val_loss: 0.1326 - val_mae: 0.2759\n",
      "Epoch 200/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1085 - mae: 0.2576\n",
      "Epoch 200: val_loss improved from 0.13262 to 0.13251, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1085 - mae: 0.2576 - val_loss: 0.1325 - val_mae: 0.2757\n",
      "Epoch 201/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1082 - mae: 0.2573\n",
      "Epoch 201: val_loss improved from 0.13251 to 0.13241, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1082 - mae: 0.2573 - val_loss: 0.1324 - val_mae: 0.2756\n",
      "Epoch 202/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1081 - mae: 0.2572\n",
      "Epoch 202: val_loss improved from 0.13241 to 0.13231, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1081 - mae: 0.2572 - val_loss: 0.1323 - val_mae: 0.2755\n",
      "Epoch 203/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1081 - mae: 0.2571\n",
      "Epoch 203: val_loss improved from 0.13231 to 0.13221, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1081 - mae: 0.2571 - val_loss: 0.1322 - val_mae: 0.2754\n",
      "Epoch 204/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1080 - mae: 0.2570\n",
      "Epoch 204: val_loss improved from 0.13221 to 0.13212, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1080 - mae: 0.2570 - val_loss: 0.1321 - val_mae: 0.2753\n",
      "Epoch 205/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1079 - mae: 0.2569\n",
      "Epoch 205: val_loss improved from 0.13212 to 0.13202, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1079 - mae: 0.2569 - val_loss: 0.1320 - val_mae: 0.2752\n",
      "Epoch 206/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1076 - mae: 0.2566\n",
      "Epoch 206: val_loss improved from 0.13202 to 0.13193, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1076 - mae: 0.2566 - val_loss: 0.1319 - val_mae: 0.2751\n",
      "Epoch 207/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1076 - mae: 0.2567\n",
      "Epoch 207: val_loss improved from 0.13193 to 0.13183, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1076 - mae: 0.2567 - val_loss: 0.1318 - val_mae: 0.2749\n",
      "Epoch 208/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1076 - mae: 0.2566\n",
      "Epoch 208: val_loss improved from 0.13183 to 0.13174, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1076 - mae: 0.2566 - val_loss: 0.1317 - val_mae: 0.2748\n",
      "Epoch 209/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1073 - mae: 0.2562\n",
      "Epoch 209: val_loss improved from 0.13174 to 0.13164, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1073 - mae: 0.2562 - val_loss: 0.1316 - val_mae: 0.2747\n",
      "Epoch 210/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1073 - mae: 0.2562\n",
      "Epoch 210: val_loss improved from 0.13164 to 0.13153, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1073 - mae: 0.2562 - val_loss: 0.1315 - val_mae: 0.2746\n",
      "Epoch 211/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1073 - mae: 0.2563\n",
      "Epoch 211: val_loss improved from 0.13153 to 0.13143, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1073 - mae: 0.2563 - val_loss: 0.1314 - val_mae: 0.2745\n",
      "Epoch 212/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1072 - mae: 0.2561\n",
      "Epoch 212: val_loss improved from 0.13143 to 0.13133, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1072 - mae: 0.2561 - val_loss: 0.1313 - val_mae: 0.2744\n",
      "Epoch 213/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1071 - mae: 0.2560\n",
      "Epoch 213: val_loss improved from 0.13133 to 0.13123, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1071 - mae: 0.2560 - val_loss: 0.1312 - val_mae: 0.2743\n",
      "Epoch 214/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1070 - mae: 0.2559\n",
      "Epoch 214: val_loss improved from 0.13123 to 0.13114, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1070 - mae: 0.2559 - val_loss: 0.1311 - val_mae: 0.2742\n",
      "Epoch 215/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1071 - mae: 0.2560\n",
      "Epoch 215: val_loss improved from 0.13114 to 0.13106, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1071 - mae: 0.2560 - val_loss: 0.1311 - val_mae: 0.2740\n",
      "Epoch 216/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1068 - mae: 0.2556\n",
      "Epoch 216: val_loss improved from 0.13106 to 0.13099, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1068 - mae: 0.2556 - val_loss: 0.1310 - val_mae: 0.2739\n",
      "Epoch 217/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1065 - mae: 0.2552\n",
      "Epoch 217: val_loss improved from 0.13099 to 0.13091, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1065 - mae: 0.2552 - val_loss: 0.1309 - val_mae: 0.2738\n",
      "Epoch 218/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1066 - mae: 0.2554\n",
      "Epoch 218: val_loss improved from 0.13091 to 0.13083, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1066 - mae: 0.2554 - val_loss: 0.1308 - val_mae: 0.2737\n",
      "Epoch 219/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1067 - mae: 0.2554\n",
      "Epoch 219: val_loss improved from 0.13083 to 0.13074, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1067 - mae: 0.2554 - val_loss: 0.1307 - val_mae: 0.2736\n",
      "Epoch 220/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1065 - mae: 0.2552\n",
      "Epoch 220: val_loss improved from 0.13074 to 0.13065, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1065 - mae: 0.2552 - val_loss: 0.1307 - val_mae: 0.2735\n",
      "Epoch 221/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1063 - mae: 0.2549\n",
      "Epoch 221: val_loss improved from 0.13065 to 0.13056, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1063 - mae: 0.2549 - val_loss: 0.1306 - val_mae: 0.2734\n",
      "Epoch 222/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1062 - mae: 0.2549\n",
      "Epoch 222: val_loss improved from 0.13056 to 0.13048, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1062 - mae: 0.2549 - val_loss: 0.1305 - val_mae: 0.2733\n",
      "Epoch 223/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1059 - mae: 0.2545\n",
      "Epoch 223: val_loss improved from 0.13048 to 0.13039, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1059 - mae: 0.2545 - val_loss: 0.1304 - val_mae: 0.2732\n",
      "Epoch 224/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1058 - mae: 0.2544\n",
      "Epoch 224: val_loss improved from 0.13039 to 0.13031, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1058 - mae: 0.2544 - val_loss: 0.1303 - val_mae: 0.2731\n",
      "Epoch 225/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1058 - mae: 0.2545\n",
      "Epoch 225: val_loss improved from 0.13031 to 0.13022, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1058 - mae: 0.2545 - val_loss: 0.1302 - val_mae: 0.2730\n",
      "Epoch 226/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1059 - mae: 0.2544\n",
      "Epoch 226: val_loss improved from 0.13022 to 0.13014, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1059 - mae: 0.2544 - val_loss: 0.1301 - val_mae: 0.2729\n",
      "Epoch 227/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1057 - mae: 0.2542\n",
      "Epoch 227: val_loss improved from 0.13014 to 0.13006, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1057 - mae: 0.2542 - val_loss: 0.1301 - val_mae: 0.2728\n",
      "Epoch 228/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1054 - mae: 0.2538\n",
      "Epoch 228: val_loss improved from 0.13006 to 0.12998, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1054 - mae: 0.2538 - val_loss: 0.1300 - val_mae: 0.2727\n",
      "Epoch 229/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1056 - mae: 0.2542\n",
      "Epoch 229: val_loss improved from 0.12998 to 0.12990, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1056 - mae: 0.2542 - val_loss: 0.1299 - val_mae: 0.2726\n",
      "Epoch 230/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1054 - mae: 0.2539\n",
      "Epoch 230: val_loss improved from 0.12990 to 0.12982, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1054 - mae: 0.2539 - val_loss: 0.1298 - val_mae: 0.2725\n",
      "Epoch 231/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1053 - mae: 0.2538\n",
      "Epoch 231: val_loss improved from 0.12982 to 0.12974, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1053 - mae: 0.2538 - val_loss: 0.1297 - val_mae: 0.2724\n",
      "Epoch 232/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1053 - mae: 0.2536\n",
      "Epoch 232: val_loss improved from 0.12974 to 0.12966, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1053 - mae: 0.2536 - val_loss: 0.1297 - val_mae: 0.2723\n",
      "Epoch 233/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1054 - mae: 0.2537\n",
      "Epoch 233: val_loss improved from 0.12966 to 0.12959, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1054 - mae: 0.2537 - val_loss: 0.1296 - val_mae: 0.2722\n",
      "Epoch 234/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1050 - mae: 0.2533\n",
      "Epoch 234: val_loss improved from 0.12959 to 0.12952, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1050 - mae: 0.2533 - val_loss: 0.1295 - val_mae: 0.2721\n",
      "Epoch 235/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1048 - mae: 0.2531\n",
      "Epoch 235: val_loss improved from 0.12952 to 0.12945, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1048 - mae: 0.2531 - val_loss: 0.1295 - val_mae: 0.2720\n",
      "Epoch 236/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1048 - mae: 0.2532\n",
      "Epoch 236: val_loss improved from 0.12945 to 0.12939, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1048 - mae: 0.2532 - val_loss: 0.1294 - val_mae: 0.2719\n",
      "Epoch 237/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1048 - mae: 0.2530\n",
      "Epoch 237: val_loss improved from 0.12939 to 0.12933, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.1048 - mae: 0.2530 - val_loss: 0.1293 - val_mae: 0.2718\n",
      "Epoch 238/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1046 - mae: 0.2528\n",
      "Epoch 238: val_loss improved from 0.12933 to 0.12926, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1046 - mae: 0.2528 - val_loss: 0.1293 - val_mae: 0.2717\n",
      "Epoch 239/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1044 - mae: 0.2526\n",
      "Epoch 239: val_loss improved from 0.12926 to 0.12919, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1044 - mae: 0.2526 - val_loss: 0.1292 - val_mae: 0.2716\n",
      "Epoch 240/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1046 - mae: 0.2527\n",
      "Epoch 240: val_loss improved from 0.12919 to 0.12912, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1046 - mae: 0.2527 - val_loss: 0.1291 - val_mae: 0.2715\n",
      "Epoch 241/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1043 - mae: 0.2524\n",
      "Epoch 241: val_loss improved from 0.12912 to 0.12904, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1043 - mae: 0.2524 - val_loss: 0.1290 - val_mae: 0.2714\n",
      "Epoch 242/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1042 - mae: 0.2524\n",
      "Epoch 242: val_loss improved from 0.12904 to 0.12896, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1042 - mae: 0.2524 - val_loss: 0.1290 - val_mae: 0.2713\n",
      "Epoch 243/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1041 - mae: 0.2522\n",
      "Epoch 243: val_loss improved from 0.12896 to 0.12888, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1041 - mae: 0.2522 - val_loss: 0.1289 - val_mae: 0.2712\n",
      "Epoch 244/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1040 - mae: 0.2521\n",
      "Epoch 244: val_loss improved from 0.12888 to 0.12879, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1040 - mae: 0.2521 - val_loss: 0.1288 - val_mae: 0.2711\n",
      "Epoch 245/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1041 - mae: 0.2522\n",
      "Epoch 245: val_loss improved from 0.12879 to 0.12871, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1041 - mae: 0.2522 - val_loss: 0.1287 - val_mae: 0.2710\n",
      "Epoch 246/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1039 - mae: 0.2519\n",
      "Epoch 246: val_loss improved from 0.12871 to 0.12862, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1039 - mae: 0.2519 - val_loss: 0.1286 - val_mae: 0.2709\n",
      "Epoch 247/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1037 - mae: 0.2517\n",
      "Epoch 247: val_loss improved from 0.12862 to 0.12852, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1037 - mae: 0.2517 - val_loss: 0.1285 - val_mae: 0.2708\n",
      "Epoch 248/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1039 - mae: 0.2519\n",
      "Epoch 248: val_loss improved from 0.12852 to 0.12843, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1039 - mae: 0.2519 - val_loss: 0.1284 - val_mae: 0.2707\n",
      "Epoch 249/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1034 - mae: 0.2514\n",
      "Epoch 249: val_loss improved from 0.12843 to 0.12833, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1034 - mae: 0.2514 - val_loss: 0.1283 - val_mae: 0.2706\n",
      "Epoch 250/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1035 - mae: 0.2515\n",
      "Epoch 250: val_loss improved from 0.12833 to 0.12824, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1035 - mae: 0.2515 - val_loss: 0.1282 - val_mae: 0.2705\n",
      "Epoch 251/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1034 - mae: 0.2514\n",
      "Epoch 251: val_loss improved from 0.12824 to 0.12816, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1034 - mae: 0.2514 - val_loss: 0.1282 - val_mae: 0.2704\n",
      "Epoch 252/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1032 - mae: 0.2512\n",
      "Epoch 252: val_loss improved from 0.12816 to 0.12807, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1032 - mae: 0.2512 - val_loss: 0.1281 - val_mae: 0.2702\n",
      "Epoch 253/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1032 - mae: 0.2510\n",
      "Epoch 253: val_loss improved from 0.12807 to 0.12799, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1032 - mae: 0.2510 - val_loss: 0.1280 - val_mae: 0.2701\n",
      "Epoch 254/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1032 - mae: 0.2511\n",
      "Epoch 254: val_loss improved from 0.12799 to 0.12791, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1032 - mae: 0.2511 - val_loss: 0.1279 - val_mae: 0.2700\n",
      "Epoch 255/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1028 - mae: 0.2506\n",
      "Epoch 255: val_loss improved from 0.12791 to 0.12783, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1028 - mae: 0.2506 - val_loss: 0.1278 - val_mae: 0.2699\n",
      "Epoch 256/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1028 - mae: 0.2506\n",
      "Epoch 256: val_loss improved from 0.12783 to 0.12775, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1028 - mae: 0.2506 - val_loss: 0.1278 - val_mae: 0.2698\n",
      "Epoch 257/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1029 - mae: 0.2508\n",
      "Epoch 257: val_loss improved from 0.12775 to 0.12766, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1029 - mae: 0.2508 - val_loss: 0.1277 - val_mae: 0.2697\n",
      "Epoch 258/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1027 - mae: 0.2505\n",
      "Epoch 258: val_loss improved from 0.12766 to 0.12757, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1027 - mae: 0.2505 - val_loss: 0.1276 - val_mae: 0.2696\n",
      "Epoch 259/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1025 - mae: 0.2502\n",
      "Epoch 259: val_loss improved from 0.12757 to 0.12749, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1025 - mae: 0.2502 - val_loss: 0.1275 - val_mae: 0.2695\n",
      "Epoch 260/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1025 - mae: 0.2502\n",
      "Epoch 260: val_loss improved from 0.12749 to 0.12740, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1025 - mae: 0.2502 - val_loss: 0.1274 - val_mae: 0.2693\n",
      "Epoch 261/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1022 - mae: 0.2499\n",
      "Epoch 261: val_loss improved from 0.12740 to 0.12731, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1022 - mae: 0.2499 - val_loss: 0.1273 - val_mae: 0.2692\n",
      "Epoch 262/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1022 - mae: 0.2498\n",
      "Epoch 262: val_loss improved from 0.12731 to 0.12722, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1022 - mae: 0.2498 - val_loss: 0.1272 - val_mae: 0.2691\n",
      "Epoch 263/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1019 - mae: 0.2495\n",
      "Epoch 263: val_loss improved from 0.12722 to 0.12712, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.1019 - mae: 0.2495 - val_loss: 0.1271 - val_mae: 0.2690\n",
      "Epoch 264/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1022 - mae: 0.2497\n",
      "Epoch 264: val_loss improved from 0.12712 to 0.12701, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1022 - mae: 0.2497 - val_loss: 0.1270 - val_mae: 0.2689\n",
      "Epoch 265/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1019 - mae: 0.2495\n",
      "Epoch 265: val_loss improved from 0.12701 to 0.12689, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1019 - mae: 0.2495 - val_loss: 0.1269 - val_mae: 0.2687\n",
      "Epoch 266/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1018 - mae: 0.2494\n",
      "Epoch 266: val_loss improved from 0.12689 to 0.12678, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1018 - mae: 0.2494 - val_loss: 0.1268 - val_mae: 0.2686\n",
      "Epoch 267/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1017 - mae: 0.2493\n",
      "Epoch 267: val_loss improved from 0.12678 to 0.12667, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1017 - mae: 0.2493 - val_loss: 0.1267 - val_mae: 0.2685\n",
      "Epoch 268/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1017 - mae: 0.2491\n",
      "Epoch 268: val_loss improved from 0.12667 to 0.12657, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1017 - mae: 0.2491 - val_loss: 0.1266 - val_mae: 0.2684\n",
      "Epoch 269/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1015 - mae: 0.2490\n",
      "Epoch 269: val_loss improved from 0.12657 to 0.12647, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1015 - mae: 0.2490 - val_loss: 0.1265 - val_mae: 0.2683\n",
      "Epoch 270/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1015 - mae: 0.2489\n",
      "Epoch 270: val_loss improved from 0.12647 to 0.12637, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1015 - mae: 0.2489 - val_loss: 0.1264 - val_mae: 0.2681\n",
      "Epoch 271/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1012 - mae: 0.2487\n",
      "Epoch 271: val_loss improved from 0.12637 to 0.12627, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.1012 - mae: 0.2487 - val_loss: 0.1263 - val_mae: 0.2680\n",
      "Epoch 272/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1012 - mae: 0.2486\n",
      "Epoch 272: val_loss improved from 0.12627 to 0.12618, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1012 - mae: 0.2486 - val_loss: 0.1262 - val_mae: 0.2679\n",
      "Epoch 273/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1011 - mae: 0.2484\n",
      "Epoch 273: val_loss improved from 0.12618 to 0.12609, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1011 - mae: 0.2484 - val_loss: 0.1261 - val_mae: 0.2678\n",
      "Epoch 274/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1010 - mae: 0.2483\n",
      "Epoch 274: val_loss improved from 0.12609 to 0.12601, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1010 - mae: 0.2483 - val_loss: 0.1260 - val_mae: 0.2676\n",
      "Epoch 275/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1008 - mae: 0.2482\n",
      "Epoch 275: val_loss improved from 0.12601 to 0.12593, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1008 - mae: 0.2482 - val_loss: 0.1259 - val_mae: 0.2675\n",
      "Epoch 276/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1008 - mae: 0.2481\n",
      "Epoch 276: val_loss improved from 0.12593 to 0.12586, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1008 - mae: 0.2481 - val_loss: 0.1259 - val_mae: 0.2674\n",
      "Epoch 277/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1008 - mae: 0.2481\n",
      "Epoch 277: val_loss improved from 0.12586 to 0.12579, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1008 - mae: 0.2481 - val_loss: 0.1258 - val_mae: 0.2672\n",
      "Epoch 278/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1007 - mae: 0.2480\n",
      "Epoch 278: val_loss improved from 0.12579 to 0.12573, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1007 - mae: 0.2480 - val_loss: 0.1257 - val_mae: 0.2671\n",
      "Epoch 279/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1004 - mae: 0.2476\n",
      "Epoch 279: val_loss improved from 0.12573 to 0.12566, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1004 - mae: 0.2476 - val_loss: 0.1257 - val_mae: 0.2670\n",
      "Epoch 280/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1002 - mae: 0.2475\n",
      "Epoch 280: val_loss improved from 0.12566 to 0.12559, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1002 - mae: 0.2475 - val_loss: 0.1256 - val_mae: 0.2669\n",
      "Epoch 281/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1003 - mae: 0.2475\n",
      "Epoch 281: val_loss improved from 0.12559 to 0.12551, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1003 - mae: 0.2475 - val_loss: 0.1255 - val_mae: 0.2667\n",
      "Epoch 282/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1002 - mae: 0.2474\n",
      "Epoch 282: val_loss improved from 0.12551 to 0.12543, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.1002 - mae: 0.2474 - val_loss: 0.1254 - val_mae: 0.2666\n",
      "Epoch 283/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0997 - mae: 0.2469\n",
      "Epoch 283: val_loss improved from 0.12543 to 0.12535, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0997 - mae: 0.2469 - val_loss: 0.1254 - val_mae: 0.2665\n",
      "Epoch 284/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1000 - mae: 0.2471\n",
      "Epoch 284: val_loss improved from 0.12535 to 0.12526, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1000 - mae: 0.2471 - val_loss: 0.1253 - val_mae: 0.2664\n",
      "Epoch 285/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0998 - mae: 0.2468\n",
      "Epoch 285: val_loss improved from 0.12526 to 0.12516, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0998 - mae: 0.2468 - val_loss: 0.1252 - val_mae: 0.2663\n",
      "Epoch 286/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0997 - mae: 0.2467\n",
      "Epoch 286: val_loss improved from 0.12516 to 0.12507, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0997 - mae: 0.2467 - val_loss: 0.1251 - val_mae: 0.2661\n",
      "Epoch 287/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0996 - mae: 0.2467\n",
      "Epoch 287: val_loss improved from 0.12507 to 0.12498, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0996 - mae: 0.2467 - val_loss: 0.1250 - val_mae: 0.2660\n",
      "Epoch 288/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0995 - mae: 0.2464\n",
      "Epoch 288: val_loss improved from 0.12498 to 0.12491, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0995 - mae: 0.2464 - val_loss: 0.1249 - val_mae: 0.2659\n",
      "Epoch 289/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0993 - mae: 0.2462\n",
      "Epoch 289: val_loss improved from 0.12491 to 0.12486, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0993 - mae: 0.2462 - val_loss: 0.1249 - val_mae: 0.2658\n",
      "Epoch 290/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0992 - mae: 0.2461\n",
      "Epoch 290: val_loss improved from 0.12486 to 0.12482, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0992 - mae: 0.2461 - val_loss: 0.1248 - val_mae: 0.2657\n",
      "Epoch 291/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0993 - mae: 0.2462\n",
      "Epoch 291: val_loss improved from 0.12482 to 0.12477, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0993 - mae: 0.2462 - val_loss: 0.1248 - val_mae: 0.2655\n",
      "Epoch 292/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0989 - mae: 0.2458\n",
      "Epoch 292: val_loss improved from 0.12477 to 0.12473, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0989 - mae: 0.2458 - val_loss: 0.1247 - val_mae: 0.2654\n",
      "Epoch 293/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0990 - mae: 0.2459\n",
      "Epoch 293: val_loss improved from 0.12473 to 0.12467, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0990 - mae: 0.2459 - val_loss: 0.1247 - val_mae: 0.2653\n",
      "Epoch 294/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0986 - mae: 0.2454\n",
      "Epoch 294: val_loss improved from 0.12467 to 0.12460, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0986 - mae: 0.2454 - val_loss: 0.1246 - val_mae: 0.2652\n",
      "Epoch 295/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0986 - mae: 0.2453\n",
      "Epoch 295: val_loss improved from 0.12460 to 0.12453, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0986 - mae: 0.2453 - val_loss: 0.1245 - val_mae: 0.2650\n",
      "Epoch 296/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0984 - mae: 0.2451\n",
      "Epoch 296: val_loss improved from 0.12453 to 0.12445, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0984 - mae: 0.2451 - val_loss: 0.1244 - val_mae: 0.2649\n",
      "Epoch 297/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0983 - mae: 0.2450\n",
      "Epoch 297: val_loss improved from 0.12445 to 0.12436, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0983 - mae: 0.2450 - val_loss: 0.1244 - val_mae: 0.2648\n",
      "Epoch 298/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0982 - mae: 0.2450\n",
      "Epoch 298: val_loss improved from 0.12436 to 0.12428, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0982 - mae: 0.2450 - val_loss: 0.1243 - val_mae: 0.2647\n",
      "Epoch 299/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0980 - mae: 0.2446\n",
      "Epoch 299: val_loss improved from 0.12428 to 0.12419, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0980 - mae: 0.2446 - val_loss: 0.1242 - val_mae: 0.2645\n",
      "Epoch 300/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0978 - mae: 0.2444\n",
      "Epoch 300: val_loss improved from 0.12419 to 0.12408, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0978 - mae: 0.2444 - val_loss: 0.1241 - val_mae: 0.2644\n",
      "Epoch 301/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0977 - mae: 0.2444\n",
      "Epoch 301: val_loss improved from 0.12408 to 0.12396, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0977 - mae: 0.2444 - val_loss: 0.1240 - val_mae: 0.2642\n",
      "Epoch 302/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0978 - mae: 0.2444\n",
      "Epoch 302: val_loss improved from 0.12396 to 0.12384, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0978 - mae: 0.2444 - val_loss: 0.1238 - val_mae: 0.2641\n",
      "Epoch 303/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0974 - mae: 0.2440\n",
      "Epoch 303: val_loss improved from 0.12384 to 0.12371, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0974 - mae: 0.2440 - val_loss: 0.1237 - val_mae: 0.2639\n",
      "Epoch 304/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0973 - mae: 0.2438\n",
      "Epoch 304: val_loss improved from 0.12371 to 0.12358, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0973 - mae: 0.2438 - val_loss: 0.1236 - val_mae: 0.2638\n",
      "Epoch 305/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0972 - mae: 0.2437\n",
      "Epoch 305: val_loss improved from 0.12358 to 0.12346, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0972 - mae: 0.2437 - val_loss: 0.1235 - val_mae: 0.2636\n",
      "Epoch 306/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0972 - mae: 0.2436\n",
      "Epoch 306: val_loss improved from 0.12346 to 0.12333, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0972 - mae: 0.2436 - val_loss: 0.1233 - val_mae: 0.2634\n",
      "Epoch 307/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0970 - mae: 0.2434\n",
      "Epoch 307: val_loss improved from 0.12333 to 0.12322, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0970 - mae: 0.2434 - val_loss: 0.1232 - val_mae: 0.2633\n",
      "Epoch 308/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0969 - mae: 0.2432\n",
      "Epoch 308: val_loss improved from 0.12322 to 0.12310, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0969 - mae: 0.2432 - val_loss: 0.1231 - val_mae: 0.2631\n",
      "Epoch 309/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0968 - mae: 0.2432\n",
      "Epoch 309: val_loss improved from 0.12310 to 0.12300, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0968 - mae: 0.2432 - val_loss: 0.1230 - val_mae: 0.2629\n",
      "Epoch 310/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0966 - mae: 0.2431\n",
      "Epoch 310: val_loss improved from 0.12300 to 0.12291, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0966 - mae: 0.2431 - val_loss: 0.1229 - val_mae: 0.2628\n",
      "Epoch 311/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0964 - mae: 0.2427\n",
      "Epoch 311: val_loss improved from 0.12291 to 0.12282, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0964 - mae: 0.2427 - val_loss: 0.1228 - val_mae: 0.2626\n",
      "Epoch 312/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0962 - mae: 0.2425\n",
      "Epoch 312: val_loss improved from 0.12282 to 0.12271, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0962 - mae: 0.2425 - val_loss: 0.1227 - val_mae: 0.2625\n",
      "Epoch 313/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0962 - mae: 0.2425\n",
      "Epoch 313: val_loss improved from 0.12271 to 0.12260, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0962 - mae: 0.2425 - val_loss: 0.1226 - val_mae: 0.2623\n",
      "Epoch 314/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0960 - mae: 0.2423\n",
      "Epoch 314: val_loss improved from 0.12260 to 0.12250, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0960 - mae: 0.2423 - val_loss: 0.1225 - val_mae: 0.2622\n",
      "Epoch 315/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0958 - mae: 0.2421\n",
      "Epoch 315: val_loss improved from 0.12250 to 0.12240, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0958 - mae: 0.2421 - val_loss: 0.1224 - val_mae: 0.2620\n",
      "Epoch 316/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0956 - mae: 0.2418\n",
      "Epoch 316: val_loss improved from 0.12240 to 0.12230, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0956 - mae: 0.2418 - val_loss: 0.1223 - val_mae: 0.2618\n",
      "Epoch 317/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0958 - mae: 0.2420\n",
      "Epoch 317: val_loss improved from 0.12230 to 0.12220, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0958 - mae: 0.2420 - val_loss: 0.1222 - val_mae: 0.2617\n",
      "Epoch 318/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0955 - mae: 0.2416\n",
      "Epoch 318: val_loss improved from 0.12220 to 0.12210, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0955 - mae: 0.2416 - val_loss: 0.1221 - val_mae: 0.2615\n",
      "Epoch 319/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0954 - mae: 0.2415\n",
      "Epoch 319: val_loss improved from 0.12210 to 0.12199, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0954 - mae: 0.2415 - val_loss: 0.1220 - val_mae: 0.2613\n",
      "Epoch 320/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0952 - mae: 0.2412\n",
      "Epoch 320: val_loss improved from 0.12199 to 0.12186, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0952 - mae: 0.2412 - val_loss: 0.1219 - val_mae: 0.2611\n",
      "Epoch 321/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0950 - mae: 0.2410\n",
      "Epoch 321: val_loss improved from 0.12186 to 0.12174, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0950 - mae: 0.2410 - val_loss: 0.1217 - val_mae: 0.2609\n",
      "Epoch 322/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0950 - mae: 0.2410\n",
      "Epoch 322: val_loss improved from 0.12174 to 0.12164, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0950 - mae: 0.2410 - val_loss: 0.1216 - val_mae: 0.2607\n",
      "Epoch 323/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0948 - mae: 0.2408\n",
      "Epoch 323: val_loss improved from 0.12164 to 0.12152, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0948 - mae: 0.2408 - val_loss: 0.1215 - val_mae: 0.2605\n",
      "Epoch 324/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0947 - mae: 0.2407\n",
      "Epoch 324: val_loss improved from 0.12152 to 0.12140, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0947 - mae: 0.2407 - val_loss: 0.1214 - val_mae: 0.2603\n",
      "Epoch 325/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0944 - mae: 0.2402\n",
      "Epoch 325: val_loss improved from 0.12140 to 0.12130, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0944 - mae: 0.2402 - val_loss: 0.1213 - val_mae: 0.2602\n",
      "Epoch 326/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0944 - mae: 0.2403\n",
      "Epoch 326: val_loss improved from 0.12130 to 0.12119, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0944 - mae: 0.2403 - val_loss: 0.1212 - val_mae: 0.2600\n",
      "Epoch 327/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0940 - mae: 0.2398\n",
      "Epoch 327: val_loss improved from 0.12119 to 0.12106, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0940 - mae: 0.2398 - val_loss: 0.1211 - val_mae: 0.2598\n",
      "Epoch 328/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0939 - mae: 0.2397\n",
      "Epoch 328: val_loss improved from 0.12106 to 0.12096, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0939 - mae: 0.2397 - val_loss: 0.1210 - val_mae: 0.2596\n",
      "Epoch 329/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0935 - mae: 0.2392\n",
      "Epoch 329: val_loss improved from 0.12096 to 0.12088, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0935 - mae: 0.2392 - val_loss: 0.1209 - val_mae: 0.2594\n",
      "Epoch 330/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0936 - mae: 0.2394\n",
      "Epoch 330: val_loss improved from 0.12088 to 0.12082, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0936 - mae: 0.2394 - val_loss: 0.1208 - val_mae: 0.2593\n",
      "Epoch 331/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0934 - mae: 0.2390\n",
      "Epoch 331: val_loss improved from 0.12082 to 0.12076, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0934 - mae: 0.2390 - val_loss: 0.1208 - val_mae: 0.2590\n",
      "Epoch 332/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0932 - mae: 0.2389\n",
      "Epoch 332: val_loss improved from 0.12076 to 0.12068, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0932 - mae: 0.2389 - val_loss: 0.1207 - val_mae: 0.2588\n",
      "Epoch 333/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0930 - mae: 0.2386\n",
      "Epoch 333: val_loss improved from 0.12068 to 0.12060, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0930 - mae: 0.2386 - val_loss: 0.1206 - val_mae: 0.2586\n",
      "Epoch 334/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0929 - mae: 0.2386\n",
      "Epoch 334: val_loss improved from 0.12060 to 0.12050, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0929 - mae: 0.2386 - val_loss: 0.1205 - val_mae: 0.2584\n",
      "Epoch 335/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0928 - mae: 0.2383\n",
      "Epoch 335: val_loss improved from 0.12050 to 0.12037, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0928 - mae: 0.2383 - val_loss: 0.1204 - val_mae: 0.2582\n",
      "Epoch 336/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0927 - mae: 0.2382\n",
      "Epoch 336: val_loss improved from 0.12037 to 0.12023, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0927 - mae: 0.2382 - val_loss: 0.1202 - val_mae: 0.2580\n",
      "Epoch 337/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0925 - mae: 0.2379\n",
      "Epoch 337: val_loss improved from 0.12023 to 0.12007, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0925 - mae: 0.2379 - val_loss: 0.1201 - val_mae: 0.2578\n",
      "Epoch 338/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0923 - mae: 0.2378\n",
      "Epoch 338: val_loss improved from 0.12007 to 0.11992, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0923 - mae: 0.2378 - val_loss: 0.1199 - val_mae: 0.2576\n",
      "Epoch 339/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0920 - mae: 0.2373\n",
      "Epoch 339: val_loss improved from 0.11992 to 0.11977, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0920 - mae: 0.2373 - val_loss: 0.1198 - val_mae: 0.2574\n",
      "Epoch 340/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0919 - mae: 0.2372\n",
      "Epoch 340: val_loss improved from 0.11977 to 0.11961, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0919 - mae: 0.2372 - val_loss: 0.1196 - val_mae: 0.2572\n",
      "Epoch 341/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0918 - mae: 0.2370\n",
      "Epoch 341: val_loss improved from 0.11961 to 0.11944, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0918 - mae: 0.2370 - val_loss: 0.1194 - val_mae: 0.2569\n",
      "Epoch 342/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0915 - mae: 0.2367\n",
      "Epoch 342: val_loss improved from 0.11944 to 0.11928, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0915 - mae: 0.2367 - val_loss: 0.1193 - val_mae: 0.2567\n",
      "Epoch 343/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0913 - mae: 0.2365\n",
      "Epoch 343: val_loss improved from 0.11928 to 0.11913, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0913 - mae: 0.2365 - val_loss: 0.1191 - val_mae: 0.2566\n",
      "Epoch 344/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0911 - mae: 0.2362\n",
      "Epoch 344: val_loss improved from 0.11913 to 0.11895, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0911 - mae: 0.2362 - val_loss: 0.1190 - val_mae: 0.2562\n",
      "Epoch 345/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0911 - mae: 0.2361\n",
      "Epoch 345: val_loss improved from 0.11895 to 0.11880, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0911 - mae: 0.2361 - val_loss: 0.1188 - val_mae: 0.2560\n",
      "Epoch 346/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0908 - mae: 0.2358\n",
      "Epoch 346: val_loss improved from 0.11880 to 0.11868, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0908 - mae: 0.2358 - val_loss: 0.1187 - val_mae: 0.2558\n",
      "Epoch 347/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0904 - mae: 0.2353\n",
      "Epoch 347: val_loss improved from 0.11868 to 0.11853, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0904 - mae: 0.2353 - val_loss: 0.1185 - val_mae: 0.2555\n",
      "Epoch 348/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0904 - mae: 0.2353\n",
      "Epoch 348: val_loss improved from 0.11853 to 0.11836, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0904 - mae: 0.2353 - val_loss: 0.1184 - val_mae: 0.2552\n",
      "Epoch 349/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0900 - mae: 0.2349\n",
      "Epoch 349: val_loss improved from 0.11836 to 0.11816, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0900 - mae: 0.2349 - val_loss: 0.1182 - val_mae: 0.2549\n",
      "Epoch 350/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0902 - mae: 0.2349\n",
      "Epoch 350: val_loss improved from 0.11816 to 0.11790, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0902 - mae: 0.2349 - val_loss: 0.1179 - val_mae: 0.2547\n",
      "Epoch 351/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0897 - mae: 0.2343\n",
      "Epoch 351: val_loss improved from 0.11790 to 0.11760, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0897 - mae: 0.2343 - val_loss: 0.1176 - val_mae: 0.2543\n",
      "Epoch 352/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0897 - mae: 0.2343\n",
      "Epoch 352: val_loss improved from 0.11760 to 0.11729, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0897 - mae: 0.2343 - val_loss: 0.1173 - val_mae: 0.2540\n",
      "Epoch 353/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0892 - mae: 0.2337\n",
      "Epoch 353: val_loss improved from 0.11729 to 0.11699, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0892 - mae: 0.2337 - val_loss: 0.1170 - val_mae: 0.2537\n",
      "Epoch 354/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0892 - mae: 0.2336\n",
      "Epoch 354: val_loss improved from 0.11699 to 0.11668, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0892 - mae: 0.2336 - val_loss: 0.1167 - val_mae: 0.2533\n",
      "Epoch 355/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0890 - mae: 0.2333\n",
      "Epoch 355: val_loss improved from 0.11668 to 0.11638, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.0890 - mae: 0.2333 - val_loss: 0.1164 - val_mae: 0.2530\n",
      "Epoch 356/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0887 - mae: 0.2330\n",
      "Epoch 356: val_loss improved from 0.11638 to 0.11608, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0887 - mae: 0.2330 - val_loss: 0.1161 - val_mae: 0.2526\n",
      "Epoch 357/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0883 - mae: 0.2325\n",
      "Epoch 357: val_loss improved from 0.11608 to 0.11580, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0883 - mae: 0.2325 - val_loss: 0.1158 - val_mae: 0.2523\n",
      "Epoch 358/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0883 - mae: 0.2324\n",
      "Epoch 358: val_loss improved from 0.11580 to 0.11550, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0883 - mae: 0.2324 - val_loss: 0.1155 - val_mae: 0.2519\n",
      "Epoch 359/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0879 - mae: 0.2320\n",
      "Epoch 359: val_loss improved from 0.11550 to 0.11518, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0879 - mae: 0.2320 - val_loss: 0.1152 - val_mae: 0.2515\n",
      "Epoch 360/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0878 - mae: 0.2318\n",
      "Epoch 360: val_loss improved from 0.11518 to 0.11481, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0878 - mae: 0.2318 - val_loss: 0.1148 - val_mae: 0.2511\n",
      "Epoch 361/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0877 - mae: 0.2316\n",
      "Epoch 361: val_loss improved from 0.11481 to 0.11436, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0877 - mae: 0.2316 - val_loss: 0.1144 - val_mae: 0.2507\n",
      "Epoch 362/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0872 - mae: 0.2311\n",
      "Epoch 362: val_loss improved from 0.11436 to 0.11395, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0872 - mae: 0.2311 - val_loss: 0.1139 - val_mae: 0.2504\n",
      "Epoch 363/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0871 - mae: 0.2308\n",
      "Epoch 363: val_loss improved from 0.11395 to 0.11357, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0871 - mae: 0.2308 - val_loss: 0.1136 - val_mae: 0.2500\n",
      "Epoch 364/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0869 - mae: 0.2306\n",
      "Epoch 364: val_loss improved from 0.11357 to 0.11329, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0869 - mae: 0.2306 - val_loss: 0.1133 - val_mae: 0.2495\n",
      "Epoch 365/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0865 - mae: 0.2301\n",
      "Epoch 365: val_loss improved from 0.11329 to 0.11307, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0865 - mae: 0.2301 - val_loss: 0.1131 - val_mae: 0.2492\n",
      "Epoch 366/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0862 - mae: 0.2298\n",
      "Epoch 366: val_loss improved from 0.11307 to 0.11275, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0862 - mae: 0.2298 - val_loss: 0.1127 - val_mae: 0.2486\n",
      "Epoch 367/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0860 - mae: 0.2295\n",
      "Epoch 367: val_loss improved from 0.11275 to 0.11233, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0860 - mae: 0.2295 - val_loss: 0.1123 - val_mae: 0.2483\n",
      "Epoch 368/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0860 - mae: 0.2292\n",
      "Epoch 368: val_loss improved from 0.11233 to 0.11170, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0860 - mae: 0.2292 - val_loss: 0.1117 - val_mae: 0.2476\n",
      "Epoch 369/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0856 - mae: 0.2288\n",
      "Epoch 369: val_loss improved from 0.11170 to 0.11109, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0856 - mae: 0.2288 - val_loss: 0.1111 - val_mae: 0.2472\n",
      "Epoch 370/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0852 - mae: 0.2282\n",
      "Epoch 370: val_loss improved from 0.11109 to 0.11043, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0852 - mae: 0.2282 - val_loss: 0.1104 - val_mae: 0.2465\n",
      "Epoch 371/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0849 - mae: 0.2278\n",
      "Epoch 371: val_loss improved from 0.11043 to 0.10984, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0849 - mae: 0.2278 - val_loss: 0.1098 - val_mae: 0.2460\n",
      "Epoch 372/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0848 - mae: 0.2277\n",
      "Epoch 372: val_loss improved from 0.10984 to 0.10929, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0848 - mae: 0.2277 - val_loss: 0.1093 - val_mae: 0.2454\n",
      "Epoch 373/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0844 - mae: 0.2273\n",
      "Epoch 373: val_loss improved from 0.10929 to 0.10879, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0844 - mae: 0.2273 - val_loss: 0.1088 - val_mae: 0.2449\n",
      "Epoch 374/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0840 - mae: 0.2268\n",
      "Epoch 374: val_loss improved from 0.10879 to 0.10844, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0840 - mae: 0.2268 - val_loss: 0.1084 - val_mae: 0.2446\n",
      "Epoch 375/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0837 - mae: 0.2261\n",
      "Epoch 375: val_loss improved from 0.10844 to 0.10810, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0837 - mae: 0.2261 - val_loss: 0.1081 - val_mae: 0.2438\n",
      "Epoch 376/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0834 - mae: 0.2259\n",
      "Epoch 376: val_loss improved from 0.10810 to 0.10795, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0834 - mae: 0.2259 - val_loss: 0.1079 - val_mae: 0.2440\n",
      "Epoch 377/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0831 - mae: 0.2253\n",
      "Epoch 377: val_loss improved from 0.10795 to 0.10759, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0831 - mae: 0.2253 - val_loss: 0.1076 - val_mae: 0.2430\n",
      "Epoch 378/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0828 - mae: 0.2252\n",
      "Epoch 378: val_loss improved from 0.10759 to 0.10725, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0828 - mae: 0.2252 - val_loss: 0.1072 - val_mae: 0.2427\n",
      "Epoch 379/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0824 - mae: 0.2245\n",
      "Epoch 379: val_loss improved from 0.10725 to 0.10696, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0824 - mae: 0.2245 - val_loss: 0.1070 - val_mae: 0.2427\n",
      "Epoch 380/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0822 - mae: 0.2241\n",
      "Epoch 380: val_loss improved from 0.10696 to 0.10656, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0822 - mae: 0.2241 - val_loss: 0.1066 - val_mae: 0.2417\n",
      "Epoch 381/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0820 - mae: 0.2240\n",
      "Epoch 381: val_loss improved from 0.10656 to 0.10623, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0820 - mae: 0.2240 - val_loss: 0.1062 - val_mae: 0.2414\n",
      "Epoch 382/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0815 - mae: 0.2232\n",
      "Epoch 382: val_loss improved from 0.10623 to 0.10598, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0815 - mae: 0.2232 - val_loss: 0.1060 - val_mae: 0.2412\n",
      "Epoch 383/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0813 - mae: 0.2230\n",
      "Epoch 383: val_loss improved from 0.10598 to 0.10566, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0813 - mae: 0.2230 - val_loss: 0.1057 - val_mae: 0.2403\n",
      "Epoch 384/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0810 - mae: 0.2227\n",
      "Epoch 384: val_loss improved from 0.10566 to 0.10539, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0810 - mae: 0.2227 - val_loss: 0.1054 - val_mae: 0.2402\n",
      "Epoch 385/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0805 - mae: 0.2219\n",
      "Epoch 385: val_loss improved from 0.10539 to 0.10512, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0805 - mae: 0.2219 - val_loss: 0.1051 - val_mae: 0.2398\n",
      "Epoch 386/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0803 - mae: 0.2217\n",
      "Epoch 386: val_loss improved from 0.10512 to 0.10486, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0803 - mae: 0.2217 - val_loss: 0.1049 - val_mae: 0.2390\n",
      "Epoch 387/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0799 - mae: 0.2213\n",
      "Epoch 387: val_loss improved from 0.10486 to 0.10462, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0799 - mae: 0.2213 - val_loss: 0.1046 - val_mae: 0.2390\n",
      "Epoch 388/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0794 - mae: 0.2205\n",
      "Epoch 388: val_loss improved from 0.10462 to 0.10421, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0794 - mae: 0.2205 - val_loss: 0.1042 - val_mae: 0.2382\n",
      "Epoch 389/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0792 - mae: 0.2202\n",
      "Epoch 389: val_loss improved from 0.10421 to 0.10373, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0792 - mae: 0.2202 - val_loss: 0.1037 - val_mae: 0.2375\n",
      "Epoch 390/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0789 - mae: 0.2198\n",
      "Epoch 390: val_loss improved from 0.10373 to 0.10324, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0789 - mae: 0.2198 - val_loss: 0.1032 - val_mae: 0.2374\n",
      "Epoch 391/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0783 - mae: 0.2190\n",
      "Epoch 391: val_loss improved from 0.10324 to 0.10266, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0783 - mae: 0.2190 - val_loss: 0.1027 - val_mae: 0.2364\n",
      "Epoch 392/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0782 - mae: 0.2188\n",
      "Epoch 392: val_loss improved from 0.10266 to 0.10218, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0782 - mae: 0.2188 - val_loss: 0.1022 - val_mae: 0.2359\n",
      "Epoch 393/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0778 - mae: 0.2182\n",
      "Epoch 393: val_loss improved from 0.10218 to 0.10173, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0778 - mae: 0.2182 - val_loss: 0.1017 - val_mae: 0.2352\n",
      "Epoch 394/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0773 - mae: 0.2176\n",
      "Epoch 394: val_loss improved from 0.10173 to 0.10121, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0773 - mae: 0.2176 - val_loss: 0.1012 - val_mae: 0.2344\n",
      "Epoch 395/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0769 - mae: 0.2171\n",
      "Epoch 395: val_loss improved from 0.10121 to 0.10068, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0769 - mae: 0.2171 - val_loss: 0.1007 - val_mae: 0.2341\n",
      "Epoch 396/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0766 - mae: 0.2165\n",
      "Epoch 396: val_loss improved from 0.10068 to 0.10006, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0766 - mae: 0.2165 - val_loss: 0.1001 - val_mae: 0.2329\n",
      "Epoch 397/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0762 - mae: 0.2163\n",
      "Epoch 397: val_loss improved from 0.10006 to 0.09958, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0762 - mae: 0.2163 - val_loss: 0.0996 - val_mae: 0.2330\n",
      "Epoch 398/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0759 - mae: 0.2154\n",
      "Epoch 398: val_loss improved from 0.09958 to 0.09892, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0759 - mae: 0.2154 - val_loss: 0.0989 - val_mae: 0.2314\n",
      "Epoch 399/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0757 - mae: 0.2156\n",
      "Epoch 399: val_loss improved from 0.09892 to 0.09854, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0757 - mae: 0.2156 - val_loss: 0.0985 - val_mae: 0.2318\n",
      "Epoch 400/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0753 - mae: 0.2145\n",
      "Epoch 400: val_loss improved from 0.09854 to 0.09769, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0753 - mae: 0.2145 - val_loss: 0.0977 - val_mae: 0.2300\n",
      "Epoch 401/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0748 - mae: 0.2141\n",
      "Epoch 401: val_loss improved from 0.09769 to 0.09721, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0748 - mae: 0.2141 - val_loss: 0.0972 - val_mae: 0.2298\n",
      "Epoch 402/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0742 - mae: 0.2130\n",
      "Epoch 402: val_loss improved from 0.09721 to 0.09663, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0742 - mae: 0.2130 - val_loss: 0.0966 - val_mae: 0.2289\n",
      "Epoch 403/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0737 - mae: 0.2124\n",
      "Epoch 403: val_loss improved from 0.09663 to 0.09608, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0737 - mae: 0.2124 - val_loss: 0.0961 - val_mae: 0.2278\n",
      "Epoch 404/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0733 - mae: 0.2120\n",
      "Epoch 404: val_loss improved from 0.09608 to 0.09580, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0733 - mae: 0.2120 - val_loss: 0.0958 - val_mae: 0.2279\n",
      "Epoch 405/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0730 - mae: 0.2113\n",
      "Epoch 405: val_loss improved from 0.09580 to 0.09507, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0730 - mae: 0.2113 - val_loss: 0.0951 - val_mae: 0.2262\n",
      "Epoch 406/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0725 - mae: 0.2110\n",
      "Epoch 406: val_loss improved from 0.09507 to 0.09481, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0725 - mae: 0.2110 - val_loss: 0.0948 - val_mae: 0.2269\n",
      "Epoch 407/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0721 - mae: 0.2100\n",
      "Epoch 407: val_loss improved from 0.09481 to 0.09387, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0721 - mae: 0.2100 - val_loss: 0.0939 - val_mae: 0.2247\n",
      "Epoch 408/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0717 - mae: 0.2097\n",
      "Epoch 408: val_loss improved from 0.09387 to 0.09360, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0717 - mae: 0.2097 - val_loss: 0.0936 - val_mae: 0.2250\n",
      "Epoch 409/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0711 - mae: 0.2085\n",
      "Epoch 409: val_loss improved from 0.09360 to 0.09280, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0711 - mae: 0.2085 - val_loss: 0.0928 - val_mae: 0.2231\n",
      "Epoch 410/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0707 - mae: 0.2080\n",
      "Epoch 410: val_loss improved from 0.09280 to 0.09229, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0707 - mae: 0.2080 - val_loss: 0.0923 - val_mae: 0.2227\n",
      "Epoch 411/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0700 - mae: 0.2068\n",
      "Epoch 411: val_loss improved from 0.09229 to 0.09160, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0700 - mae: 0.2068 - val_loss: 0.0916 - val_mae: 0.2219\n",
      "Epoch 412/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0696 - mae: 0.2060\n",
      "Epoch 412: val_loss improved from 0.09160 to 0.09081, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0696 - mae: 0.2060 - val_loss: 0.0908 - val_mae: 0.2206\n",
      "Epoch 413/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0691 - mae: 0.2054\n",
      "Epoch 413: val_loss improved from 0.09081 to 0.09058, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0691 - mae: 0.2054 - val_loss: 0.0906 - val_mae: 0.2211\n",
      "Epoch 414/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0688 - mae: 0.2046\n",
      "Epoch 414: val_loss improved from 0.09058 to 0.08981, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0688 - mae: 0.2046 - val_loss: 0.0898 - val_mae: 0.2188\n",
      "Epoch 415/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0688 - mae: 0.2051\n",
      "Epoch 415: val_loss did not improve from 0.08981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0688 - mae: 0.2051 - val_loss: 0.0905 - val_mae: 0.2220\n",
      "Epoch 416/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0690 - mae: 0.2051\n",
      "Epoch 416: val_loss improved from 0.08981 to 0.08910, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0690 - mae: 0.2051 - val_loss: 0.0891 - val_mae: 0.2179\n",
      "Epoch 417/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0686 - mae: 0.2048\n",
      "Epoch 417: val_loss improved from 0.08910 to 0.08859, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0686 - mae: 0.2048 - val_loss: 0.0886 - val_mae: 0.2186\n",
      "Epoch 418/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0671 - mae: 0.2019\n",
      "Epoch 418: val_loss improved from 0.08859 to 0.08742, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0671 - mae: 0.2019 - val_loss: 0.0874 - val_mae: 0.2161\n",
      "Epoch 419/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0659 - mae: 0.2002\n",
      "Epoch 419: val_loss improved from 0.08742 to 0.08718, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0659 - mae: 0.2002 - val_loss: 0.0872 - val_mae: 0.2149\n",
      "Epoch 420/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0663 - mae: 0.2010\n",
      "Epoch 420: val_loss did not improve from 0.08718\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0663 - mae: 0.2010 - val_loss: 0.0878 - val_mae: 0.2177\n",
      "Epoch 421/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0661 - mae: 0.2005\n",
      "Epoch 421: val_loss improved from 0.08718 to 0.08591, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0661 - mae: 0.2005 - val_loss: 0.0859 - val_mae: 0.2132\n",
      "Epoch 422/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0648 - mae: 0.1985\n",
      "Epoch 422: val_loss improved from 0.08591 to 0.08530, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0648 - mae: 0.1985 - val_loss: 0.0853 - val_mae: 0.2126\n",
      "Epoch 423/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0640 - mae: 0.1970\n",
      "Epoch 423: val_loss did not improve from 0.08530\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0640 - mae: 0.1970 - val_loss: 0.0856 - val_mae: 0.2141\n",
      "Epoch 424/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0641 - mae: 0.1972\n",
      "Epoch 424: val_loss improved from 0.08530 to 0.08459, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0641 - mae: 0.1972 - val_loss: 0.0846 - val_mae: 0.2111\n",
      "Epoch 425/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0638 - mae: 0.1970\n",
      "Epoch 425: val_loss improved from 0.08459 to 0.08431, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0638 - mae: 0.1970 - val_loss: 0.0843 - val_mae: 0.2119\n",
      "Epoch 426/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0629 - mae: 0.1952\n",
      "Epoch 426: val_loss improved from 0.08431 to 0.08315, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0629 - mae: 0.1952 - val_loss: 0.0831 - val_mae: 0.2095\n",
      "Epoch 427/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0620 - mae: 0.1938\n",
      "Epoch 427: val_loss improved from 0.08315 to 0.08265, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0620 - mae: 0.1938 - val_loss: 0.0827 - val_mae: 0.2083\n",
      "Epoch 428/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0619 - mae: 0.1936\n",
      "Epoch 428: val_loss did not improve from 0.08265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0619 - mae: 0.1936 - val_loss: 0.0831 - val_mae: 0.2105\n",
      "Epoch 429/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0617 - mae: 0.1935\n",
      "Epoch 429: val_loss improved from 0.08265 to 0.08167, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0617 - mae: 0.1935 - val_loss: 0.0817 - val_mae: 0.2068\n",
      "Epoch 430/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0611 - mae: 0.1925\n",
      "Epoch 430: val_loss improved from 0.08167 to 0.08118, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0611 - mae: 0.1925 - val_loss: 0.0812 - val_mae: 0.2069\n",
      "Epoch 431/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0601 - mae: 0.1907\n",
      "Epoch 431: val_loss improved from 0.08118 to 0.08040, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0601 - mae: 0.1907 - val_loss: 0.0804 - val_mae: 0.2056\n",
      "Epoch 432/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0593 - mae: 0.1895\n",
      "Epoch 432: val_loss improved from 0.08040 to 0.07963, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0593 - mae: 0.1895 - val_loss: 0.0796 - val_mae: 0.2040\n",
      "Epoch 433/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0593 - mae: 0.1894\n",
      "Epoch 433: val_loss did not improve from 0.07963\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0593 - mae: 0.1894 - val_loss: 0.0798 - val_mae: 0.2056\n",
      "Epoch 434/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0589 - mae: 0.1888\n",
      "Epoch 434: val_loss improved from 0.07963 to 0.07852, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0589 - mae: 0.1888 - val_loss: 0.0785 - val_mae: 0.2024\n",
      "Epoch 435/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0583 - mae: 0.1879\n",
      "Epoch 435: val_loss improved from 0.07852 to 0.07831, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0583 - mae: 0.1879 - val_loss: 0.0783 - val_mae: 0.2030\n",
      "Epoch 436/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0575 - mae: 0.1865\n",
      "Epoch 436: val_loss improved from 0.07831 to 0.07724, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0575 - mae: 0.1865 - val_loss: 0.0772 - val_mae: 0.2006\n",
      "Epoch 437/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0568 - mae: 0.1853\n",
      "Epoch 437: val_loss improved from 0.07724 to 0.07680, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0568 - mae: 0.1853 - val_loss: 0.0768 - val_mae: 0.1999\n",
      "Epoch 438/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0564 - mae: 0.1844\n",
      "Epoch 438: val_loss improved from 0.07680 to 0.07652, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0564 - mae: 0.1844 - val_loss: 0.0765 - val_mae: 0.1998\n",
      "Epoch 439/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0558 - mae: 0.1835\n",
      "Epoch 439: val_loss improved from 0.07652 to 0.07565, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0558 - mae: 0.1835 - val_loss: 0.0756 - val_mae: 0.1979\n",
      "Epoch 440/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0554 - mae: 0.1831\n",
      "Epoch 440: val_loss did not improve from 0.07565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0554 - mae: 0.1831 - val_loss: 0.0757 - val_mae: 0.1995\n",
      "Epoch 441/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0553 - mae: 0.1826\n",
      "Epoch 441: val_loss improved from 0.07565 to 0.07464, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0553 - mae: 0.1826 - val_loss: 0.0746 - val_mae: 0.1965\n",
      "Epoch 442/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0551 - mae: 0.1827\n",
      "Epoch 442: val_loss did not improve from 0.07464\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0551 - mae: 0.1827 - val_loss: 0.0756 - val_mae: 0.2001\n",
      "Epoch 443/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0552 - mae: 0.1828\n",
      "Epoch 443: val_loss improved from 0.07464 to 0.07432, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0552 - mae: 0.1828 - val_loss: 0.0743 - val_mae: 0.1959\n",
      "Epoch 444/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0552 - mae: 0.1833\n",
      "Epoch 444: val_loss did not improve from 0.07432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0552 - mae: 0.1833 - val_loss: 0.0750 - val_mae: 0.1994\n",
      "Epoch 445/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0545 - mae: 0.1818\n",
      "Epoch 445: val_loss improved from 0.07432 to 0.07273, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0545 - mae: 0.1818 - val_loss: 0.0727 - val_mae: 0.1933\n",
      "Epoch 446/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0530 - mae: 0.1794\n",
      "Epoch 446: val_loss improved from 0.07273 to 0.07219, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0530 - mae: 0.1794 - val_loss: 0.0722 - val_mae: 0.1932\n",
      "Epoch 447/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0519 - mae: 0.1767\n",
      "Epoch 447: val_loss improved from 0.07219 to 0.07158, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0519 - mae: 0.1767 - val_loss: 0.0716 - val_mae: 0.1918\n",
      "Epoch 448/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0511 - mae: 0.1754\n",
      "Epoch 448: val_loss improved from 0.07158 to 0.07108, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0511 - mae: 0.1754 - val_loss: 0.0711 - val_mae: 0.1904\n",
      "Epoch 449/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0512 - mae: 0.1760\n",
      "Epoch 449: val_loss did not improve from 0.07108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0512 - mae: 0.1760 - val_loss: 0.0718 - val_mae: 0.1937\n",
      "Epoch 450/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0514 - mae: 0.1762\n",
      "Epoch 450: val_loss improved from 0.07108 to 0.07048, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0514 - mae: 0.1762 - val_loss: 0.0705 - val_mae: 0.1896\n",
      "Epoch 451/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0511 - mae: 0.1761\n",
      "Epoch 451: val_loss did not improve from 0.07048\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0511 - mae: 0.1761 - val_loss: 0.0706 - val_mae: 0.1914\n",
      "Epoch 452/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0502 - mae: 0.1740\n",
      "Epoch 452: val_loss improved from 0.07048 to 0.06902, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0502 - mae: 0.1740 - val_loss: 0.0690 - val_mae: 0.1870\n",
      "Epoch 453/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0490 - mae: 0.1719\n",
      "Epoch 453: val_loss improved from 0.06902 to 0.06853, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0490 - mae: 0.1719 - val_loss: 0.0685 - val_mae: 0.1865\n",
      "Epoch 454/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0484 - mae: 0.1703\n",
      "Epoch 454: val_loss improved from 0.06853 to 0.06836, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0484 - mae: 0.1703 - val_loss: 0.0684 - val_mae: 0.1868\n",
      "Epoch 455/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0480 - mae: 0.1696\n",
      "Epoch 455: val_loss improved from 0.06836 to 0.06778, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0480 - mae: 0.1696 - val_loss: 0.0678 - val_mae: 0.1848\n",
      "Epoch 456/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0479 - mae: 0.1702\n",
      "Epoch 456: val_loss did not improve from 0.06778\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0479 - mae: 0.1702 - val_loss: 0.0685 - val_mae: 0.1879\n",
      "Epoch 457/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0479 - mae: 0.1700\n",
      "Epoch 457: val_loss improved from 0.06778 to 0.06729, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0479 - mae: 0.1700 - val_loss: 0.0673 - val_mae: 0.1838\n",
      "Epoch 458/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0478 - mae: 0.1702\n",
      "Epoch 458: val_loss did not improve from 0.06729\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0478 - mae: 0.1702 - val_loss: 0.0674 - val_mae: 0.1861\n",
      "Epoch 459/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0470 - mae: 0.1683\n",
      "Epoch 459: val_loss improved from 0.06729 to 0.06586, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0470 - mae: 0.1683 - val_loss: 0.0659 - val_mae: 0.1814\n",
      "Epoch 460/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0460 - mae: 0.1666\n",
      "Epoch 460: val_loss improved from 0.06586 to 0.06556, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0460 - mae: 0.1666 - val_loss: 0.0656 - val_mae: 0.1817\n",
      "Epoch 461/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0451 - mae: 0.1645\n",
      "Epoch 461: val_loss improved from 0.06556 to 0.06477, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0451 - mae: 0.1645 - val_loss: 0.0648 - val_mae: 0.1799\n",
      "Epoch 462/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0446 - mae: 0.1634\n",
      "Epoch 462: val_loss improved from 0.06477 to 0.06430, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0446 - mae: 0.1634 - val_loss: 0.0643 - val_mae: 0.1788\n",
      "Epoch 463/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0442 - mae: 0.1628\n",
      "Epoch 463: val_loss did not improve from 0.06430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0442 - mae: 0.1628 - val_loss: 0.0644 - val_mae: 0.1801\n",
      "Epoch 464/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0441 - mae: 0.1627\n",
      "Epoch 464: val_loss improved from 0.06430 to 0.06377, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0441 - mae: 0.1627 - val_loss: 0.0638 - val_mae: 0.1775\n",
      "Epoch 465/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0441 - mae: 0.1631\n",
      "Epoch 465: val_loss did not improve from 0.06377\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0441 - mae: 0.1631 - val_loss: 0.0645 - val_mae: 0.1813\n",
      "Epoch 466/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0445 - mae: 0.1636\n",
      "Epoch 466: val_loss improved from 0.06377 to 0.06370, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0445 - mae: 0.1636 - val_loss: 0.0637 - val_mae: 0.1775\n",
      "Epoch 467/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0445 - mae: 0.1646\n",
      "Epoch 467: val_loss did not improve from 0.06370\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0445 - mae: 0.1646 - val_loss: 0.0645 - val_mae: 0.1824\n",
      "Epoch 468/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0447 - mae: 0.1646\n",
      "Epoch 468: val_loss improved from 0.06370 to 0.06278, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0447 - mae: 0.1646 - val_loss: 0.0628 - val_mae: 0.1761\n",
      "Epoch 469/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0439 - mae: 0.1636\n",
      "Epoch 469: val_loss improved from 0.06278 to 0.06230, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0439 - mae: 0.1636 - val_loss: 0.0623 - val_mae: 0.1774\n",
      "Epoch 470/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0425 - mae: 0.1597\n",
      "Epoch 470: val_loss improved from 0.06230 to 0.06048, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0425 - mae: 0.1597 - val_loss: 0.0605 - val_mae: 0.1720\n",
      "Epoch 471/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0409 - mae: 0.1566\n",
      "Epoch 471: val_loss improved from 0.06048 to 0.05987, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0409 - mae: 0.1566 - val_loss: 0.0599 - val_mae: 0.1713\n",
      "Epoch 472/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0403 - mae: 0.1552\n",
      "Epoch 472: val_loss did not improve from 0.05987\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0403 - mae: 0.1552 - val_loss: 0.0599 - val_mae: 0.1723\n",
      "Epoch 473/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0401 - mae: 0.1550\n",
      "Epoch 473: val_loss improved from 0.05987 to 0.05949, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0401 - mae: 0.1550 - val_loss: 0.0595 - val_mae: 0.1703\n",
      "Epoch 474/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0407 - mae: 0.1566\n",
      "Epoch 474: val_loss did not improve from 0.05949\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0407 - mae: 0.1566 - val_loss: 0.0601 - val_mae: 0.1738\n",
      "Epoch 475/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0407 - mae: 0.1565\n",
      "Epoch 475: val_loss improved from 0.05949 to 0.05905, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0407 - mae: 0.1565 - val_loss: 0.0590 - val_mae: 0.1694\n",
      "Epoch 476/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0402 - mae: 0.1559\n",
      "Epoch 476: val_loss improved from 0.05905 to 0.05898, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0402 - mae: 0.1559 - val_loss: 0.0590 - val_mae: 0.1714\n",
      "Epoch 477/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0394 - mae: 0.1539\n",
      "Epoch 477: val_loss improved from 0.05898 to 0.05752, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0394 - mae: 0.1539 - val_loss: 0.0575 - val_mae: 0.1667\n",
      "Epoch 478/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0386 - mae: 0.1521\n",
      "Epoch 478: val_loss improved from 0.05752 to 0.05702, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0386 - mae: 0.1521 - val_loss: 0.0570 - val_mae: 0.1666\n",
      "Epoch 479/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0378 - mae: 0.1500\n",
      "Epoch 479: val_loss improved from 0.05702 to 0.05639, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0378 - mae: 0.1500 - val_loss: 0.0564 - val_mae: 0.1651\n",
      "Epoch 480/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0372 - mae: 0.1488\n",
      "Epoch 480: val_loss improved from 0.05639 to 0.05592, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0372 - mae: 0.1488 - val_loss: 0.0559 - val_mae: 0.1640\n",
      "Epoch 481/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0370 - mae: 0.1487\n",
      "Epoch 481: val_loss did not improve from 0.05592\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0370 - mae: 0.1487 - val_loss: 0.0561 - val_mae: 0.1657\n",
      "Epoch 482/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0370 - mae: 0.1486\n",
      "Epoch 482: val_loss improved from 0.05592 to 0.05586, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0370 - mae: 0.1486 - val_loss: 0.0559 - val_mae: 0.1633\n",
      "Epoch 483/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0371 - mae: 0.1494\n",
      "Epoch 483: val_loss did not improve from 0.05586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0371 - mae: 0.1494 - val_loss: 0.0564 - val_mae: 0.1670\n",
      "Epoch 484/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0373 - mae: 0.1497\n",
      "Epoch 484: val_loss improved from 0.05586 to 0.05572, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0373 - mae: 0.1497 - val_loss: 0.0557 - val_mae: 0.1632\n",
      "Epoch 485/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0374 - mae: 0.1505\n",
      "Epoch 485: val_loss did not improve from 0.05572\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0374 - mae: 0.1505 - val_loss: 0.0561 - val_mae: 0.1674\n",
      "Epoch 486/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0373 - mae: 0.1501\n",
      "Epoch 486: val_loss improved from 0.05572 to 0.05503, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0373 - mae: 0.1501 - val_loss: 0.0550 - val_mae: 0.1619\n",
      "Epoch 487/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0368 - mae: 0.1492\n",
      "Epoch 487: val_loss improved from 0.05503 to 0.05475, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0368 - mae: 0.1492 - val_loss: 0.0547 - val_mae: 0.1641\n",
      "Epoch 488/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0358 - mae: 0.1467\n",
      "Epoch 488: val_loss improved from 0.05475 to 0.05310, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0358 - mae: 0.1467 - val_loss: 0.0531 - val_mae: 0.1584\n",
      "Epoch 489/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0347 - mae: 0.1443\n",
      "Epoch 489: val_loss improved from 0.05310 to 0.05241, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0347 - mae: 0.1443 - val_loss: 0.0524 - val_mae: 0.1584\n",
      "Epoch 490/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0339 - mae: 0.1418\n",
      "Epoch 490: val_loss improved from 0.05241 to 0.05181, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0339 - mae: 0.1418 - val_loss: 0.0518 - val_mae: 0.1564\n",
      "Epoch 491/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0333 - mae: 0.1405\n",
      "Epoch 491: val_loss improved from 0.05181 to 0.05153, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0333 - mae: 0.1405 - val_loss: 0.0515 - val_mae: 0.1554\n",
      "Epoch 492/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0330 - mae: 0.1400\n",
      "Epoch 492: val_loss did not improve from 0.05153\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0330 - mae: 0.1400 - val_loss: 0.0517 - val_mae: 0.1572\n",
      "Epoch 493/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0332 - mae: 0.1404\n",
      "Epoch 493: val_loss improved from 0.05153 to 0.05142, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0332 - mae: 0.1404 - val_loss: 0.0514 - val_mae: 0.1551\n",
      "Epoch 494/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0335 - mae: 0.1415\n",
      "Epoch 494: val_loss did not improve from 0.05142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0335 - mae: 0.1415 - val_loss: 0.0520 - val_mae: 0.1593\n",
      "Epoch 495/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0339 - mae: 0.1425\n",
      "Epoch 495: val_loss did not improve from 0.05142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0339 - mae: 0.1425 - val_loss: 0.0517 - val_mae: 0.1557\n",
      "Epoch 496/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0343 - mae: 0.1439\n",
      "Epoch 496: val_loss did not improve from 0.05142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0343 - mae: 0.1439 - val_loss: 0.0522 - val_mae: 0.1607\n",
      "Epoch 497/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0342 - mae: 0.1437\n",
      "Epoch 497: val_loss improved from 0.05142 to 0.05107, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0342 - mae: 0.1437 - val_loss: 0.0511 - val_mae: 0.1546\n",
      "Epoch 498/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0337 - mae: 0.1427\n",
      "Epoch 498: val_loss improved from 0.05107 to 0.05053, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0337 - mae: 0.1427 - val_loss: 0.0505 - val_mae: 0.1568\n",
      "Epoch 499/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0326 - mae: 0.1399\n",
      "Epoch 499: val_loss improved from 0.05053 to 0.04908, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0326 - mae: 0.1399 - val_loss: 0.0491 - val_mae: 0.1504\n",
      "Epoch 500/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0314 - mae: 0.1368\n",
      "Epoch 500: val_loss improved from 0.04908 to 0.04837, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0314 - mae: 0.1368 - val_loss: 0.0484 - val_mae: 0.1501\n",
      "Epoch 501/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0304 - mae: 0.1338\n",
      "Epoch 501: val_loss improved from 0.04837 to 0.04770, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0304 - mae: 0.1338 - val_loss: 0.0477 - val_mae: 0.1485\n",
      "Epoch 502/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0300 - mae: 0.1327\n",
      "Epoch 502: val_loss improved from 0.04770 to 0.04747, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0300 - mae: 0.1327 - val_loss: 0.0475 - val_mae: 0.1475\n",
      "Epoch 503/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0301 - mae: 0.1330\n",
      "Epoch 503: val_loss did not improve from 0.04747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0301 - mae: 0.1330 - val_loss: 0.0477 - val_mae: 0.1502\n",
      "Epoch 504/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0302 - mae: 0.1337\n",
      "Epoch 504: val_loss did not improve from 0.04747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0302 - mae: 0.1337 - val_loss: 0.0476 - val_mae: 0.1477\n",
      "Epoch 505/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0305 - mae: 0.1350\n",
      "Epoch 505: val_loss did not improve from 0.04747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0305 - mae: 0.1350 - val_loss: 0.0480 - val_mae: 0.1523\n",
      "Epoch 506/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0308 - mae: 0.1359\n",
      "Epoch 506: val_loss did not improve from 0.04747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0308 - mae: 0.1359 - val_loss: 0.0476 - val_mae: 0.1478\n",
      "Epoch 507/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0309 - mae: 0.1361\n",
      "Epoch 507: val_loss did not improve from 0.04747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0309 - mae: 0.1361 - val_loss: 0.0475 - val_mae: 0.1512\n",
      "Epoch 508/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0303 - mae: 0.1346\n",
      "Epoch 508: val_loss improved from 0.04747 to 0.04638, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0303 - mae: 0.1346 - val_loss: 0.0464 - val_mae: 0.1453\n",
      "Epoch 509/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0295 - mae: 0.1326\n",
      "Epoch 509: val_loss improved from 0.04638 to 0.04569, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0295 - mae: 0.1326 - val_loss: 0.0457 - val_mae: 0.1461\n",
      "Epoch 510/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0285 - mae: 0.1296\n",
      "Epoch 510: val_loss improved from 0.04569 to 0.04474, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0285 - mae: 0.1296 - val_loss: 0.0447 - val_mae: 0.1419\n",
      "Epoch 511/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0277 - mae: 0.1274\n",
      "Epoch 511: val_loss improved from 0.04474 to 0.04421, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0277 - mae: 0.1274 - val_loss: 0.0442 - val_mae: 0.1415\n",
      "Epoch 512/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0272 - mae: 0.1259\n",
      "Epoch 512: val_loss improved from 0.04421 to 0.04389, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0272 - mae: 0.1259 - val_loss: 0.0439 - val_mae: 0.1414\n",
      "Epoch 513/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0270 - mae: 0.1256\n",
      "Epoch 513: val_loss improved from 0.04389 to 0.04378, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0270 - mae: 0.1256 - val_loss: 0.0438 - val_mae: 0.1401\n",
      "Epoch 514/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0271 - mae: 0.1260\n",
      "Epoch 514: val_loss did not improve from 0.04378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0271 - mae: 0.1260 - val_loss: 0.0440 - val_mae: 0.1428\n",
      "Epoch 515/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0272 - mae: 0.1265\n",
      "Epoch 515: val_loss did not improve from 0.04378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0272 - mae: 0.1265 - val_loss: 0.0440 - val_mae: 0.1404\n",
      "Epoch 516/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0277 - mae: 0.1279\n",
      "Epoch 516: val_loss did not improve from 0.04378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0277 - mae: 0.1279 - val_loss: 0.0442 - val_mae: 0.1447\n",
      "Epoch 517/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0278 - mae: 0.1286\n",
      "Epoch 517: val_loss did not improve from 0.04378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0278 - mae: 0.1286 - val_loss: 0.0441 - val_mae: 0.1409\n",
      "Epoch 518/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0280 - mae: 0.1295\n",
      "Epoch 518: val_loss did not improve from 0.04378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0280 - mae: 0.1295 - val_loss: 0.0441 - val_mae: 0.1456\n",
      "Epoch 519/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0280 - mae: 0.1297\n",
      "Epoch 519: val_loss improved from 0.04378 to 0.04359, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0280 - mae: 0.1297 - val_loss: 0.0436 - val_mae: 0.1403\n",
      "Epoch 520/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0279 - mae: 0.1293\n",
      "Epoch 520: val_loss improved from 0.04359 to 0.04315, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0279 - mae: 0.1293 - val_loss: 0.0432 - val_mae: 0.1434\n",
      "Epoch 521/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0272 - mae: 0.1275\n",
      "Epoch 521: val_loss improved from 0.04315 to 0.04216, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0272 - mae: 0.1275 - val_loss: 0.0422 - val_mae: 0.1370\n",
      "Epoch 522/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0265 - mae: 0.1251\n",
      "Epoch 522: val_loss improved from 0.04216 to 0.04137, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0265 - mae: 0.1251 - val_loss: 0.0414 - val_mae: 0.1377\n",
      "Epoch 523/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0254 - mae: 0.1219\n",
      "Epoch 523: val_loss improved from 0.04137 to 0.04058, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0254 - mae: 0.1219 - val_loss: 0.0406 - val_mae: 0.1335\n",
      "Epoch 524/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0247 - mae: 0.1195\n",
      "Epoch 524: val_loss improved from 0.04058 to 0.04009, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0247 - mae: 0.1195 - val_loss: 0.0401 - val_mae: 0.1331\n",
      "Epoch 525/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0241 - mae: 0.1178\n",
      "Epoch 525: val_loss improved from 0.04009 to 0.03988, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0241 - mae: 0.1178 - val_loss: 0.0399 - val_mae: 0.1331\n",
      "Epoch 526/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0240 - mae: 0.1176\n",
      "Epoch 526: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0240 - mae: 0.1176 - val_loss: 0.0399 - val_mae: 0.1320\n",
      "Epoch 527/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0240 - mae: 0.1179\n",
      "Epoch 527: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0240 - mae: 0.1179 - val_loss: 0.0401 - val_mae: 0.1350\n",
      "Epoch 528/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0245 - mae: 0.1194\n",
      "Epoch 528: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0245 - mae: 0.1194 - val_loss: 0.0403 - val_mae: 0.1331\n",
      "Epoch 529/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0249 - mae: 0.1210\n",
      "Epoch 529: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0249 - mae: 0.1210 - val_loss: 0.0407 - val_mae: 0.1384\n",
      "Epoch 530/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0255 - mae: 0.1230\n",
      "Epoch 530: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0255 - mae: 0.1230 - val_loss: 0.0411 - val_mae: 0.1352\n",
      "Epoch 531/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0259 - mae: 0.1246\n",
      "Epoch 531: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0259 - mae: 0.1246 - val_loss: 0.0412 - val_mae: 0.1411\n",
      "Epoch 532/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0263 - mae: 0.1262\n",
      "Epoch 532: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0263 - mae: 0.1262 - val_loss: 0.0409 - val_mae: 0.1353\n",
      "Epoch 533/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0260 - mae: 0.1251\n",
      "Epoch 533: val_loss did not improve from 0.03988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0260 - mae: 0.1251 - val_loss: 0.0400 - val_mae: 0.1379\n",
      "Epoch 534/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0251 - mae: 0.1227\n",
      "Epoch 534: val_loss improved from 0.03988 to 0.03883, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0251 - mae: 0.1227 - val_loss: 0.0388 - val_mae: 0.1300\n",
      "Epoch 535/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0238 - mae: 0.1180\n",
      "Epoch 535: val_loss improved from 0.03883 to 0.03751, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0238 - mae: 0.1180 - val_loss: 0.0375 - val_mae: 0.1290\n",
      "Epoch 536/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0225 - mae: 0.1136\n",
      "Epoch 536: val_loss improved from 0.03751 to 0.03677, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0225 - mae: 0.1136 - val_loss: 0.0368 - val_mae: 0.1258\n",
      "Epoch 537/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0217 - mae: 0.1111\n",
      "Epoch 537: val_loss improved from 0.03677 to 0.03675, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0217 - mae: 0.1111 - val_loss: 0.0368 - val_mae: 0.1254\n",
      "Epoch 538/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0217 - mae: 0.1112\n",
      "Epoch 538: val_loss did not improve from 0.03675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0217 - mae: 0.1112 - val_loss: 0.0371 - val_mae: 0.1283\n",
      "Epoch 539/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0220 - mae: 0.1128\n",
      "Epoch 539: val_loss did not improve from 0.03675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0220 - mae: 0.1128 - val_loss: 0.0375 - val_mae: 0.1270\n",
      "Epoch 540/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0224 - mae: 0.1144\n",
      "Epoch 540: val_loss did not improve from 0.03675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0224 - mae: 0.1144 - val_loss: 0.0374 - val_mae: 0.1308\n",
      "Epoch 541/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0229 - mae: 0.1158\n",
      "Epoch 541: val_loss did not improve from 0.03675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0229 - mae: 0.1158 - val_loss: 0.0374 - val_mae: 0.1272\n",
      "Epoch 542/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0227 - mae: 0.1153\n",
      "Epoch 542: val_loss improved from 0.03675 to 0.03675, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0227 - mae: 0.1153 - val_loss: 0.0367 - val_mae: 0.1290\n",
      "Epoch 543/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0221 - mae: 0.1135\n",
      "Epoch 543: val_loss improved from 0.03675 to 0.03616, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0221 - mae: 0.1135 - val_loss: 0.0362 - val_mae: 0.1239\n",
      "Epoch 544/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0213 - mae: 0.1108\n",
      "Epoch 544: val_loss improved from 0.03616 to 0.03533, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0213 - mae: 0.1108 - val_loss: 0.0353 - val_mae: 0.1238\n",
      "Epoch 545/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0205 - mae: 0.1083\n",
      "Epoch 545: val_loss improved from 0.03533 to 0.03492, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0205 - mae: 0.1083 - val_loss: 0.0349 - val_mae: 0.1211\n",
      "Epoch 546/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0201 - mae: 0.1065\n",
      "Epoch 546: val_loss improved from 0.03492 to 0.03466, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0201 - mae: 0.1065 - val_loss: 0.0347 - val_mae: 0.1207\n",
      "Epoch 547/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0199 - mae: 0.1059\n",
      "Epoch 547: val_loss improved from 0.03466 to 0.03457, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0199 - mae: 0.1059 - val_loss: 0.0346 - val_mae: 0.1215\n",
      "Epoch 548/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0198 - mae: 0.1060\n",
      "Epoch 548: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0198 - mae: 0.1060 - val_loss: 0.0347 - val_mae: 0.1205\n",
      "Epoch 549/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0201 - mae: 0.1069\n",
      "Epoch 549: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0201 - mae: 0.1069 - val_loss: 0.0346 - val_mae: 0.1233\n",
      "Epoch 550/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0204 - mae: 0.1082\n",
      "Epoch 550: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0204 - mae: 0.1082 - val_loss: 0.0350 - val_mae: 0.1217\n",
      "Epoch 551/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0207 - mae: 0.1092\n",
      "Epoch 551: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0207 - mae: 0.1092 - val_loss: 0.0349 - val_mae: 0.1254\n",
      "Epoch 552/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0209 - mae: 0.1104\n",
      "Epoch 552: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0209 - mae: 0.1104 - val_loss: 0.0353 - val_mae: 0.1229\n",
      "Epoch 553/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0211 - mae: 0.1112\n",
      "Epoch 553: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0211 - mae: 0.1112 - val_loss: 0.0349 - val_mae: 0.1266\n",
      "Epoch 554/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0214 - mae: 0.1123\n",
      "Epoch 554: val_loss did not improve from 0.03457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0214 - mae: 0.1123 - val_loss: 0.0352 - val_mae: 0.1230\n",
      "Epoch 555/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0213 - mae: 0.1119\n",
      "Epoch 555: val_loss improved from 0.03457 to 0.03444, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0213 - mae: 0.1119 - val_loss: 0.0344 - val_mae: 0.1258\n",
      "Epoch 556/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0211 - mae: 0.1115\n",
      "Epoch 556: val_loss improved from 0.03444 to 0.03423, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0211 - mae: 0.1115 - val_loss: 0.0342 - val_mae: 0.1208\n",
      "Epoch 557/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0204 - mae: 0.1090\n",
      "Epoch 557: val_loss improved from 0.03423 to 0.03311, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0204 - mae: 0.1090 - val_loss: 0.0331 - val_mae: 0.1212\n",
      "Epoch 558/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0197 - mae: 0.1065\n",
      "Epoch 558: val_loss improved from 0.03311 to 0.03279, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0197 - mae: 0.1065 - val_loss: 0.0328 - val_mae: 0.1165\n",
      "Epoch 559/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0189 - mae: 0.1034\n",
      "Epoch 559: val_loss improved from 0.03279 to 0.03193, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0189 - mae: 0.1034 - val_loss: 0.0319 - val_mae: 0.1158\n",
      "Epoch 560/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0182 - mae: 0.1008\n",
      "Epoch 560: val_loss improved from 0.03193 to 0.03154, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0182 - mae: 0.1008 - val_loss: 0.0315 - val_mae: 0.1139\n",
      "Epoch 561/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0178 - mae: 0.0994\n",
      "Epoch 561: val_loss improved from 0.03154 to 0.03137, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0178 - mae: 0.0994 - val_loss: 0.0314 - val_mae: 0.1134\n",
      "Epoch 562/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0176 - mae: 0.0988\n",
      "Epoch 562: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0176 - mae: 0.0988 - val_loss: 0.0314 - val_mae: 0.1146\n",
      "Epoch 563/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0178 - mae: 0.0997\n",
      "Epoch 563: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0178 - mae: 0.0997 - val_loss: 0.0318 - val_mae: 0.1143\n",
      "Epoch 564/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0182 - mae: 0.1011\n",
      "Epoch 564: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0182 - mae: 0.1011 - val_loss: 0.0317 - val_mae: 0.1174\n",
      "Epoch 565/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0184 - mae: 0.1024\n",
      "Epoch 565: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0184 - mae: 0.1024 - val_loss: 0.0325 - val_mae: 0.1165\n",
      "Epoch 566/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0189 - mae: 0.1041\n",
      "Epoch 566: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0189 - mae: 0.1041 - val_loss: 0.0324 - val_mae: 0.1204\n",
      "Epoch 567/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0193 - mae: 0.1060\n",
      "Epoch 567: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0193 - mae: 0.1060 - val_loss: 0.0332 - val_mae: 0.1186\n",
      "Epoch 568/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0196 - mae: 0.1070\n",
      "Epoch 568: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0196 - mae: 0.1070 - val_loss: 0.0325 - val_mae: 0.1227\n",
      "Epoch 569/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0200 - mae: 0.1087\n",
      "Epoch 569: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0200 - mae: 0.1087 - val_loss: 0.0332 - val_mae: 0.1192\n",
      "Epoch 570/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0198 - mae: 0.1081\n",
      "Epoch 570: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0198 - mae: 0.1081 - val_loss: 0.0321 - val_mae: 0.1210\n",
      "Epoch 571/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0194 - mae: 0.1068\n",
      "Epoch 571: val_loss did not improve from 0.03137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0194 - mae: 0.1068 - val_loss: 0.0319 - val_mae: 0.1151\n",
      "Epoch 572/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0185 - mae: 0.1030\n",
      "Epoch 572: val_loss improved from 0.03137 to 0.03035, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0185 - mae: 0.1030 - val_loss: 0.0304 - val_mae: 0.1136\n",
      "Epoch 573/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0173 - mae: 0.0986\n",
      "Epoch 573: val_loss improved from 0.03035 to 0.02992, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0173 - mae: 0.0986 - val_loss: 0.0299 - val_mae: 0.1094\n",
      "Epoch 574/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0164 - mae: 0.0950\n",
      "Epoch 574: val_loss improved from 0.02992 to 0.02947, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0164 - mae: 0.0950 - val_loss: 0.0295 - val_mae: 0.1085\n",
      "Epoch 575/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0160 - mae: 0.0935\n",
      "Epoch 575: val_loss improved from 0.02947 to 0.02934, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0160 - mae: 0.0935 - val_loss: 0.0293 - val_mae: 0.1092\n",
      "Epoch 576/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0162 - mae: 0.0943\n",
      "Epoch 576: val_loss did not improve from 0.02934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0162 - mae: 0.0943 - val_loss: 0.0299 - val_mae: 0.1097\n",
      "Epoch 577/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0165 - mae: 0.0959\n",
      "Epoch 577: val_loss did not improve from 0.02934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0165 - mae: 0.0959 - val_loss: 0.0299 - val_mae: 0.1132\n",
      "Epoch 578/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0170 - mae: 0.0983\n",
      "Epoch 578: val_loss did not improve from 0.02934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0170 - mae: 0.0983 - val_loss: 0.0308 - val_mae: 0.1125\n",
      "Epoch 579/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0175 - mae: 0.1000\n",
      "Epoch 579: val_loss did not improve from 0.02934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0175 - mae: 0.1000 - val_loss: 0.0301 - val_mae: 0.1152\n",
      "Epoch 580/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0176 - mae: 0.1008\n",
      "Epoch 580: val_loss did not improve from 0.02934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0176 - mae: 0.1008 - val_loss: 0.0305 - val_mae: 0.1120\n",
      "Epoch 581/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0174 - mae: 0.0997\n",
      "Epoch 581: val_loss improved from 0.02934 to 0.02926, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0174 - mae: 0.0997 - val_loss: 0.0293 - val_mae: 0.1119\n",
      "Epoch 582/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0168 - mae: 0.0973\n",
      "Epoch 582: val_loss improved from 0.02926 to 0.02917, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0168 - mae: 0.0973 - val_loss: 0.0292 - val_mae: 0.1080\n",
      "Epoch 583/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0162 - mae: 0.0946\n",
      "Epoch 583: val_loss improved from 0.02917 to 0.02818, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0162 - mae: 0.0946 - val_loss: 0.0282 - val_mae: 0.1067\n",
      "Epoch 584/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0155 - mae: 0.0918\n",
      "Epoch 584: val_loss improved from 0.02818 to 0.02805, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0155 - mae: 0.0918 - val_loss: 0.0280 - val_mae: 0.1049\n",
      "Epoch 585/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0149 - mae: 0.0901\n",
      "Epoch 585: val_loss improved from 0.02805 to 0.02787, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0149 - mae: 0.0901 - val_loss: 0.0279 - val_mae: 0.1045\n",
      "Epoch 586/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0149 - mae: 0.0897\n",
      "Epoch 586: val_loss improved from 0.02787 to 0.02772, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0149 - mae: 0.0897 - val_loss: 0.0277 - val_mae: 0.1051\n",
      "Epoch 587/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0151 - mae: 0.0903\n",
      "Epoch 587: val_loss did not improve from 0.02772\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0151 - mae: 0.0903 - val_loss: 0.0281 - val_mae: 0.1052\n",
      "Epoch 588/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0152 - mae: 0.0912\n",
      "Epoch 588: val_loss improved from 0.02772 to 0.02770, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0152 - mae: 0.0912 - val_loss: 0.0277 - val_mae: 0.1069\n",
      "Epoch 589/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0155 - mae: 0.0925\n",
      "Epoch 589: val_loss did not improve from 0.02770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0155 - mae: 0.0925 - val_loss: 0.0284 - val_mae: 0.1066\n",
      "Epoch 590/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0158 - mae: 0.0937\n",
      "Epoch 590: val_loss did not improve from 0.02770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0158 - mae: 0.0937 - val_loss: 0.0278 - val_mae: 0.1084\n",
      "Epoch 591/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0158 - mae: 0.0942\n",
      "Epoch 591: val_loss did not improve from 0.02770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0158 - mae: 0.0942 - val_loss: 0.0285 - val_mae: 0.1073\n",
      "Epoch 592/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0160 - mae: 0.0946\n",
      "Epoch 592: val_loss did not improve from 0.02770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0160 - mae: 0.0946 - val_loss: 0.0277 - val_mae: 0.1086\n",
      "Epoch 593/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0160 - mae: 0.0948\n",
      "Epoch 593: val_loss did not improve from 0.02770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0160 - mae: 0.0948 - val_loss: 0.0284 - val_mae: 0.1071\n",
      "Epoch 594/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0159 - mae: 0.0946\n",
      "Epoch 594: val_loss improved from 0.02770 to 0.02746, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0159 - mae: 0.0946 - val_loss: 0.0275 - val_mae: 0.1080\n",
      "Epoch 595/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0157 - mae: 0.0940\n",
      "Epoch 595: val_loss did not improve from 0.02746\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0157 - mae: 0.0940 - val_loss: 0.0280 - val_mae: 0.1060\n",
      "Epoch 596/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0155 - mae: 0.0931\n",
      "Epoch 596: val_loss improved from 0.02746 to 0.02697, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0155 - mae: 0.0931 - val_loss: 0.0270 - val_mae: 0.1061\n",
      "Epoch 597/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0153 - mae: 0.0920\n",
      "Epoch 597: val_loss did not improve from 0.02697\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0153 - mae: 0.0920 - val_loss: 0.0273 - val_mae: 0.1039\n",
      "Epoch 598/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0150 - mae: 0.0909\n",
      "Epoch 598: val_loss improved from 0.02697 to 0.02632, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0150 - mae: 0.0909 - val_loss: 0.0263 - val_mae: 0.1033\n",
      "Epoch 599/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0146 - mae: 0.0890\n",
      "Epoch 599: val_loss did not improve from 0.02632\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0146 - mae: 0.0890 - val_loss: 0.0265 - val_mae: 0.1014\n",
      "Epoch 600/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0142 - mae: 0.0876\n",
      "Epoch 600: val_loss improved from 0.02632 to 0.02580, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0142 - mae: 0.0876 - val_loss: 0.0258 - val_mae: 0.1007\n",
      "Epoch 601/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0139 - mae: 0.0862\n",
      "Epoch 601: val_loss did not improve from 0.02580\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0139 - mae: 0.0862 - val_loss: 0.0258 - val_mae: 0.0996\n",
      "Epoch 602/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0137 - mae: 0.0853\n",
      "Epoch 602: val_loss improved from 0.02580 to 0.02546, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0137 - mae: 0.0853 - val_loss: 0.0255 - val_mae: 0.0991\n",
      "Epoch 603/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0136 - mae: 0.0846\n",
      "Epoch 603: val_loss improved from 0.02546 to 0.02539, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0136 - mae: 0.0846 - val_loss: 0.0254 - val_mae: 0.0987\n",
      "Epoch 604/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0135 - mae: 0.0842\n",
      "Epoch 604: val_loss improved from 0.02539 to 0.02539, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0135 - mae: 0.0842 - val_loss: 0.0254 - val_mae: 0.0985\n",
      "Epoch 605/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0134 - mae: 0.0840\n",
      "Epoch 605: val_loss improved from 0.02539 to 0.02521, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0134 - mae: 0.0840 - val_loss: 0.0252 - val_mae: 0.0985\n",
      "Epoch 606/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0134 - mae: 0.0839\n",
      "Epoch 606: val_loss did not improve from 0.02521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0134 - mae: 0.0839 - val_loss: 0.0254 - val_mae: 0.0984\n",
      "Epoch 607/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0134 - mae: 0.0840\n",
      "Epoch 607: val_loss improved from 0.02521 to 0.02509, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0134 - mae: 0.0840 - val_loss: 0.0251 - val_mae: 0.0988\n",
      "Epoch 608/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0135 - mae: 0.0845\n",
      "Epoch 608: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0135 - mae: 0.0845 - val_loss: 0.0256 - val_mae: 0.0991\n",
      "Epoch 609/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0136 - mae: 0.0853\n",
      "Epoch 609: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0136 - mae: 0.0853 - val_loss: 0.0251 - val_mae: 0.1003\n",
      "Epoch 610/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0138 - mae: 0.0864\n",
      "Epoch 610: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0138 - mae: 0.0864 - val_loss: 0.0262 - val_mae: 0.1017\n",
      "Epoch 611/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0143 - mae: 0.0887\n",
      "Epoch 611: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0143 - mae: 0.0887 - val_loss: 0.0259 - val_mae: 0.1053\n",
      "Epoch 612/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0151 - mae: 0.0925\n",
      "Epoch 612: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0151 - mae: 0.0925 - val_loss: 0.0281 - val_mae: 0.1094\n",
      "Epoch 613/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0165 - mae: 0.0985\n",
      "Epoch 613: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0165 - mae: 0.0985 - val_loss: 0.0281 - val_mae: 0.1170\n",
      "Epoch 614/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0184 - mae: 0.1067\n",
      "Epoch 614: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0184 - mae: 0.1067 - val_loss: 0.0317 - val_mae: 0.1225\n",
      "Epoch 615/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0206 - mae: 0.1144\n",
      "Epoch 615: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0206 - mae: 0.1144 - val_loss: 0.0307 - val_mae: 0.1281\n",
      "Epoch 616/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0222 - mae: 0.1206\n",
      "Epoch 616: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0222 - mae: 0.1206 - val_loss: 0.0319 - val_mae: 0.1239\n",
      "Epoch 617/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0209 - mae: 0.1158\n",
      "Epoch 617: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0209 - mae: 0.1158 - val_loss: 0.0272 - val_mae: 0.1142\n",
      "Epoch 618/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0177 - mae: 0.1038\n",
      "Epoch 618: val_loss did not improve from 0.02509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0177 - mae: 0.1038 - val_loss: 0.0254 - val_mae: 0.1000\n",
      "Epoch 619/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0139 - mae: 0.0873\n",
      "Epoch 619: val_loss improved from 0.02509 to 0.02379, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0139 - mae: 0.0873 - val_loss: 0.0238 - val_mae: 0.0946\n",
      "Epoch 620/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0125 - mae: 0.0804\n",
      "Epoch 620: val_loss did not improve from 0.02379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0125 - mae: 0.0804 - val_loss: 0.0244 - val_mae: 0.1005\n",
      "Epoch 621/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0140 - mae: 0.0877\n",
      "Epoch 621: val_loss did not improve from 0.02379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0140 - mae: 0.0877 - val_loss: 0.0274 - val_mae: 0.1085\n",
      "Epoch 622/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0162 - mae: 0.0977\n",
      "Epoch 622: val_loss did not improve from 0.02379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0162 - mae: 0.0977 - val_loss: 0.0263 - val_mae: 0.1105\n",
      "Epoch 623/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0167 - mae: 0.0998\n",
      "Epoch 623: val_loss did not improve from 0.02379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0167 - mae: 0.0998 - val_loss: 0.0262 - val_mae: 0.1040\n",
      "Epoch 624/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0149 - mae: 0.0923\n",
      "Epoch 624: val_loss improved from 0.02379 to 0.02345, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0149 - mae: 0.0923 - val_loss: 0.0235 - val_mae: 0.0954\n",
      "Epoch 625/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0127 - mae: 0.0817\n",
      "Epoch 625: val_loss improved from 0.02345 to 0.02324, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0127 - mae: 0.0817 - val_loss: 0.0232 - val_mae: 0.0938\n",
      "Epoch 626/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0123 - mae: 0.0798\n",
      "Epoch 626: val_loss did not improve from 0.02324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0123 - mae: 0.0798 - val_loss: 0.0248 - val_mae: 0.0989\n",
      "Epoch 627/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0136 - mae: 0.0862\n",
      "Epoch 627: val_loss did not improve from 0.02324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0136 - mae: 0.0862 - val_loss: 0.0245 - val_mae: 0.1029\n",
      "Epoch 628/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0147 - mae: 0.0910\n",
      "Epoch 628: val_loss did not improve from 0.02324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0147 - mae: 0.0910 - val_loss: 0.0254 - val_mae: 0.1015\n",
      "Epoch 629/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0141 - mae: 0.0893\n",
      "Epoch 629: val_loss did not improve from 0.02324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0141 - mae: 0.0893 - val_loss: 0.0232 - val_mae: 0.0957\n",
      "Epoch 630/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0127 - mae: 0.0823\n",
      "Epoch 630: val_loss improved from 0.02324 to 0.02292, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0127 - mae: 0.0823 - val_loss: 0.0229 - val_mae: 0.0925\n",
      "Epoch 631/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0120 - mae: 0.0782\n",
      "Epoch 631: val_loss did not improve from 0.02292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0120 - mae: 0.0782 - val_loss: 0.0234 - val_mae: 0.0941\n",
      "Epoch 632/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0123 - mae: 0.0803\n",
      "Epoch 632: val_loss did not improve from 0.02292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0123 - mae: 0.0803 - val_loss: 0.0233 - val_mae: 0.0970\n",
      "Epoch 633/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0131 - mae: 0.0842\n",
      "Epoch 633: val_loss did not improve from 0.02292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0131 - mae: 0.0842 - val_loss: 0.0243 - val_mae: 0.0982\n",
      "Epoch 634/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0132 - mae: 0.0853\n",
      "Epoch 634: val_loss improved from 0.02292 to 0.02282, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0132 - mae: 0.0853 - val_loss: 0.0228 - val_mae: 0.0951\n",
      "Epoch 635/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0127 - mae: 0.0820\n",
      "Epoch 635: val_loss improved from 0.02282 to 0.02281, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0127 - mae: 0.0820 - val_loss: 0.0228 - val_mae: 0.0922\n",
      "Epoch 636/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0118 - mae: 0.0781\n",
      "Epoch 636: val_loss improved from 0.02281 to 0.02244, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0118 - mae: 0.0781 - val_loss: 0.0224 - val_mae: 0.0913\n",
      "Epoch 637/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0117 - mae: 0.0773\n",
      "Epoch 637: val_loss improved from 0.02244 to 0.02234, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0117 - mae: 0.0773 - val_loss: 0.0223 - val_mae: 0.0926\n",
      "Epoch 638/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0120 - mae: 0.0790\n",
      "Epoch 638: val_loss did not improve from 0.02234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0120 - mae: 0.0790 - val_loss: 0.0233 - val_mae: 0.0944\n",
      "Epoch 639/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0123 - mae: 0.0810\n",
      "Epoch 639: val_loss did not improve from 0.02234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0123 - mae: 0.0810 - val_loss: 0.0224 - val_mae: 0.0934\n",
      "Epoch 640/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0122 - mae: 0.0801\n",
      "Epoch 640: val_loss did not improve from 0.02234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0122 - mae: 0.0801 - val_loss: 0.0225 - val_mae: 0.0919\n",
      "Epoch 641/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0118 - mae: 0.0782\n",
      "Epoch 641: val_loss improved from 0.02234 to 0.02190, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0118 - mae: 0.0782 - val_loss: 0.0219 - val_mae: 0.0902\n",
      "Epoch 642/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0113 - mae: 0.0761\n",
      "Epoch 642: val_loss improved from 0.02190 to 0.02188, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0113 - mae: 0.0761 - val_loss: 0.0219 - val_mae: 0.0903\n",
      "Epoch 643/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0115 - mae: 0.0766\n",
      "Epoch 643: val_loss did not improve from 0.02188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0115 - mae: 0.0766 - val_loss: 0.0225 - val_mae: 0.0919\n",
      "Epoch 644/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0117 - mae: 0.0782\n",
      "Epoch 644: val_loss improved from 0.02188 to 0.02180, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0117 - mae: 0.0782 - val_loss: 0.0218 - val_mae: 0.0918\n",
      "Epoch 645/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0120 - mae: 0.0788\n",
      "Epoch 645: val_loss did not improve from 0.02180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0120 - mae: 0.0788 - val_loss: 0.0223 - val_mae: 0.0919\n",
      "Epoch 646/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0118 - mae: 0.0784\n",
      "Epoch 646: val_loss improved from 0.02180 to 0.02157, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0118 - mae: 0.0784 - val_loss: 0.0216 - val_mae: 0.0900\n",
      "Epoch 647/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0114 - mae: 0.0764\n",
      "Epoch 647: val_loss improved from 0.02157 to 0.02155, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0114 - mae: 0.0764 - val_loss: 0.0215 - val_mae: 0.0892\n",
      "Epoch 648/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0113 - mae: 0.0754\n",
      "Epoch 648: val_loss improved from 0.02155 to 0.02147, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0113 - mae: 0.0754 - val_loss: 0.0215 - val_mae: 0.0891\n",
      "Epoch 649/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0113 - mae: 0.0752\n",
      "Epoch 649: val_loss improved from 0.02147 to 0.02137, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0113 - mae: 0.0752 - val_loss: 0.0214 - val_mae: 0.0894\n",
      "Epoch 650/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0113 - mae: 0.0758\n",
      "Epoch 650: val_loss did not improve from 0.02137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0113 - mae: 0.0758 - val_loss: 0.0219 - val_mae: 0.0905\n",
      "Epoch 651/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0114 - mae: 0.0769\n",
      "Epoch 651: val_loss improved from 0.02137 to 0.02125, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0114 - mae: 0.0769 - val_loss: 0.0212 - val_mae: 0.0897\n",
      "Epoch 652/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0113 - mae: 0.0763\n",
      "Epoch 652: val_loss did not improve from 0.02125\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0113 - mae: 0.0763 - val_loss: 0.0216 - val_mae: 0.0895\n",
      "Epoch 653/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0113 - mae: 0.0759\n",
      "Epoch 653: val_loss improved from 0.02125 to 0.02109, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0113 - mae: 0.0759 - val_loss: 0.0211 - val_mae: 0.0882\n",
      "Epoch 654/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0111 - mae: 0.0745\n",
      "Epoch 654: val_loss did not improve from 0.02109\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0111 - mae: 0.0745 - val_loss: 0.0211 - val_mae: 0.0878\n",
      "Epoch 655/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0110 - mae: 0.0741\n",
      "Epoch 655: val_loss improved from 0.02109 to 0.02108, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0110 - mae: 0.0741 - val_loss: 0.0211 - val_mae: 0.0879\n",
      "Epoch 656/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0110 - mae: 0.0741\n",
      "Epoch 656: val_loss improved from 0.02108 to 0.02083, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0110 - mae: 0.0741 - val_loss: 0.0208 - val_mae: 0.0878\n",
      "Epoch 657/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0110 - mae: 0.0743\n",
      "Epoch 657: val_loss did not improve from 0.02083\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0110 - mae: 0.0743 - val_loss: 0.0212 - val_mae: 0.0886\n",
      "Epoch 658/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0110 - mae: 0.0749\n",
      "Epoch 658: val_loss improved from 0.02083 to 0.02069, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0110 - mae: 0.0749 - val_loss: 0.0207 - val_mae: 0.0877\n",
      "Epoch 659/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0111 - mae: 0.0745\n",
      "Epoch 659: val_loss did not improve from 0.02069\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0111 - mae: 0.0745 - val_loss: 0.0210 - val_mae: 0.0879\n",
      "Epoch 660/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0108 - mae: 0.0740\n",
      "Epoch 660: val_loss improved from 0.02069 to 0.02056, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0108 - mae: 0.0740 - val_loss: 0.0206 - val_mae: 0.0869\n",
      "Epoch 661/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0107 - mae: 0.0732\n",
      "Epoch 661: val_loss did not improve from 0.02056\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0107 - mae: 0.0732 - val_loss: 0.0206 - val_mae: 0.0867\n",
      "Epoch 662/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0108 - mae: 0.0731\n",
      "Epoch 662: val_loss improved from 0.02056 to 0.02051, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0108 - mae: 0.0731 - val_loss: 0.0205 - val_mae: 0.0865\n",
      "Epoch 663/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0106 - mae: 0.0727\n",
      "Epoch 663: val_loss improved from 0.02051 to 0.02038, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0204 - val_mae: 0.0863\n",
      "Epoch 664/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0106 - mae: 0.0727\n",
      "Epoch 664: val_loss did not improve from 0.02038\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0206 - val_mae: 0.0868\n",
      "Epoch 665/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0107 - mae: 0.0731\n",
      "Epoch 665: val_loss improved from 0.02038 to 0.02023, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0107 - mae: 0.0731 - val_loss: 0.0202 - val_mae: 0.0862\n",
      "Epoch 666/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0108 - mae: 0.0730\n",
      "Epoch 666: val_loss did not improve from 0.02023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0108 - mae: 0.0730 - val_loss: 0.0206 - val_mae: 0.0868\n",
      "Epoch 667/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0107 - mae: 0.0732\n",
      "Epoch 667: val_loss improved from 0.02023 to 0.02020, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0107 - mae: 0.0732 - val_loss: 0.0202 - val_mae: 0.0860\n",
      "Epoch 668/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0106 - mae: 0.0726\n",
      "Epoch 668: val_loss did not improve from 0.02020\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0106 - mae: 0.0726 - val_loss: 0.0204 - val_mae: 0.0863\n",
      "Epoch 669/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0106 - mae: 0.0726\n",
      "Epoch 669: val_loss improved from 0.02020 to 0.01990, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.0106 - mae: 0.0726 - val_loss: 0.0199 - val_mae: 0.0853\n",
      "Epoch 670/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0105 - mae: 0.0720\n",
      "Epoch 670: val_loss did not improve from 0.01990\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0105 - mae: 0.0720 - val_loss: 0.0201 - val_mae: 0.0854\n",
      "Epoch 671/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0105 - mae: 0.0719\n",
      "Epoch 671: val_loss improved from 0.01990 to 0.01979, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0105 - mae: 0.0719 - val_loss: 0.0198 - val_mae: 0.0848\n",
      "Epoch 672/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0104 - mae: 0.0714\n",
      "Epoch 672: val_loss improved from 0.01979 to 0.01969, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0104 - mae: 0.0714 - val_loss: 0.0197 - val_mae: 0.0846\n",
      "Epoch 673/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0102 - mae: 0.0711\n",
      "Epoch 673: val_loss did not improve from 0.01969\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0102 - mae: 0.0711 - val_loss: 0.0197 - val_mae: 0.0847\n",
      "Epoch 674/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0102 - mae: 0.0712\n",
      "Epoch 674: val_loss improved from 0.01969 to 0.01963, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0102 - mae: 0.0712 - val_loss: 0.0196 - val_mae: 0.0844\n",
      "Epoch 675/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0103 - mae: 0.0711\n",
      "Epoch 675: val_loss did not improve from 0.01963\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0103 - mae: 0.0711 - val_loss: 0.0198 - val_mae: 0.0847\n",
      "Epoch 676/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0103 - mae: 0.0712\n",
      "Epoch 676: val_loss improved from 0.01963 to 0.01941, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0103 - mae: 0.0712 - val_loss: 0.0194 - val_mae: 0.0841\n",
      "Epoch 677/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0102 - mae: 0.0708\n",
      "Epoch 677: val_loss did not improve from 0.01941\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0102 - mae: 0.0708 - val_loss: 0.0196 - val_mae: 0.0845\n",
      "Epoch 678/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0103 - mae: 0.0712\n",
      "Epoch 678: val_loss improved from 0.01941 to 0.01938, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0103 - mae: 0.0712 - val_loss: 0.0194 - val_mae: 0.0839\n",
      "Epoch 679/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0103 - mae: 0.0709\n",
      "Epoch 679: val_loss did not improve from 0.01938\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0103 - mae: 0.0709 - val_loss: 0.0196 - val_mae: 0.0843\n",
      "Epoch 680/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0103 - mae: 0.0711\n",
      "Epoch 680: val_loss improved from 0.01938 to 0.01910, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0103 - mae: 0.0711 - val_loss: 0.0191 - val_mae: 0.0835\n",
      "Epoch 681/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0101 - mae: 0.0704\n",
      "Epoch 681: val_loss did not improve from 0.01910\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0101 - mae: 0.0704 - val_loss: 0.0194 - val_mae: 0.0840\n",
      "Epoch 682/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0101 - mae: 0.0707\n",
      "Epoch 682: val_loss did not improve from 0.01910\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0101 - mae: 0.0707 - val_loss: 0.0192 - val_mae: 0.0833\n",
      "Epoch 683/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0100 - mae: 0.0701\n",
      "Epoch 683: val_loss did not improve from 0.01910\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0100 - mae: 0.0701 - val_loss: 0.0193 - val_mae: 0.0836\n",
      "Epoch 684/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0100 - mae: 0.0701\n",
      "Epoch 684: val_loss improved from 0.01910 to 0.01895, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0100 - mae: 0.0701 - val_loss: 0.0190 - val_mae: 0.0829\n",
      "Epoch 685/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0099 - mae: 0.0697\n",
      "Epoch 685: val_loss did not improve from 0.01895\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0099 - mae: 0.0697 - val_loss: 0.0191 - val_mae: 0.0833\n",
      "Epoch 686/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0100 - mae: 0.0700\n",
      "Epoch 686: val_loss improved from 0.01895 to 0.01892, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0100 - mae: 0.0700 - val_loss: 0.0189 - val_mae: 0.0827\n",
      "Epoch 687/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0099 - mae: 0.0696\n",
      "Epoch 687: val_loss did not improve from 0.01892\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0099 - mae: 0.0696 - val_loss: 0.0191 - val_mae: 0.0833\n",
      "Epoch 688/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0099 - mae: 0.0699\n",
      "Epoch 688: val_loss improved from 0.01892 to 0.01871, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0099 - mae: 0.0699 - val_loss: 0.0187 - val_mae: 0.0825\n",
      "Epoch 689/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0100 - mae: 0.0698\n",
      "Epoch 689: val_loss did not improve from 0.01871\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0100 - mae: 0.0698 - val_loss: 0.0191 - val_mae: 0.0836\n",
      "Epoch 690/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0100 - mae: 0.0702\n",
      "Epoch 690: val_loss did not improve from 0.01871\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0100 - mae: 0.0702 - val_loss: 0.0187 - val_mae: 0.0827\n",
      "Epoch 691/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0100 - mae: 0.0701\n",
      "Epoch 691: val_loss did not improve from 0.01871\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0100 - mae: 0.0701 - val_loss: 0.0192 - val_mae: 0.0846\n",
      "Epoch 692/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0100 - mae: 0.0711\n",
      "Epoch 692: val_loss improved from 0.01871 to 0.01863, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0100 - mae: 0.0711 - val_loss: 0.0186 - val_mae: 0.0838\n",
      "Epoch 693/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0103 - mae: 0.0719\n",
      "Epoch 693: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0103 - mae: 0.0719 - val_loss: 0.0199 - val_mae: 0.0879\n",
      "Epoch 694/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0107 - mae: 0.0750\n",
      "Epoch 694: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0107 - mae: 0.0750 - val_loss: 0.0191 - val_mae: 0.0875\n",
      "Epoch 695/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0113 - mae: 0.0771\n",
      "Epoch 695: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0113 - mae: 0.0771 - val_loss: 0.0213 - val_mae: 0.0949\n",
      "Epoch 696/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0121 - mae: 0.0831\n",
      "Epoch 696: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0121 - mae: 0.0831 - val_loss: 0.0206 - val_mae: 0.0963\n",
      "Epoch 697/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0135 - mae: 0.0885\n",
      "Epoch 697: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0135 - mae: 0.0885 - val_loss: 0.0243 - val_mae: 0.1079\n",
      "Epoch 698/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0151 - mae: 0.0974\n",
      "Epoch 698: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0151 - mae: 0.0974 - val_loss: 0.0230 - val_mae: 0.1092\n",
      "Epoch 699/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0173 - mae: 0.1048\n",
      "Epoch 699: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0173 - mae: 0.1048 - val_loss: 0.0268 - val_mae: 0.1180\n",
      "Epoch 700/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0181 - mae: 0.1091\n",
      "Epoch 700: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0181 - mae: 0.1091 - val_loss: 0.0233 - val_mae: 0.1098\n",
      "Epoch 701/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0175 - mae: 0.1057\n",
      "Epoch 701: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0175 - mae: 0.1057 - val_loss: 0.0234 - val_mae: 0.1048\n",
      "Epoch 702/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0144 - mae: 0.0943\n",
      "Epoch 702: val_loss did not improve from 0.01863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0144 - mae: 0.0943 - val_loss: 0.0187 - val_mae: 0.0870\n",
      "Epoch 703/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0111 - mae: 0.0768\n",
      "Epoch 703: val_loss improved from 0.01863 to 0.01820, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0111 - mae: 0.0768 - val_loss: 0.0182 - val_mae: 0.0810\n",
      "Epoch 704/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0096 - mae: 0.0681\n",
      "Epoch 704: val_loss did not improve from 0.01820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0096 - mae: 0.0681 - val_loss: 0.0192 - val_mae: 0.0857\n",
      "Epoch 705/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0102 - mae: 0.0727\n",
      "Epoch 705: val_loss did not improve from 0.01820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0102 - mae: 0.0727 - val_loss: 0.0193 - val_mae: 0.0911\n",
      "Epoch 706/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0124 - mae: 0.0827\n",
      "Epoch 706: val_loss did not improve from 0.01820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0124 - mae: 0.0827 - val_loss: 0.0223 - val_mae: 0.1009\n",
      "Epoch 707/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0135 - mae: 0.0902\n",
      "Epoch 707: val_loss did not improve from 0.01820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0135 - mae: 0.0902 - val_loss: 0.0198 - val_mae: 0.0936\n",
      "Epoch 708/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0130 - mae: 0.0859\n",
      "Epoch 708: val_loss did not improve from 0.01820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0130 - mae: 0.0859 - val_loss: 0.0198 - val_mae: 0.0892\n",
      "Epoch 709/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0109 - mae: 0.0766\n",
      "Epoch 709: val_loss improved from 0.01820 to 0.01774, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 0.0109 - mae: 0.0766 - val_loss: 0.0177 - val_mae: 0.0800\n",
      "Epoch 710/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0095 - mae: 0.0675\n",
      "Epoch 710: val_loss did not improve from 0.01774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0095 - mae: 0.0675 - val_loss: 0.0178 - val_mae: 0.0807\n",
      "Epoch 711/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0097 - mae: 0.0687\n",
      "Epoch 711: val_loss did not improve from 0.01774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0097 - mae: 0.0687 - val_loss: 0.0198 - val_mae: 0.0892\n",
      "Epoch 712/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0109 - mae: 0.0768\n",
      "Epoch 712: val_loss did not improve from 0.01774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0109 - mae: 0.0768 - val_loss: 0.0188 - val_mae: 0.0884\n",
      "Epoch 713/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0116 - mae: 0.0793\n",
      "Epoch 713: val_loss did not improve from 0.01774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0116 - mae: 0.0793 - val_loss: 0.0198 - val_mae: 0.0898\n",
      "Epoch 714/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0109 - mae: 0.0774\n",
      "Epoch 714: val_loss improved from 0.01774 to 0.01769, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0109 - mae: 0.0774 - val_loss: 0.0177 - val_mae: 0.0812\n",
      "Epoch 715/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0099 - mae: 0.0698\n",
      "Epoch 715: val_loss did not improve from 0.01769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0099 - mae: 0.0698 - val_loss: 0.0177 - val_mae: 0.0795\n",
      "Epoch 716/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0092 - mae: 0.0666\n",
      "Epoch 716: val_loss did not improve from 0.01769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0092 - mae: 0.0666 - val_loss: 0.0183 - val_mae: 0.0823\n",
      "Epoch 717/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0096 - mae: 0.0691\n",
      "Epoch 717: val_loss did not improve from 0.01769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0096 - mae: 0.0691 - val_loss: 0.0178 - val_mae: 0.0828\n",
      "Epoch 718/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0102 - mae: 0.0720\n",
      "Epoch 718: val_loss did not improve from 0.01769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0102 - mae: 0.0720 - val_loss: 0.0191 - val_mae: 0.0868\n",
      "Epoch 719/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0103 - mae: 0.0740\n",
      "Epoch 719: val_loss improved from 0.01769 to 0.01765, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0103 - mae: 0.0740 - val_loss: 0.0177 - val_mae: 0.0813\n",
      "Epoch 720/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0099 - mae: 0.0700\n",
      "Epoch 720: val_loss did not improve from 0.01765\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0099 - mae: 0.0700 - val_loss: 0.0178 - val_mae: 0.0804\n",
      "Epoch 721/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0093 - mae: 0.0673\n",
      "Epoch 721: val_loss improved from 0.01765 to 0.01752, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0093 - mae: 0.0673 - val_loss: 0.0175 - val_mae: 0.0791\n",
      "Epoch 722/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0091 - mae: 0.0662\n",
      "Epoch 722: val_loss improved from 0.01752 to 0.01737, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0091 - mae: 0.0662 - val_loss: 0.0174 - val_mae: 0.0794\n",
      "Epoch 723/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0095 - mae: 0.0676\n",
      "Epoch 723: val_loss did not improve from 0.01737\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0095 - mae: 0.0676 - val_loss: 0.0183 - val_mae: 0.0832\n",
      "Epoch 724/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0097 - mae: 0.0702\n",
      "Epoch 724: val_loss did not improve from 0.01737\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0097 - mae: 0.0702 - val_loss: 0.0174 - val_mae: 0.0805\n",
      "Epoch 725/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0097 - mae: 0.0693\n",
      "Epoch 725: val_loss did not improve from 0.01737\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0097 - mae: 0.0693 - val_loss: 0.0178 - val_mae: 0.0813\n",
      "Epoch 726/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0094 - mae: 0.0683\n",
      "Epoch 726: val_loss improved from 0.01737 to 0.01712, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0094 - mae: 0.0683 - val_loss: 0.0171 - val_mae: 0.0782\n",
      "Epoch 727/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0090 - mae: 0.0657\n",
      "Epoch 727: val_loss improved from 0.01712 to 0.01709, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0090 - mae: 0.0657 - val_loss: 0.0171 - val_mae: 0.0780\n",
      "Epoch 728/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0090 - mae: 0.0656\n",
      "Epoch 728: val_loss did not improve from 0.01709\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0090 - mae: 0.0656 - val_loss: 0.0176 - val_mae: 0.0803\n",
      "Epoch 729/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0092 - mae: 0.0672\n",
      "Epoch 729: val_loss improved from 0.01709 to 0.01704, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0092 - mae: 0.0672 - val_loss: 0.0170 - val_mae: 0.0790\n",
      "Epoch 730/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0094 - mae: 0.0674\n",
      "Epoch 730: val_loss did not improve from 0.01704\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0094 - mae: 0.0674 - val_loss: 0.0177 - val_mae: 0.0811\n",
      "Epoch 731/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0092 - mae: 0.0680\n",
      "Epoch 731: val_loss improved from 0.01704 to 0.01687, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0092 - mae: 0.0680 - val_loss: 0.0169 - val_mae: 0.0780\n",
      "Epoch 732/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0091 - mae: 0.0660\n",
      "Epoch 732: val_loss did not improve from 0.01687\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0091 - mae: 0.0660 - val_loss: 0.0171 - val_mae: 0.0783\n",
      "Epoch 733/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0090 - mae: 0.0655\n",
      "Epoch 733: val_loss did not improve from 0.01687\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0090 - mae: 0.0655 - val_loss: 0.0170 - val_mae: 0.0779\n",
      "Epoch 734/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0089 - mae: 0.0652\n",
      "Epoch 734: val_loss improved from 0.01687 to 0.01682, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0089 - mae: 0.0652 - val_loss: 0.0168 - val_mae: 0.0775\n",
      "Epoch 735/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0090 - mae: 0.0654\n",
      "Epoch 735: val_loss did not improve from 0.01682\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0090 - mae: 0.0654 - val_loss: 0.0173 - val_mae: 0.0797\n",
      "Epoch 736/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0090 - mae: 0.0667\n",
      "Epoch 736: val_loss improved from 0.01682 to 0.01672, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0090 - mae: 0.0667 - val_loss: 0.0167 - val_mae: 0.0778\n",
      "Epoch 737/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0090 - mae: 0.0659\n",
      "Epoch 737: val_loss did not improve from 0.01672\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0090 - mae: 0.0659 - val_loss: 0.0171 - val_mae: 0.0790\n",
      "Epoch 738/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0091 - mae: 0.0661\n",
      "Epoch 738: val_loss improved from 0.01672 to 0.01659, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0091 - mae: 0.0661 - val_loss: 0.0166 - val_mae: 0.0769\n",
      "Epoch 739/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0088 - mae: 0.0646\n",
      "Epoch 739: val_loss did not improve from 0.01659\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0088 - mae: 0.0646 - val_loss: 0.0167 - val_mae: 0.0771\n",
      "Epoch 740/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0088 - mae: 0.0645\n",
      "Epoch 740: val_loss did not improve from 0.01659\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0088 - mae: 0.0645 - val_loss: 0.0168 - val_mae: 0.0776\n",
      "Epoch 741/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0088 - mae: 0.0649\n",
      "Epoch 741: val_loss improved from 0.01659 to 0.01651, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0088 - mae: 0.0649 - val_loss: 0.0165 - val_mae: 0.0768\n",
      "Epoch 742/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0088 - mae: 0.0646\n",
      "Epoch 742: val_loss did not improve from 0.01651\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0088 - mae: 0.0646 - val_loss: 0.0169 - val_mae: 0.0784\n",
      "Epoch 743/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0088 - mae: 0.0653\n",
      "Epoch 743: val_loss improved from 0.01651 to 0.01639, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0088 - mae: 0.0653 - val_loss: 0.0164 - val_mae: 0.0766\n",
      "Epoch 744/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0089 - mae: 0.0647\n",
      "Epoch 744: val_loss did not improve from 0.01639\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0089 - mae: 0.0647 - val_loss: 0.0166 - val_mae: 0.0774\n",
      "Epoch 745/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0087 - mae: 0.0646\n",
      "Epoch 745: val_loss improved from 0.01639 to 0.01638, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0087 - mae: 0.0646 - val_loss: 0.0164 - val_mae: 0.0763\n",
      "Epoch 746/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0087 - mae: 0.0640\n",
      "Epoch 746: val_loss did not improve from 0.01638\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0087 - mae: 0.0640 - val_loss: 0.0164 - val_mae: 0.0764\n",
      "Epoch 747/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0087 - mae: 0.0639\n",
      "Epoch 747: val_loss did not improve from 0.01638\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0087 - mae: 0.0639 - val_loss: 0.0165 - val_mae: 0.0767\n",
      "Epoch 748/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0086 - mae: 0.0639\n",
      "Epoch 748: val_loss improved from 0.01638 to 0.01631, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0086 - mae: 0.0639 - val_loss: 0.0163 - val_mae: 0.0761\n",
      "Epoch 749/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0087 - mae: 0.0639\n",
      "Epoch 749: val_loss did not improve from 0.01631\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0087 - mae: 0.0639 - val_loss: 0.0166 - val_mae: 0.0773\n",
      "Epoch 750/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0086 - mae: 0.0643\n",
      "Epoch 750: val_loss improved from 0.01631 to 0.01621, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0086 - mae: 0.0643 - val_loss: 0.0162 - val_mae: 0.0760\n",
      "Epoch 751/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0086 - mae: 0.0639\n",
      "Epoch 751: val_loss did not improve from 0.01621\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0086 - mae: 0.0639 - val_loss: 0.0164 - val_mae: 0.0770\n",
      "Epoch 752/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0087 - mae: 0.0644\n",
      "Epoch 752: val_loss improved from 0.01621 to 0.01608, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0087 - mae: 0.0644 - val_loss: 0.0161 - val_mae: 0.0756\n",
      "Epoch 753/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0085 - mae: 0.0633\n",
      "Epoch 753: val_loss did not improve from 0.01608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0085 - mae: 0.0633 - val_loss: 0.0162 - val_mae: 0.0759\n",
      "Epoch 754/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0086 - mae: 0.0636\n",
      "Epoch 754: val_loss did not improve from 0.01608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0086 - mae: 0.0636 - val_loss: 0.0161 - val_mae: 0.0758\n",
      "Epoch 755/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0084 - mae: 0.0632\n",
      "Epoch 755: val_loss improved from 0.01608 to 0.01597, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0084 - mae: 0.0632 - val_loss: 0.0160 - val_mae: 0.0753\n",
      "Epoch 756/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0085 - mae: 0.0632\n",
      "Epoch 756: val_loss did not improve from 0.01597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0085 - mae: 0.0632 - val_loss: 0.0163 - val_mae: 0.0766\n",
      "Epoch 757/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0085 - mae: 0.0639\n",
      "Epoch 757: val_loss improved from 0.01597 to 0.01596, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0085 - mae: 0.0639 - val_loss: 0.0160 - val_mae: 0.0753\n",
      "Epoch 758/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0085 - mae: 0.0633\n",
      "Epoch 758: val_loss did not improve from 0.01596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0085 - mae: 0.0633 - val_loss: 0.0162 - val_mae: 0.0765\n",
      "Epoch 759/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0086 - mae: 0.0640\n",
      "Epoch 759: val_loss improved from 0.01596 to 0.01585, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0086 - mae: 0.0640 - val_loss: 0.0159 - val_mae: 0.0751\n",
      "Epoch 760/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0084 - mae: 0.0629\n",
      "Epoch 760: val_loss did not improve from 0.01585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0084 - mae: 0.0629 - val_loss: 0.0161 - val_mae: 0.0757\n",
      "Epoch 761/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0084 - mae: 0.0631\n",
      "Epoch 761: val_loss improved from 0.01585 to 0.01582, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0084 - mae: 0.0631 - val_loss: 0.0158 - val_mae: 0.0750\n",
      "Epoch 762/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0084 - mae: 0.0628\n",
      "Epoch 762: val_loss improved from 0.01582 to 0.01578, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0084 - mae: 0.0628 - val_loss: 0.0158 - val_mae: 0.0749\n",
      "Epoch 763/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0083 - mae: 0.0625\n",
      "Epoch 763: val_loss did not improve from 0.01578\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0083 - mae: 0.0625 - val_loss: 0.0159 - val_mae: 0.0755\n",
      "Epoch 764/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0084 - mae: 0.0629\n",
      "Epoch 764: val_loss improved from 0.01578 to 0.01570, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0084 - mae: 0.0629 - val_loss: 0.0157 - val_mae: 0.0746\n",
      "Epoch 765/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0084 - mae: 0.0626\n",
      "Epoch 765: val_loss did not improve from 0.01570\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0084 - mae: 0.0626 - val_loss: 0.0159 - val_mae: 0.0756\n",
      "Epoch 766/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0084 - mae: 0.0630\n",
      "Epoch 766: val_loss improved from 0.01570 to 0.01562, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0084 - mae: 0.0630 - val_loss: 0.0156 - val_mae: 0.0744\n",
      "Epoch 767/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0085 - mae: 0.0627\n",
      "Epoch 767: val_loss did not improve from 0.01562\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0085 - mae: 0.0627 - val_loss: 0.0159 - val_mae: 0.0754\n",
      "Epoch 768/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0085 - mae: 0.0631\n",
      "Epoch 768: val_loss improved from 0.01562 to 0.01558, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0085 - mae: 0.0631 - val_loss: 0.0156 - val_mae: 0.0743\n",
      "Epoch 769/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0084 - mae: 0.0623\n",
      "Epoch 769: val_loss did not improve from 0.01558\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0084 - mae: 0.0623 - val_loss: 0.0156 - val_mae: 0.0748\n",
      "Epoch 770/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0083 - mae: 0.0624\n",
      "Epoch 770: val_loss improved from 0.01558 to 0.01554, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0083 - mae: 0.0624 - val_loss: 0.0155 - val_mae: 0.0742\n",
      "Epoch 771/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0083 - mae: 0.0620\n",
      "Epoch 771: val_loss did not improve from 0.01554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0083 - mae: 0.0620 - val_loss: 0.0156 - val_mae: 0.0744\n",
      "Epoch 772/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0082 - mae: 0.0619\n",
      "Epoch 772: val_loss improved from 0.01554 to 0.01553, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0082 - mae: 0.0619 - val_loss: 0.0155 - val_mae: 0.0743\n",
      "Epoch 773/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0082 - mae: 0.0619\n",
      "Epoch 773: val_loss improved from 0.01553 to 0.01545, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0082 - mae: 0.0619 - val_loss: 0.0154 - val_mae: 0.0740\n",
      "Epoch 774/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0081 - mae: 0.0616\n",
      "Epoch 774: val_loss did not improve from 0.01545\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0081 - mae: 0.0616 - val_loss: 0.0155 - val_mae: 0.0743\n",
      "Epoch 775/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0082 - mae: 0.0620\n",
      "Epoch 775: val_loss improved from 0.01545 to 0.01537, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0082 - mae: 0.0620 - val_loss: 0.0154 - val_mae: 0.0738\n",
      "Epoch 776/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0081 - mae: 0.0614\n",
      "Epoch 776: val_loss did not improve from 0.01537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0081 - mae: 0.0614 - val_loss: 0.0154 - val_mae: 0.0740\n",
      "Epoch 777/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0081 - mae: 0.0617\n",
      "Epoch 777: val_loss improved from 0.01537 to 0.01535, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0081 - mae: 0.0617 - val_loss: 0.0154 - val_mae: 0.0737\n",
      "Epoch 778/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0082 - mae: 0.0616\n",
      "Epoch 778: val_loss did not improve from 0.01535\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0082 - mae: 0.0616 - val_loss: 0.0154 - val_mae: 0.0739\n",
      "Epoch 779/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0082 - mae: 0.0617\n",
      "Epoch 779: val_loss improved from 0.01535 to 0.01525, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0082 - mae: 0.0617 - val_loss: 0.0153 - val_mae: 0.0735\n",
      "Epoch 780/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0081 - mae: 0.0614\n",
      "Epoch 780: val_loss did not improve from 0.01525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0081 - mae: 0.0614 - val_loss: 0.0153 - val_mae: 0.0737\n",
      "Epoch 781/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0080 - mae: 0.0613\n",
      "Epoch 781: val_loss did not improve from 0.01525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0080 - mae: 0.0613 - val_loss: 0.0153 - val_mae: 0.0735\n",
      "Epoch 782/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0080 - mae: 0.0611\n",
      "Epoch 782: val_loss did not improve from 0.01525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0080 - mae: 0.0611 - val_loss: 0.0153 - val_mae: 0.0738\n",
      "Epoch 783/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0080 - mae: 0.0613\n",
      "Epoch 783: val_loss improved from 0.01525 to 0.01513, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0080 - mae: 0.0613 - val_loss: 0.0151 - val_mae: 0.0732\n",
      "Epoch 784/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0081 - mae: 0.0611\n",
      "Epoch 784: val_loss did not improve from 0.01513\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0081 - mae: 0.0611 - val_loss: 0.0153 - val_mae: 0.0736\n",
      "Epoch 785/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0080 - mae: 0.0612\n",
      "Epoch 785: val_loss did not improve from 0.01513\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0080 - mae: 0.0612 - val_loss: 0.0152 - val_mae: 0.0732\n",
      "Epoch 786/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0080 - mae: 0.0610\n",
      "Epoch 786: val_loss did not improve from 0.01513\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0080 - mae: 0.0610 - val_loss: 0.0151 - val_mae: 0.0733\n",
      "Epoch 787/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0080 - mae: 0.0610\n",
      "Epoch 787: val_loss improved from 0.01513 to 0.01502, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0080 - mae: 0.0610 - val_loss: 0.0150 - val_mae: 0.0729\n",
      "Epoch 788/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0080 - mae: 0.0609\n",
      "Epoch 788: val_loss did not improve from 0.01502\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0080 - mae: 0.0609 - val_loss: 0.0151 - val_mae: 0.0732\n",
      "Epoch 789/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0080 - mae: 0.0609\n",
      "Epoch 789: val_loss improved from 0.01502 to 0.01501, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0080 - mae: 0.0609 - val_loss: 0.0150 - val_mae: 0.0728\n",
      "Epoch 790/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0080 - mae: 0.0607\n",
      "Epoch 790: val_loss did not improve from 0.01501\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0080 - mae: 0.0607 - val_loss: 0.0150 - val_mae: 0.0730\n",
      "Epoch 791/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0079 - mae: 0.0607\n",
      "Epoch 791: val_loss improved from 0.01501 to 0.01496, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0079 - mae: 0.0607 - val_loss: 0.0150 - val_mae: 0.0727\n",
      "Epoch 792/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0080 - mae: 0.0607\n",
      "Epoch 792: val_loss did not improve from 0.01496\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0080 - mae: 0.0607 - val_loss: 0.0151 - val_mae: 0.0732\n",
      "Epoch 793/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0080 - mae: 0.0608\n",
      "Epoch 793: val_loss improved from 0.01496 to 0.01485, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0080 - mae: 0.0608 - val_loss: 0.0148 - val_mae: 0.0724\n",
      "Epoch 794/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0080 - mae: 0.0605\n",
      "Epoch 794: val_loss did not improve from 0.01485\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0080 - mae: 0.0605 - val_loss: 0.0150 - val_mae: 0.0733\n",
      "Epoch 795/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0079 - mae: 0.0610\n",
      "Epoch 795: val_loss improved from 0.01485 to 0.01479, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0079 - mae: 0.0610 - val_loss: 0.0148 - val_mae: 0.0723\n",
      "Epoch 796/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0079 - mae: 0.0605\n",
      "Epoch 796: val_loss did not improve from 0.01479\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0079 - mae: 0.0605 - val_loss: 0.0151 - val_mae: 0.0738\n",
      "Epoch 797/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0079 - mae: 0.0612\n",
      "Epoch 797: val_loss improved from 0.01479 to 0.01460, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0079 - mae: 0.0612 - val_loss: 0.0146 - val_mae: 0.0721\n",
      "Epoch 798/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0081 - mae: 0.0609\n",
      "Epoch 798: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0081 - mae: 0.0609 - val_loss: 0.0153 - val_mae: 0.0750\n",
      "Epoch 799/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0081 - mae: 0.0624\n",
      "Epoch 799: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0081 - mae: 0.0624 - val_loss: 0.0147 - val_mae: 0.0728\n",
      "Epoch 800/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0081 - mae: 0.0619\n",
      "Epoch 800: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0081 - mae: 0.0619 - val_loss: 0.0155 - val_mae: 0.0770\n",
      "Epoch 801/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0083 - mae: 0.0644\n",
      "Epoch 801: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0083 - mae: 0.0644 - val_loss: 0.0148 - val_mae: 0.0744\n",
      "Epoch 802/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0085 - mae: 0.0644\n",
      "Epoch 802: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0085 - mae: 0.0644 - val_loss: 0.0163 - val_mae: 0.0810\n",
      "Epoch 803/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0089 - mae: 0.0687\n",
      "Epoch 803: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0089 - mae: 0.0687 - val_loss: 0.0152 - val_mae: 0.0779\n",
      "Epoch 804/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0094 - mae: 0.0696\n",
      "Epoch 804: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0094 - mae: 0.0696 - val_loss: 0.0174 - val_mae: 0.0875\n",
      "Epoch 805/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0100 - mae: 0.0757\n",
      "Epoch 805: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0100 - mae: 0.0757 - val_loss: 0.0162 - val_mae: 0.0841\n",
      "Epoch 806/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0110 - mae: 0.0780\n",
      "Epoch 806: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0110 - mae: 0.0780 - val_loss: 0.0193 - val_mae: 0.0965\n",
      "Epoch 807/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0119 - mae: 0.0857\n",
      "Epoch 807: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0119 - mae: 0.0857 - val_loss: 0.0175 - val_mae: 0.0919\n",
      "Epoch 808/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0130 - mae: 0.0881\n",
      "Epoch 808: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0130 - mae: 0.0881 - val_loss: 0.0206 - val_mae: 0.1028\n",
      "Epoch 809/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0134 - mae: 0.0927\n",
      "Epoch 809: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0134 - mae: 0.0927 - val_loss: 0.0176 - val_mae: 0.0930\n",
      "Epoch 810/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0134 - mae: 0.0895\n",
      "Epoch 810: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0134 - mae: 0.0895 - val_loss: 0.0191 - val_mae: 0.0963\n",
      "Epoch 811/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0120 - mae: 0.0859\n",
      "Epoch 811: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0120 - mae: 0.0859 - val_loss: 0.0154 - val_mae: 0.0806\n",
      "Epoch 812/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0101 - mae: 0.0737\n",
      "Epoch 812: val_loss did not improve from 0.01460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0101 - mae: 0.0737 - val_loss: 0.0156 - val_mae: 0.0781\n",
      "Epoch 813/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0084 - mae: 0.0658\n",
      "Epoch 813: val_loss improved from 0.01460 to 0.01432, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0084 - mae: 0.0658 - val_loss: 0.0143 - val_mae: 0.0709\n",
      "Epoch 814/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0077 - mae: 0.0592\n",
      "Epoch 814: val_loss improved from 0.01432 to 0.01430, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0077 - mae: 0.0592 - val_loss: 0.0143 - val_mae: 0.0714\n",
      "Epoch 815/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0079 - mae: 0.0606\n",
      "Epoch 815: val_loss did not improve from 0.01430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0079 - mae: 0.0606 - val_loss: 0.0160 - val_mae: 0.0803\n",
      "Epoch 816/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0088 - mae: 0.0682\n",
      "Epoch 816: val_loss did not improve from 0.01430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0088 - mae: 0.0682 - val_loss: 0.0151 - val_mae: 0.0783\n",
      "Epoch 817/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0096 - mae: 0.0706\n",
      "Epoch 817: val_loss did not improve from 0.01430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0096 - mae: 0.0706 - val_loss: 0.0169 - val_mae: 0.0857\n",
      "Epoch 818/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0098 - mae: 0.0740\n",
      "Epoch 818: val_loss did not improve from 0.01430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0098 - mae: 0.0740 - val_loss: 0.0148 - val_mae: 0.0766\n",
      "Epoch 819/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0093 - mae: 0.0685\n",
      "Epoch 819: val_loss did not improve from 0.01430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0093 - mae: 0.0685 - val_loss: 0.0154 - val_mae: 0.0773\n",
      "Epoch 820/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0083 - mae: 0.0648\n",
      "Epoch 820: val_loss improved from 0.01430 to 0.01416, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0083 - mae: 0.0648 - val_loss: 0.0142 - val_mae: 0.0706\n",
      "Epoch 821/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0078 - mae: 0.0596\n",
      "Epoch 821: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0078 - mae: 0.0596 - val_loss: 0.0143 - val_mae: 0.0706\n",
      "Epoch 822/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0075 - mae: 0.0586\n",
      "Epoch 822: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0075 - mae: 0.0586 - val_loss: 0.0148 - val_mae: 0.0738\n",
      "Epoch 823/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0078 - mae: 0.0614\n",
      "Epoch 823: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0078 - mae: 0.0614 - val_loss: 0.0143 - val_mae: 0.0722\n",
      "Epoch 824/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0081 - mae: 0.0619\n",
      "Epoch 824: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0081 - mae: 0.0619 - val_loss: 0.0153 - val_mae: 0.0772\n",
      "Epoch 825/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0083 - mae: 0.0649\n",
      "Epoch 825: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0083 - mae: 0.0649 - val_loss: 0.0142 - val_mae: 0.0721\n",
      "Epoch 826/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0082 - mae: 0.0621\n",
      "Epoch 826: val_loss did not improve from 0.01416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0082 - mae: 0.0621 - val_loss: 0.0148 - val_mae: 0.0740\n",
      "Epoch 827/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0078 - mae: 0.0615\n",
      "Epoch 827: val_loss improved from 0.01416 to 0.01400, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0078 - mae: 0.0615 - val_loss: 0.0140 - val_mae: 0.0701\n",
      "Epoch 828/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0076 - mae: 0.0587\n",
      "Epoch 828: val_loss did not improve from 0.01400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0076 - mae: 0.0587 - val_loss: 0.0141 - val_mae: 0.0704\n",
      "Epoch 829/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0074 - mae: 0.0584\n",
      "Epoch 829: val_loss did not improve from 0.01400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0074 - mae: 0.0584 - val_loss: 0.0144 - val_mae: 0.0715\n",
      "Epoch 830/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0075 - mae: 0.0593\n",
      "Epoch 830: val_loss improved from 0.01400 to 0.01398, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0075 - mae: 0.0593 - val_loss: 0.0140 - val_mae: 0.0703\n",
      "Epoch 831/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0078 - mae: 0.0594\n",
      "Epoch 831: val_loss did not improve from 0.01398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0078 - mae: 0.0594 - val_loss: 0.0147 - val_mae: 0.0738\n",
      "Epoch 832/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0078 - mae: 0.0615\n",
      "Epoch 832: val_loss did not improve from 0.01398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0078 - mae: 0.0615 - val_loss: 0.0140 - val_mae: 0.0706\n",
      "Epoch 833/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0077 - mae: 0.0597\n",
      "Epoch 833: val_loss did not improve from 0.01398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0077 - mae: 0.0597 - val_loss: 0.0145 - val_mae: 0.0725\n",
      "Epoch 834/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0076 - mae: 0.0601\n",
      "Epoch 834: val_loss improved from 0.01398 to 0.01392, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0076 - mae: 0.0601 - val_loss: 0.0139 - val_mae: 0.0697\n",
      "Epoch 835/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0074 - mae: 0.0582\n",
      "Epoch 835: val_loss did not improve from 0.01392\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0074 - mae: 0.0582 - val_loss: 0.0140 - val_mae: 0.0701\n",
      "Epoch 836/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0075 - mae: 0.0583\n",
      "Epoch 836: val_loss did not improve from 0.01392\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0075 - mae: 0.0583 - val_loss: 0.0142 - val_mae: 0.0706\n",
      "Epoch 837/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0074 - mae: 0.0585\n",
      "Epoch 837: val_loss improved from 0.01392 to 0.01392, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0074 - mae: 0.0585 - val_loss: 0.0139 - val_mae: 0.0697\n",
      "Epoch 838/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0075 - mae: 0.0584\n",
      "Epoch 838: val_loss did not improve from 0.01392\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0075 - mae: 0.0584 - val_loss: 0.0144 - val_mae: 0.0723\n",
      "Epoch 839/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0075 - mae: 0.0599\n",
      "Epoch 839: val_loss improved from 0.01392 to 0.01381, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0075 - mae: 0.0599 - val_loss: 0.0138 - val_mae: 0.0698\n",
      "Epoch 840/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0076 - mae: 0.0589\n",
      "Epoch 840: val_loss did not improve from 0.01381\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0076 - mae: 0.0589 - val_loss: 0.0144 - val_mae: 0.0721\n",
      "Epoch 841/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0076 - mae: 0.0599\n",
      "Epoch 841: val_loss improved from 0.01381 to 0.01380, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0076 - mae: 0.0599 - val_loss: 0.0138 - val_mae: 0.0694\n",
      "Epoch 842/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0074 - mae: 0.0579\n",
      "Epoch 842: val_loss did not improve from 0.01380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0074 - mae: 0.0579 - val_loss: 0.0140 - val_mae: 0.0701\n",
      "Epoch 843/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0073 - mae: 0.0580\n",
      "Epoch 843: val_loss did not improve from 0.01380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0073 - mae: 0.0580 - val_loss: 0.0138 - val_mae: 0.0694\n",
      "Epoch 844/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0073 - mae: 0.0576\n",
      "Epoch 844: val_loss improved from 0.01380 to 0.01379, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0073 - mae: 0.0576 - val_loss: 0.0138 - val_mae: 0.0692\n",
      "Epoch 845/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0072 - mae: 0.0573\n",
      "Epoch 845: val_loss did not improve from 0.01379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0072 - mae: 0.0573 - val_loss: 0.0139 - val_mae: 0.0701\n",
      "Epoch 846/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0073 - mae: 0.0580\n",
      "Epoch 846: val_loss improved from 0.01379 to 0.01368, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0073 - mae: 0.0580 - val_loss: 0.0137 - val_mae: 0.0690\n",
      "Epoch 847/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0074 - mae: 0.0577\n",
      "Epoch 847: val_loss did not improve from 0.01368\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0074 - mae: 0.0577 - val_loss: 0.0140 - val_mae: 0.0707\n",
      "Epoch 848/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0074 - mae: 0.0586\n",
      "Epoch 848: val_loss improved from 0.01368 to 0.01363, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0074 - mae: 0.0586 - val_loss: 0.0136 - val_mae: 0.0689\n",
      "Epoch 849/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0073 - mae: 0.0575\n",
      "Epoch 849: val_loss did not improve from 0.01363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0073 - mae: 0.0575 - val_loss: 0.0139 - val_mae: 0.0703\n",
      "Epoch 850/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0073 - mae: 0.0583\n",
      "Epoch 850: val_loss improved from 0.01363 to 0.01362, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0073 - mae: 0.0583 - val_loss: 0.0136 - val_mae: 0.0687\n",
      "Epoch 851/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0573\n",
      "Epoch 851: val_loss did not improve from 0.01362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0072 - mae: 0.0573 - val_loss: 0.0138 - val_mae: 0.0696\n",
      "Epoch 852/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0073 - mae: 0.0577\n",
      "Epoch 852: val_loss did not improve from 0.01362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0073 - mae: 0.0577 - val_loss: 0.0137 - val_mae: 0.0688\n",
      "Epoch 853/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0570\n",
      "Epoch 853: val_loss did not improve from 0.01362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0570 - val_loss: 0.0137 - val_mae: 0.0688\n",
      "Epoch 854/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0571\n",
      "Epoch 854: val_loss did not improve from 0.01362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0072 - mae: 0.0571 - val_loss: 0.0138 - val_mae: 0.0695\n",
      "Epoch 855/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0071 - mae: 0.0573\n",
      "Epoch 855: val_loss improved from 0.01362 to 0.01353, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0071 - mae: 0.0573 - val_loss: 0.0135 - val_mae: 0.0685\n",
      "Epoch 856/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0072 - mae: 0.0571\n",
      "Epoch 856: val_loss did not improve from 0.01353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0072 - mae: 0.0571 - val_loss: 0.0139 - val_mae: 0.0705\n",
      "Epoch 857/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0074 - mae: 0.0585\n",
      "Epoch 857: val_loss improved from 0.01353 to 0.01347, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0074 - mae: 0.0585 - val_loss: 0.0135 - val_mae: 0.0685\n",
      "Epoch 858/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0072 - mae: 0.0572\n",
      "Epoch 858: val_loss did not improve from 0.01347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0072 - mae: 0.0572 - val_loss: 0.0139 - val_mae: 0.0703\n",
      "Epoch 859/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0072 - mae: 0.0581\n",
      "Epoch 859: val_loss did not improve from 0.01347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0581 - val_loss: 0.0135 - val_mae: 0.0683\n",
      "Epoch 860/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0072 - mae: 0.0569\n",
      "Epoch 860: val_loss did not improve from 0.01347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0072 - mae: 0.0569 - val_loss: 0.0137 - val_mae: 0.0697\n",
      "Epoch 861/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0574\n",
      "Epoch 861: val_loss improved from 0.01347 to 0.01341, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0071 - mae: 0.0574 - val_loss: 0.0134 - val_mae: 0.0681\n",
      "Epoch 862/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0071 - mae: 0.0566\n",
      "Epoch 862: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0071 - mae: 0.0566 - val_loss: 0.0136 - val_mae: 0.0690\n",
      "Epoch 863/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0072 - mae: 0.0571\n",
      "Epoch 863: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0072 - mae: 0.0571 - val_loss: 0.0135 - val_mae: 0.0683\n",
      "Epoch 864/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0565\n",
      "Epoch 864: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0071 - mae: 0.0565 - val_loss: 0.0135 - val_mae: 0.0682\n",
      "Epoch 865/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0072 - mae: 0.0566\n",
      "Epoch 865: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0072 - mae: 0.0566 - val_loss: 0.0136 - val_mae: 0.0687\n",
      "Epoch 866/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0071 - mae: 0.0568\n",
      "Epoch 866: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0071 - mae: 0.0568 - val_loss: 0.0134 - val_mae: 0.0680\n",
      "Epoch 867/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0071 - mae: 0.0564\n",
      "Epoch 867: val_loss did not improve from 0.01341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0071 - mae: 0.0564 - val_loss: 0.0136 - val_mae: 0.0690\n",
      "Epoch 868/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0071 - mae: 0.0569\n",
      "Epoch 868: val_loss improved from 0.01341 to 0.01334, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0071 - mae: 0.0569 - val_loss: 0.0133 - val_mae: 0.0678\n",
      "Epoch 869/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0072 - mae: 0.0565\n",
      "Epoch 869: val_loss did not improve from 0.01334\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0072 - mae: 0.0565 - val_loss: 0.0137 - val_mae: 0.0693\n",
      "Epoch 870/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0071 - mae: 0.0570\n",
      "Epoch 870: val_loss improved from 0.01334 to 0.01328, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0071 - mae: 0.0570 - val_loss: 0.0133 - val_mae: 0.0677\n",
      "Epoch 871/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0071 - mae: 0.0565\n",
      "Epoch 871: val_loss did not improve from 0.01328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0071 - mae: 0.0565 - val_loss: 0.0136 - val_mae: 0.0694\n",
      "Epoch 872/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0071 - mae: 0.0574\n",
      "Epoch 872: val_loss improved from 0.01328 to 0.01323, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0071 - mae: 0.0574 - val_loss: 0.0132 - val_mae: 0.0676\n",
      "Epoch 873/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0071 - mae: 0.0565\n",
      "Epoch 873: val_loss did not improve from 0.01323\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0071 - mae: 0.0565 - val_loss: 0.0137 - val_mae: 0.0697\n",
      "Epoch 874/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0071 - mae: 0.0573\n",
      "Epoch 874: val_loss improved from 0.01323 to 0.01315, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0071 - mae: 0.0573 - val_loss: 0.0131 - val_mae: 0.0675\n",
      "Epoch 875/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0072 - mae: 0.0565\n",
      "Epoch 875: val_loss did not improve from 0.01315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0565 - val_loss: 0.0135 - val_mae: 0.0695\n",
      "Epoch 876/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0070 - mae: 0.0574\n",
      "Epoch 876: val_loss did not improve from 0.01315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0070 - mae: 0.0574 - val_loss: 0.0132 - val_mae: 0.0675\n",
      "Epoch 877/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0071 - mae: 0.0564\n",
      "Epoch 877: val_loss did not improve from 0.01315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0071 - mae: 0.0564 - val_loss: 0.0136 - val_mae: 0.0697\n",
      "Epoch 878/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0070 - mae: 0.0574\n",
      "Epoch 878: val_loss improved from 0.01315 to 0.01307, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0070 - mae: 0.0574 - val_loss: 0.0131 - val_mae: 0.0673\n",
      "Epoch 879/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0072 - mae: 0.0565\n",
      "Epoch 879: val_loss did not improve from 0.01307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0565 - val_loss: 0.0137 - val_mae: 0.0699\n",
      "Epoch 880/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0071 - mae: 0.0575\n",
      "Epoch 880: val_loss did not improve from 0.01307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0071 - mae: 0.0575 - val_loss: 0.0132 - val_mae: 0.0676\n",
      "Epoch 881/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0071 - mae: 0.0565\n",
      "Epoch 881: val_loss did not improve from 0.01307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0071 - mae: 0.0565 - val_loss: 0.0136 - val_mae: 0.0699\n",
      "Epoch 882/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0577\n",
      "Epoch 882: val_loss improved from 0.01307 to 0.01296, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0071 - mae: 0.0577 - val_loss: 0.0130 - val_mae: 0.0672\n",
      "Epoch 883/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0565\n",
      "Epoch 883: val_loss did not improve from 0.01296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0565 - val_loss: 0.0137 - val_mae: 0.0701\n",
      "Epoch 884/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0071 - mae: 0.0578\n",
      "Epoch 884: val_loss did not improve from 0.01296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0071 - mae: 0.0578 - val_loss: 0.0130 - val_mae: 0.0672\n",
      "Epoch 885/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0072 - mae: 0.0566\n",
      "Epoch 885: val_loss did not improve from 0.01296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0566 - val_loss: 0.0135 - val_mae: 0.0700\n",
      "Epoch 886/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0579\n",
      "Epoch 886: val_loss did not improve from 0.01296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0072 - mae: 0.0579 - val_loss: 0.0130 - val_mae: 0.0673\n",
      "Epoch 887/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0072 - mae: 0.0566\n",
      "Epoch 887: val_loss did not improve from 0.01296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0072 - mae: 0.0566 - val_loss: 0.0137 - val_mae: 0.0705\n",
      "Epoch 888/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0582\n",
      "Epoch 888: val_loss improved from 0.01296 to 0.01292, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0071 - mae: 0.0582 - val_loss: 0.0129 - val_mae: 0.0672\n",
      "Epoch 889/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0072 - mae: 0.0568\n",
      "Epoch 889: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0072 - mae: 0.0568 - val_loss: 0.0136 - val_mae: 0.0707\n",
      "Epoch 890/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0585\n",
      "Epoch 890: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0130 - val_mae: 0.0675\n",
      "Epoch 891/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0073 - mae: 0.0572\n",
      "Epoch 891: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0073 - mae: 0.0572 - val_loss: 0.0138 - val_mae: 0.0717\n",
      "Epoch 892/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0073 - mae: 0.0594\n",
      "Epoch 892: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0073 - mae: 0.0594 - val_loss: 0.0129 - val_mae: 0.0678\n",
      "Epoch 893/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0073 - mae: 0.0580\n",
      "Epoch 893: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0073 - mae: 0.0580 - val_loss: 0.0140 - val_mae: 0.0730\n",
      "Epoch 894/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0074 - mae: 0.0607\n",
      "Epoch 894: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0074 - mae: 0.0607 - val_loss: 0.0131 - val_mae: 0.0687\n",
      "Epoch 895/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0076 - mae: 0.0593\n",
      "Epoch 895: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0076 - mae: 0.0593 - val_loss: 0.0142 - val_mae: 0.0747\n",
      "Epoch 896/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0076 - mae: 0.0625\n",
      "Epoch 896: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0076 - mae: 0.0625 - val_loss: 0.0131 - val_mae: 0.0699\n",
      "Epoch 897/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0078 - mae: 0.0613\n",
      "Epoch 897: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0078 - mae: 0.0613 - val_loss: 0.0146 - val_mae: 0.0771\n",
      "Epoch 898/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0079 - mae: 0.0650\n",
      "Epoch 898: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0079 - mae: 0.0650 - val_loss: 0.0133 - val_mae: 0.0716\n",
      "Epoch 899/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0082 - mae: 0.0636\n",
      "Epoch 899: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0082 - mae: 0.0636 - val_loss: 0.0149 - val_mae: 0.0794\n",
      "Epoch 900/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0084 - mae: 0.0677\n",
      "Epoch 900: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0084 - mae: 0.0677 - val_loss: 0.0136 - val_mae: 0.0733\n",
      "Epoch 901/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0085 - mae: 0.0659\n",
      "Epoch 901: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0085 - mae: 0.0659 - val_loss: 0.0154 - val_mae: 0.0814\n",
      "Epoch 902/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0087 - mae: 0.0697\n",
      "Epoch 902: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0087 - mae: 0.0697 - val_loss: 0.0136 - val_mae: 0.0739\n",
      "Epoch 903/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0089 - mae: 0.0674\n",
      "Epoch 903: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0089 - mae: 0.0674 - val_loss: 0.0154 - val_mae: 0.0819\n",
      "Epoch 904/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0087 - mae: 0.0702\n",
      "Epoch 904: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0087 - mae: 0.0702 - val_loss: 0.0136 - val_mae: 0.0736\n",
      "Epoch 905/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0086 - mae: 0.0664\n",
      "Epoch 905: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0086 - mae: 0.0664 - val_loss: 0.0149 - val_mae: 0.0795\n",
      "Epoch 906/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0083 - mae: 0.0675\n",
      "Epoch 906: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0083 - mae: 0.0675 - val_loss: 0.0131 - val_mae: 0.0705\n",
      "Epoch 907/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0081 - mae: 0.0626\n",
      "Epoch 907: val_loss did not improve from 0.01292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0081 - mae: 0.0626 - val_loss: 0.0141 - val_mae: 0.0745\n",
      "Epoch 908/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0076 - mae: 0.0624\n",
      "Epoch 908: val_loss improved from 0.01292 to 0.01273, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0076 - mae: 0.0624 - val_loss: 0.0127 - val_mae: 0.0670\n",
      "Epoch 909/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0071 - mae: 0.0572\n",
      "Epoch 909: val_loss did not improve from 0.01273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0071 - mae: 0.0572 - val_loss: 0.0131 - val_mae: 0.0686\n",
      "Epoch 910/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0069 - mae: 0.0567\n",
      "Epoch 910: val_loss improved from 0.01273 to 0.01261, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0069 - mae: 0.0567 - val_loss: 0.0126 - val_mae: 0.0656\n",
      "Epoch 911/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0542\n",
      "Epoch 911: val_loss did not improve from 0.01261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0066 - mae: 0.0542 - val_loss: 0.0127 - val_mae: 0.0657\n",
      "Epoch 912/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0066 - mae: 0.0541\n",
      "Epoch 912: val_loss did not improve from 0.01261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0066 - mae: 0.0541 - val_loss: 0.0130 - val_mae: 0.0677\n",
      "Epoch 913/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0067 - mae: 0.0556\n",
      "Epoch 913: val_loss improved from 0.01261 to 0.01257, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0067 - mae: 0.0556 - val_loss: 0.0126 - val_mae: 0.0660\n",
      "Epoch 914/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0069 - mae: 0.0556\n",
      "Epoch 914: val_loss did not improve from 0.01257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0069 - mae: 0.0556 - val_loss: 0.0135 - val_mae: 0.0710\n",
      "Epoch 915/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0590\n",
      "Epoch 915: val_loss did not improve from 0.01257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0127 - val_mae: 0.0670\n",
      "Epoch 916/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0071 - mae: 0.0572\n",
      "Epoch 916: val_loss did not improve from 0.01257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0071 - mae: 0.0572 - val_loss: 0.0135 - val_mae: 0.0714\n",
      "Epoch 917/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0072 - mae: 0.0593\n",
      "Epoch 917: val_loss did not improve from 0.01257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0126 - val_mae: 0.0666\n",
      "Epoch 918/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0071 - mae: 0.0568\n",
      "Epoch 918: val_loss did not improve from 0.01257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0071 - mae: 0.0568 - val_loss: 0.0134 - val_mae: 0.0702\n",
      "Epoch 919/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0070 - mae: 0.0581\n",
      "Epoch 919: val_loss improved from 0.01257 to 0.01250, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0070 - mae: 0.0581 - val_loss: 0.0125 - val_mae: 0.0657\n",
      "Epoch 920/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0068 - mae: 0.0551\n",
      "Epoch 920: val_loss did not improve from 0.01250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0068 - mae: 0.0551 - val_loss: 0.0129 - val_mae: 0.0676\n",
      "Epoch 921/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0067 - mae: 0.0557\n",
      "Epoch 921: val_loss improved from 0.01250 to 0.01250, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0067 - mae: 0.0557 - val_loss: 0.0125 - val_mae: 0.0652\n",
      "Epoch 922/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0065 - mae: 0.0537\n",
      "Epoch 922: val_loss did not improve from 0.01250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0065 - mae: 0.0537 - val_loss: 0.0126 - val_mae: 0.0657\n",
      "Epoch 923/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0066 - mae: 0.0540\n",
      "Epoch 923: val_loss did not improve from 0.01250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0066 - mae: 0.0540 - val_loss: 0.0126 - val_mae: 0.0660\n",
      "Epoch 924/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0066 - mae: 0.0543\n",
      "Epoch 924: val_loss improved from 0.01250 to 0.01242, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0066 - mae: 0.0543 - val_loss: 0.0124 - val_mae: 0.0650\n",
      "Epoch 925/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0066 - mae: 0.0539\n",
      "Epoch 925: val_loss did not improve from 0.01242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.0066 - mae: 0.0539 - val_loss: 0.0129 - val_mae: 0.0676\n",
      "Epoch 926/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0067 - mae: 0.0557\n",
      "Epoch 926: val_loss improved from 0.01242 to 0.01240, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0067 - mae: 0.0557 - val_loss: 0.0124 - val_mae: 0.0652\n",
      "Epoch 927/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0069 - mae: 0.0549\n",
      "Epoch 927: val_loss did not improve from 0.01240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0069 - mae: 0.0549 - val_loss: 0.0131 - val_mae: 0.0690\n",
      "Epoch 928/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0068 - mae: 0.0569\n",
      "Epoch 928: val_loss did not improve from 0.01240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0068 - mae: 0.0569 - val_loss: 0.0124 - val_mae: 0.0657\n",
      "Epoch 929/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0069 - mae: 0.0557\n",
      "Epoch 929: val_loss did not improve from 0.01240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0069 - mae: 0.0557 - val_loss: 0.0133 - val_mae: 0.0701\n",
      "Epoch 930/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0070 - mae: 0.0580\n",
      "Epoch 930: val_loss improved from 0.01240 to 0.01239, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0070 - mae: 0.0580 - val_loss: 0.0124 - val_mae: 0.0658\n",
      "Epoch 931/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0069 - mae: 0.0560\n",
      "Epoch 931: val_loss did not improve from 0.01239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0069 - mae: 0.0560 - val_loss: 0.0132 - val_mae: 0.0700\n",
      "Epoch 932/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0070 - mae: 0.0581\n",
      "Epoch 932: val_loss improved from 0.01239 to 0.01236, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0070 - mae: 0.0581 - val_loss: 0.0124 - val_mae: 0.0656\n",
      "Epoch 933/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0068 - mae: 0.0554\n",
      "Epoch 933: val_loss did not improve from 0.01236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0068 - mae: 0.0554 - val_loss: 0.0130 - val_mae: 0.0689\n",
      "Epoch 934/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0068 - mae: 0.0569\n",
      "Epoch 934: val_loss improved from 0.01236 to 0.01225, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0068 - mae: 0.0569 - val_loss: 0.0122 - val_mae: 0.0649\n",
      "Epoch 935/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0067 - mae: 0.0545\n",
      "Epoch 935: val_loss did not improve from 0.01225\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0067 - mae: 0.0545 - val_loss: 0.0128 - val_mae: 0.0674\n",
      "Epoch 936/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0067 - mae: 0.0556\n",
      "Epoch 936: val_loss did not improve from 0.01225\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0067 - mae: 0.0556 - val_loss: 0.0123 - val_mae: 0.0646\n",
      "Epoch 937/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0066 - mae: 0.0538\n",
      "Epoch 937: val_loss did not improve from 0.01225\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0066 - mae: 0.0538 - val_loss: 0.0126 - val_mae: 0.0663\n",
      "Epoch 938/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0066 - mae: 0.0545\n",
      "Epoch 938: val_loss improved from 0.01225 to 0.01222, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0066 - mae: 0.0545 - val_loss: 0.0122 - val_mae: 0.0643\n",
      "Epoch 939/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0065 - mae: 0.0532\n",
      "Epoch 939: val_loss did not improve from 0.01222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0065 - mae: 0.0532 - val_loss: 0.0124 - val_mae: 0.0654\n",
      "Epoch 940/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0064 - mae: 0.0536\n",
      "Epoch 940: val_loss did not improve from 0.01222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0064 - mae: 0.0536 - val_loss: 0.0122 - val_mae: 0.0644\n",
      "Epoch 941/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0064 - mae: 0.0530\n",
      "Epoch 941: val_loss did not improve from 0.01222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0064 - mae: 0.0530 - val_loss: 0.0123 - val_mae: 0.0648\n",
      "Epoch 942/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0065 - mae: 0.0533\n",
      "Epoch 942: val_loss did not improve from 0.01222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0065 - mae: 0.0533 - val_loss: 0.0123 - val_mae: 0.0647\n",
      "Epoch 943/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0063 - mae: 0.0529\n",
      "Epoch 943: val_loss improved from 0.01222 to 0.01219, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0063 - mae: 0.0529 - val_loss: 0.0122 - val_mae: 0.0643\n",
      "Epoch 944/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0063 - mae: 0.0527\n",
      "Epoch 944: val_loss did not improve from 0.01219\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0063 - mae: 0.0527 - val_loss: 0.0123 - val_mae: 0.0651\n",
      "Epoch 945/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0064 - mae: 0.0534\n",
      "Epoch 945: val_loss improved from 0.01219 to 0.01215, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0064 - mae: 0.0534 - val_loss: 0.0122 - val_mae: 0.0641\n",
      "Epoch 946/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0065 - mae: 0.0530\n",
      "Epoch 946: val_loss did not improve from 0.01215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0065 - mae: 0.0530 - val_loss: 0.0125 - val_mae: 0.0658\n",
      "Epoch 947/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0065 - mae: 0.0540\n",
      "Epoch 947: val_loss improved from 0.01215 to 0.01210, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0065 - mae: 0.0540 - val_loss: 0.0121 - val_mae: 0.0640\n",
      "Epoch 948/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0533\n",
      "Epoch 948: val_loss did not improve from 0.01210\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0066 - mae: 0.0533 - val_loss: 0.0126 - val_mae: 0.0665\n",
      "Epoch 949/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0546\n",
      "Epoch 949: val_loss improved from 0.01210 to 0.01207, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0066 - mae: 0.0546 - val_loss: 0.0121 - val_mae: 0.0640\n",
      "Epoch 950/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0065 - mae: 0.0533\n",
      "Epoch 950: val_loss did not improve from 0.01207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0065 - mae: 0.0533 - val_loss: 0.0126 - val_mae: 0.0669\n",
      "Epoch 951/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0066 - mae: 0.0550\n",
      "Epoch 951: val_loss improved from 0.01207 to 0.01205, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0066 - mae: 0.0550 - val_loss: 0.0120 - val_mae: 0.0640\n",
      "Epoch 952/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0065 - mae: 0.0535\n",
      "Epoch 952: val_loss did not improve from 0.01205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0065 - mae: 0.0535 - val_loss: 0.0127 - val_mae: 0.0673\n",
      "Epoch 953/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0552\n",
      "Epoch 953: val_loss did not improve from 0.01205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0066 - mae: 0.0552 - val_loss: 0.0121 - val_mae: 0.0642\n",
      "Epoch 954/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0066 - mae: 0.0537\n",
      "Epoch 954: val_loss did not improve from 0.01205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0066 - mae: 0.0537 - val_loss: 0.0127 - val_mae: 0.0679\n",
      "Epoch 955/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0067 - mae: 0.0559\n",
      "Epoch 955: val_loss did not improve from 0.01205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0067 - mae: 0.0559 - val_loss: 0.0120 - val_mae: 0.0643\n",
      "Epoch 956/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0066 - mae: 0.0540\n",
      "Epoch 956: val_loss did not improve from 0.01205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0066 - mae: 0.0540 - val_loss: 0.0128 - val_mae: 0.0683\n",
      "Epoch 957/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0067 - mae: 0.0563\n",
      "Epoch 957: val_loss improved from 0.01205 to 0.01196, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0067 - mae: 0.0563 - val_loss: 0.0120 - val_mae: 0.0642\n",
      "Epoch 958/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0067 - mae: 0.0543\n",
      "Epoch 958: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0067 - mae: 0.0543 - val_loss: 0.0128 - val_mae: 0.0686\n",
      "Epoch 959/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0564\n",
      "Epoch 959: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0066 - mae: 0.0564 - val_loss: 0.0121 - val_mae: 0.0648\n",
      "Epoch 960/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0069 - mae: 0.0553\n",
      "Epoch 960: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0069 - mae: 0.0553 - val_loss: 0.0131 - val_mae: 0.0705\n",
      "Epoch 961/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0068 - mae: 0.0584\n",
      "Epoch 961: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0068 - mae: 0.0584 - val_loss: 0.0121 - val_mae: 0.0661\n",
      "Epoch 962/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0071 - mae: 0.0574\n",
      "Epoch 962: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0071 - mae: 0.0574 - val_loss: 0.0135 - val_mae: 0.0738\n",
      "Epoch 963/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0074 - mae: 0.0619\n",
      "Epoch 963: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0074 - mae: 0.0619 - val_loss: 0.0125 - val_mae: 0.0688\n",
      "Epoch 964/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0077 - mae: 0.0612\n",
      "Epoch 964: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0077 - mae: 0.0612 - val_loss: 0.0143 - val_mae: 0.0786\n",
      "Epoch 965/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0080 - mae: 0.0670\n",
      "Epoch 965: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0080 - mae: 0.0670 - val_loss: 0.0129 - val_mae: 0.0728\n",
      "Epoch 966/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0087 - mae: 0.0671\n",
      "Epoch 966: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0087 - mae: 0.0671 - val_loss: 0.0154 - val_mae: 0.0849\n",
      "Epoch 967/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0090 - mae: 0.0735\n",
      "Epoch 967: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0090 - mae: 0.0735 - val_loss: 0.0137 - val_mae: 0.0782\n",
      "Epoch 968/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0099 - mae: 0.0741\n",
      "Epoch 968: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0099 - mae: 0.0741 - val_loss: 0.0164 - val_mae: 0.0906\n",
      "Epoch 969/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0102 - mae: 0.0801\n",
      "Epoch 969: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0102 - mae: 0.0801 - val_loss: 0.0142 - val_mae: 0.0812\n",
      "Epoch 970/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0105 - mae: 0.0779\n",
      "Epoch 970: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0105 - mae: 0.0779 - val_loss: 0.0163 - val_mae: 0.0899\n",
      "Epoch 971/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0101 - mae: 0.0795\n",
      "Epoch 971: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0101 - mae: 0.0795 - val_loss: 0.0133 - val_mae: 0.0756\n",
      "Epoch 972/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0092 - mae: 0.0706\n",
      "Epoch 972: val_loss did not improve from 0.01196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0092 - mae: 0.0706 - val_loss: 0.0141 - val_mae: 0.0776\n",
      "Epoch 973/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0079 - mae: 0.0660\n",
      "Epoch 973: val_loss improved from 0.01196 to 0.01191, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0079 - mae: 0.0660 - val_loss: 0.0119 - val_mae: 0.0647\n",
      "Epoch 974/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0068 - mae: 0.0556\n",
      "Epoch 974: val_loss did not improve from 0.01191\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0068 - mae: 0.0556 - val_loss: 0.0121 - val_mae: 0.0644\n",
      "Epoch 975/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0062 - mae: 0.0526\n",
      "Epoch 975: val_loss did not improve from 0.01191\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0062 - mae: 0.0526 - val_loss: 0.0121 - val_mae: 0.0649\n",
      "Epoch 976/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0062 - mae: 0.0530\n",
      "Epoch 976: val_loss improved from 0.01191 to 0.01180, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0062 - mae: 0.0530 - val_loss: 0.0118 - val_mae: 0.0642\n",
      "Epoch 977/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0067 - mae: 0.0549\n",
      "Epoch 977: val_loss did not improve from 0.01180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0067 - mae: 0.0549 - val_loss: 0.0134 - val_mae: 0.0734\n",
      "Epoch 978/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0072 - mae: 0.0616\n",
      "Epoch 978: val_loss did not improve from 0.01180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0072 - mae: 0.0616 - val_loss: 0.0123 - val_mae: 0.0681\n",
      "Epoch 979/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0074 - mae: 0.0604\n",
      "Epoch 979: val_loss did not improve from 0.01180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0074 - mae: 0.0604 - val_loss: 0.0135 - val_mae: 0.0745\n",
      "Epoch 980/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0073 - mae: 0.0627\n",
      "Epoch 980: val_loss did not improve from 0.01180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0073 - mae: 0.0627 - val_loss: 0.0119 - val_mae: 0.0657\n",
      "Epoch 981/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0070 - mae: 0.0573\n",
      "Epoch 981: val_loss did not improve from 0.01180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0070 - mae: 0.0573 - val_loss: 0.0126 - val_mae: 0.0682\n",
      "Epoch 982/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0067 - mae: 0.0564\n",
      "Epoch 982: val_loss improved from 0.01180 to 0.01165, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0067 - mae: 0.0564 - val_loss: 0.0116 - val_mae: 0.0625\n",
      "Epoch 983/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0063 - mae: 0.0519\n",
      "Epoch 983: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0063 - mae: 0.0519 - val_loss: 0.0117 - val_mae: 0.0626\n",
      "Epoch 984/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0061 - mae: 0.0515\n",
      "Epoch 984: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0061 - mae: 0.0515 - val_loss: 0.0121 - val_mae: 0.0652\n",
      "Epoch 985/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0063 - mae: 0.0535\n",
      "Epoch 985: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0063 - mae: 0.0535 - val_loss: 0.0117 - val_mae: 0.0633\n",
      "Epoch 986/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0065 - mae: 0.0537\n",
      "Epoch 986: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0065 - mae: 0.0537 - val_loss: 0.0127 - val_mae: 0.0692\n",
      "Epoch 987/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0067 - mae: 0.0574\n",
      "Epoch 987: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0067 - mae: 0.0574 - val_loss: 0.0118 - val_mae: 0.0642\n",
      "Epoch 988/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0067 - mae: 0.0552\n",
      "Epoch 988: val_loss did not improve from 0.01165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0067 - mae: 0.0552 - val_loss: 0.0126 - val_mae: 0.0686\n",
      "Epoch 989/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0066 - mae: 0.0566\n",
      "Epoch 989: val_loss improved from 0.01165 to 0.01163, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0066 - mae: 0.0566 - val_loss: 0.0116 - val_mae: 0.0628\n",
      "Epoch 990/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0064 - mae: 0.0528\n",
      "Epoch 990: val_loss did not improve from 0.01163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0064 - mae: 0.0528 - val_loss: 0.0120 - val_mae: 0.0645\n",
      "Epoch 991/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0063 - mae: 0.0529\n",
      "Epoch 991: val_loss did not improve from 0.01163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0063 - mae: 0.0529 - val_loss: 0.0117 - val_mae: 0.0625\n",
      "Epoch 992/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0060 - mae: 0.0511\n",
      "Epoch 992: val_loss improved from 0.01163 to 0.01159, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0060 - mae: 0.0511 - val_loss: 0.0116 - val_mae: 0.0621\n",
      "Epoch 993/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0061 - mae: 0.0513\n",
      "Epoch 993: val_loss did not improve from 0.01159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0061 - mae: 0.0513 - val_loss: 0.0121 - val_mae: 0.0652\n",
      "Epoch 994/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0064 - mae: 0.0537\n",
      "Epoch 994: val_loss did not improve from 0.01159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0064 - mae: 0.0537 - val_loss: 0.0116 - val_mae: 0.0626\n",
      "Epoch 995/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0063 - mae: 0.0523\n",
      "Epoch 995: val_loss did not improve from 0.01159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0063 - mae: 0.0523 - val_loss: 0.0123 - val_mae: 0.0666\n",
      "Epoch 996/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0063 - mae: 0.0546\n",
      "Epoch 996: val_loss improved from 0.01159 to 0.01149, saving model to model2.6.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0063 - mae: 0.0546 - val_loss: 0.0115 - val_mae: 0.0623\n",
      "Epoch 997/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0063 - mae: 0.0523\n",
      "Epoch 997: val_loss did not improve from 0.01149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0063 - mae: 0.0523 - val_loss: 0.0120 - val_mae: 0.0651\n",
      "Epoch 998/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0062 - mae: 0.0534\n",
      "Epoch 998: val_loss did not improve from 0.01149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0062 - mae: 0.0534 - val_loss: 0.0115 - val_mae: 0.0619\n",
      "Epoch 999/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0062 - mae: 0.0513\n",
      "Epoch 999: val_loss did not improve from 0.01149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0062 - mae: 0.0513 - val_loss: 0.0117 - val_mae: 0.0629\n",
      "Epoch 1000/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0060 - mae: 0.0513\n",
      "Epoch 1000: val_loss did not improve from 0.01149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0060 - mae: 0.0513 - val_loss: 0.0115 - val_mae: 0.0622\n",
      "Restoring model weights from the end of the best epoch: 996.\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'model2.6.keras'\n",
    "BATCH_SIZE = len(X_train)\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(inputs,)),\n",
    "    tf.keras.layers.Dense(170, activation='relu'),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.002),\n",
    "    tf.keras.layers.Dense(1, kernel_initializer='normal')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose=1)\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=1000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_batch_size=len(X_valid),\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13018d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model2(model, testing_input_array, testing_label_array):\n",
    "  from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "  \"\"\"Calculates loss, makes predictions, and calculates Central Frequency (CF),\n",
    "  Mean Squared Error (MSE), Root Mean Squared Error(RMSE), Mean Absolute Error (MAE),\n",
    "  Median Absolute Error, and R-squared (R2)\n",
    "\n",
    "\tArgs:\n",
    "        model (tf.keras.model): The trained model\n",
    "\n",
    "        testing_input_array (array): Testing inputs\n",
    "\n",
    "        testing_label_array (array): Testing labels\n",
    "\t\"\"\"\n",
    "  print(\"Calculating Loss:\")\n",
    "  test_loss = model.evaluate(testing_input_array, testing_label_array, batch_size = len(testing_input_array))\n",
    "\n",
    "  print(\"Loss:\", test_loss)\n",
    "\n",
    "\n",
    "  print(\"\\nGenerating output predictions with model:\")\n",
    "  predictions = model.predict(testing_input_array, batch_size = len(testing_input_array))\n",
    "\n",
    "  # Calculate evaluation metrics\n",
    "  cf_15cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 15)\n",
    "  cf_5cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 5)\n",
    "  cf_1cm_percentage = calculate_central_frequency_percentage(testing_label_array, predictions, 1)\n",
    "  mse = mean_squared_error(testing_label_array, predictions)\n",
    "  rmse = root_mean_squared_error(testing_label_array, predictions)\n",
    "  mae = mean_absolute_error(testing_label_array, predictions)\n",
    "  medae = median_absolute_error(testing_label_array, predictions)\n",
    "  r2 = r2_score(testing_label_array, predictions)\n",
    "\n",
    "  print(\"\\nCentral Frequency Percentage 15cm:\", cf_15cm_percentage)\n",
    "  print(\"\\nCentral Frequency Percentage 5cm:\", cf_5cm_percentage)\n",
    "  print(\"\\nCentral Frequency Percentage 1cm:\", cf_1cm_percentage)\n",
    "  print(\"Mean Squared Error:\", mse)\n",
    "  print(\"Root Mean Squared Error:\", rmse)\n",
    "  print(\"Mean Absolute Error:\", mae)\n",
    "  print(\"Median Absolute Error:\", medae)\n",
    "  print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8c7df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Loss:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851ms/step - loss: 0.0084 - mae: 0.0532\n",
      "Loss: [0.008368647657334805, 0.053248047828674316]\n",
      "\n",
      "Generating output predictions with model:\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78110ad334c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\n",
      "Central Frequency Percentage 15cm: 95.49753484403148\n",
      "\n",
      "Central Frequency Percentage 5cm: 62.709775291552106\n",
      "\n",
      "Central Frequency Percentage 1cm: 14.736417938750357\n",
      "Mean Squared Error: 0.008368648938165997\n",
      "Root Mean Squared Error: 0.09148031995006356\n",
      "Mean Absolute Error: 0.05324805238532424\n",
      "Median Absolute Error: 0.03687351036071762\n",
      "R-squared: 0.7342754075465058\n"
     ]
    }
   ],
   "source": [
    "evaluate_model2(model,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0db630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, batch_size=len(X_test)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "253e9106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef06d410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamps = datasets['po15']['date_time'].iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "\n",
    "plot_range = 500\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(timestamps[:plot_range], y_test[:plot_range], label='True', marker='o', markersize=2)\n",
    "plt.plot(timestamps[:plot_range], y_pred[:plot_range], label='Predicted', marker='x', markersize=2)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Water Level (m)\")\n",
    "plt.title(\"Model Prediction vs True Water Level (First 24 hrs)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
